<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.553">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>free-lunch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="free-lunch_files/libs/clipboard/clipboard.min.js"></script>
<script src="free-lunch_files/libs/quarto-html/quarto.js"></script>
<script src="free-lunch_files/libs/quarto-html/popper.min.js"></script>
<script src="free-lunch_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="free-lunch_files/libs/quarto-html/anchor.min.js"></script>
<link href="free-lunch_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="free-lunch_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="free-lunch_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="free-lunch_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="free-lunch_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




title: “Forecast Linear Augmented Projection (FLAP): A free lunch to reduce forecast error variance” date: “23 April, 2024” author: - familyname: Yang othernames: Yangzhuoran Fin address: Monash UniversityMelbourne, Australia email: Fin.Yang@monash.edu - familyname: Athanasopoulos othernames: George address: Monash UniversityMelbourne, Australia - familyname: Hyndman othernames: Rob J. address: Monash UniversityMelbourne, Australia - familyname: Panagiotelis othernames: Anastasios address: University of SydneySydney, Australia pdf-engine: pdflatex cite-method: biblatex biblio-style: authoryear-comp bibliography: - references-key.bib - references-pkg.bib blind: false cover: true toc: false keep-tex: true fig-height: 6 fig-width: 8 number-sections: true number-depth: 2 execute: echo: false warning: false message: false cache: false editor_options: chunk_output_type: console filters: - latex-environment - abstract-section environments: toappendix header-includes: -
<ul>
<li><ul>
<li></li>
<li></li>
<li>format: wp-pdf</li>
</ul></li>
</ul>
<hr>
<section id="abstract" class="level1">
<h1>Abstract</h1>
<p>A novel forecast linear augmented projection (FLAP) method is introduced. FLAP provably reduces the forecast error variance of any unbiased multivariate forecast without introducing bias. The method first constructs new series as linear combinations of the original series. Forecasts are then generated for both the original and new series. Finally, the full vector of forecasts is projected onto a linear subspace where the constraints implied by the combination weights hold. It is proven that the trace of the forecast error variance is non-increasing with the number of components, and mild conditions are established for which it is strictly decreasing. It is also shown that the proposed method achieves maximum forecast error variance reduction among linear projections. The theoretical results are validated through simulations and two empirical applications based on Australian tourism and FRED-MD data. Notably, using FLAP with Principal Component Analysis to construct the new series leads to substantial forecast error variance reduction.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">read_chunk</span>(<span class="st">"free-lunch.R"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>current_path <span class="ot">&lt;-</span> <span class="cf">function</span>(...){</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  path <span class="ot">&lt;-</span> <span class="fu">file.path</span>(...)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">normalizePath</span>(path)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>targets<span class="sc">::</span><span class="fu">tar_source</span>(<span class="fu">current_path</span>(<span class="st">"R"</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>pa <span class="ot">&lt;-</span> <span class="cf">function</span>(...) <span class="fu">current_path</span>(<span class="st">"monarch"</span>, ...)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.0     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(kableExtra)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'kableExtra'

The following object is masked from 'package:dplyr':

    group_rows</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cb_palette_grey <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"#999999"</span>, <span class="st">"#E69F00"</span>, <span class="st">"#56B4E9"</span>, <span class="st">"#009E73"</span>, <span class="st">"#F0E442"</span>, <span class="st">"#0072B2"</span>, <span class="st">"#D55E00"</span>, <span class="st">"#CC79A7"</span>)</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>cb_palette_black <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"#000000"</span>, <span class="st">"#E69F00"</span>, <span class="st">"#56B4E9"</span>, <span class="st">"#009E73"</span>, <span class="st">"#F0E442"</span>, <span class="st">"#0072B2"</span>, <span class="st">"#D55E00"</span>, <span class="st">"#CC79A7"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="sec-introduction" class="level1">
<h1>Introduction</h1>
<p>Multivariate forecasting arises in a number of disciplines including macroeconomics and finance; see <span class="citation" data-cites="macroreview">@macroreview</span> and <span class="citation" data-cites="financereview">@financereview</span> respectively, and references therein. We introduce a new post processing framework that (i) augments the data by constructing new series that are linear combinations of the original series, (ii) forecasts both the original and new series and (iii) recovers a new set of forecasts for the original series via projections. We refer to this method as Forecast Linear Augmented Projection (FLAP). We prove that the method reduces the forecast error variance of the original series in a way that is agnostic both with respect to the weights of the linear combinations used at step (i) and with respect to the model used to generate forecasts at step (ii). The model is inspired by the forecast reconciliation literature <span class="citation" data-cites="AthEtAl2023a">[@AthEtAl2023a]</span> whereby forecasts are adjusted to cohere with known linear constraints. In contrast to that literature, the FLAP method focuses on multivariate forecasting where such constraints are not present. Indeed, the method need not only be applied to forecasting problems, but multivariate predictions in general.</p>
<p>It may appear puzzling that forecast accuracy can be improved, not by introducing any new information, but by simply taking linear combinations of existing time series. To give an intuition into how this puzzle can be resolved, we consider a toy example of two series <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> that are of concern to the forecaster and two linear combinations or <em>components</em> of the series <span class="math inline">\(c_1=0.5z_1+0.5z_2\)</span> and <span class="math inline">\(c_2=0.5z_1-0.5z_2\)</span>. Denote by <span class="math inline">\(\hat{y}_1\)</span>, <span class="math inline">\(\hat{y}_2\)</span>, <span class="math inline">\(\hat{c}_1\)</span>, <span class="math inline">\(\hat{c}_2\)</span> any forecasts of these original series and components, which we collectively refer to as <em>base forecasts</em>. The base forecasts may be generated by univariate methods, multivariate methods, or even based on expert judgement. When considering <span class="math inline">\(y_1\)</span>, there is both a <em>direct</em> forecast <span class="math inline">\(\hat{y}_1\)</span> and <em>indirect</em> forecast <span class="math inline">\(\hat{c}_1+\hat{c}_2\)</span>, similarly for <span class="math inline">\(y_2\)</span> the direct forecast is <span class="math inline">\(\hat{y}_2\)</span> and the indirect forecast <span class="math inline">\(\hat{c}_1-\hat{c}_2\)</span>.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> With the exception of some pathological cases, the direct and indirect forecasts for the same variable will not, in general, be equal. Therefore forecast accuracy can be improved by combining direct and indirect forecasts, something implicitly achieved by the proposed FLAP method. The puzzle is thus resolved; while no new information is created at the data augmentation step, there is new information embedded into forecasts of the augmented series, which can be leveraged via model combination (see <span class="citation" data-cites="WanEtAl2023">@WanEtAl2023</span> for a review of forecast combination). Something obscured by the simple toy example is the way our FLAP method differs from the usual forecast combination methods, in particular our combinations are potentially non-convex since they are obtained via projections, in a way that we now elaborate upon.</p>
<p>More formally and more generally, the FLAP method considers a vector of original series <span class="math inline">\(\mathbf{y}\in \mathbb{R}^m\)</span> and a vector of components <span class="math inline">\(\mathbf{c}\in \mathbb{R}^p\)</span>. While <span class="math inline">\(\left(\mathbf{y}',\mathbf{c}'\right)'\)</span> is a <span class="math inline">\(p+m\)</span>-vector, the construction of components as linear combinations of the original series implies that <span class="math inline">\(\left(\mathbf{y}',\mathbf{c}'\right)'\)</span> lies on a linear subspace of at most dimension <span class="math inline">\(m\)</span>. The corresponding vector of forecasts <span class="math inline">\(\left(\hat{\mathbf{y}}',\hat{\mathbf{c}}'\right)'\)</span> will, in general, have support on <span class="math inline">\(\mathbb{R}^{m+p}\)</span>. FLAP projects <span class="math inline">\(\left(\hat{\mathbf{y}}',\hat{\mathbf{c}}'\right)'\)</span> onto the <span class="math inline">\(m\)</span>-dimensional linear subspace on which <span class="math inline">\(\left(\mathbf{y}',\mathbf{c}'\right)'\)</span> has support. The setup of this problem bears similarities to the well known problem of forecast reconciliation where <span class="citation" data-cites="PanEtAl2021">@PanEtAl2021</span> provide similar geometric intuition, while <span class="citation" data-cites="WicEtAl2019">@WicEtAl2019</span>, <span class="citation" data-cites="AthEtAl2017">@AthEtAl2017</span>, and <span class="citation" data-cites="DiGir2023">@DiGir2023</span> have all shown that reconciliation can reduce forecast error variance theoretically and empirically. However, we note that these papers establish that reconciliation improves forecast accuracy for the hierarchy <em>as a whole</em>. In the general multivariate setting that we consider, this would imply improvements in forecast accuracy for <span class="math inline">\(\mathbf{y}\)</span> and <span class="math inline">\(\mathbf{c}\)</span> taken together. This poses a problem if improvements in forecast accuracy for <span class="math inline">\(\mathbf{c}\)</span> could be offset by a deterioration in forecast accuracy for <span class="math inline">\(\mathbf{y}\)</span>, since the former are not of interest in and of themselves. A key insight we make in this paper in the hierarchical setting, is that reductions in forecast error variance accrue even for a subset of variables in the hierarchy. It is this contribution that allows us to propose a method that goes beyond the case where time series adhere to linear constraints, and that instead applies to the more general setting.</p>
<p>While the theoretical results apply for any linear combinations of the original series, in practice we propose to augment the data with principal components. When doing so, the FLAP method bears a resemblance to Dynamic Factor Models (DFMs), specifically those common in macroeconomic forecasting <span class="citation" data-cites="StoWat2002a StoWat2002 StoWat2012">[@StoWat2002a; @StoWat2002; @StoWat2012]</span>, their extensions in the machine learning literature <span class="citation" data-cites="DeEtAl2019">[@DeEtAl2019]</span> as well as the factor augmented VAR <span class="citation" data-cites="BerEtAl2005">[@BerEtAl2005]</span>. The factor models assume that the multivariate time series possesses common components and the dynamics of the observed series are governed by the dynamics of these unobserved factors, often assumed to follow some parametric model. In contrast, FLAP is a post-forecasting step, indeed forecasts can even be made using a DFM and then further improved by implementing the FLAP method, something we demonstrate in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a> and <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>.</p>
<!-- , where we also show how the performance of projected forecasts from univariate ARIMA models is comparable to base DFM forecasts, with the help of components. -->
<!-- We introduce a new forecast linear augmented projection (FLAP) method for improving the accuracy of any multivariate time series method, often substantially. This is done by leveraging direct and indirect forecast of the series without introducing any new data, or any new information. Thus, we can call it a "free lunch" method [@NFLT]: it is a simple addition to any existing multivariate forecasting method that can improve its accuracy. -->
<!-- The method is based on the idea that the forecasts of linear combinations of the series should be consistent with the forecasts of the series themselves. For example, suppose we have two observed series $z_{t,1}$ and $z_{t,2}$, and we also construct the combination $c_t = 2 z_{t,1} -  z_{t,1}$, then the forecasts of $c_t$ should satisfy the same linear constraint: $\hat{c}_t = 2 \hat{z}_{t,1} - \hat{z}_{t,2}$. If they do not, then we can improve the forecasts of all three series by adjusting them to be consistent. While the forecasts of $c_t$ may be of no interest in themselves, they can be used to improve the forecasts of $z_{t,1}$ and $z_{t,2}$. -->
<!-- The idea can be extended to any number of linear combinations, and can be applied to any number of multivariate series. It does not depend on the forecasting method being used, and works well even if all series are forecast using univariate models. In fact, when used with univariate models, this allows cross-correlations between the series to be implicitly captured in the forecasts. -->
<!-- We call these linear combinations of the observed time series "components", and we call the original forecasts of the observed series "base forecasts". Our FLAP method is to adjust the base forecasts to be consistent with the forecasts of the components by projecting all forecasts onto the space where the linear constraints are satisfied. We show (theoretically and empirically) that this method leads to significant reduction in the forecast error variance, without introducing any bias. -->
<!-- Our FLAP method has close connections to forecast reconciliation in the hierarchical forecasting literature. See @AthEtAl2023a for a recent review of the area. In particular, the projection formulation is inspired by the minimum trace [MinT, @WicEtAl2019] solution of the forecast reconciliation problem. Forecast reconciliation is a method to modify forecasts using projection so that they conform to a specific hierarchical, grouped or temporal structure. Notably, @WicEtAl2019, @AthEtAl2017, and  @DiGir2023 have shown that forecast reconciliation can reduce forecast error variance theoretically and empirically, in cross-sectional settings, temporal settings, and in cross-temporal settings. @PanEtAl2021 have provided insight into the geometric interpretation of the projection used in forecast reconciliation. However, forecast reconciliation cannot be directly applied in a general multivariate time series unless the series satisfy some linear constraints such as a hierarchical structure. In contrast, our method can be applied to any multivariate time series. It can also be used in conjunction with forecast reconciliation to further reduce forecast error variance. -->
<p>In the sense that forecast accuracy can be improved without any new information, FLAP has parallels with bootstrap aggregation or “bagging” <span class="citation" data-cites="Bre1996 BerEtAl2016">[@Bre1996;@BerEtAl2016]</span>. Bagging can reduce prediction variance without increasing bias <span class="citation" data-cites="HasEtAl2003">[@HasEtAl2003]</span>, by mitigating model uncertainty <span class="citation" data-cites="PetEtAl2018">[@PetEtAl2018]</span>, and does so without introducing any new data, but rather resampled versions of the existing data. Our FLAP method also reduces forecast error variance without introducing new data, but using linear combinations of the existing data, rather than bootstrapping. The FLAP method (in addition to forecast reconciliation and bagging) can be viewed as contributing to the literature where forecasts are improved by combination and data augmentation methods. This includes the theta method <span class="citation" data-cites="AssNik2000">[@AssNik2000]</span>, temporal aggregation <span class="citation" data-cites="KouEtAl2014 AthEtAl2017">[@KouEtAl2014; @AthEtAl2017]</span>, forecasting with sub-seasonal series <span class="citation" data-cites="LiEtAl2022c">[FOSS, @LiEtAl2022c]</span> and forecast combination with multiple starting points <span class="citation" data-cites="DisPet2015">[@DisPet2015]</span>; a review of all these methods can be found in <span class="citation" data-cites="PetSpi2021">@PetSpi2021</span> who refer to them as using “the wisdom of data”. Our FLAP method is distinct in that it aims to exploit information in the data with a focus on linear combinations of multivariate series.</p>
<!-- A second difference is that bagging is model dependent: it is a procedure applied to enhance the models that produce the forecasts, where the same models are fitted repeatedly. Our method is model independent: it linearly transforms a set of forecasts, regardless of which models they come from. As a result, the two methods can be used in conjunction: the projections can be applied to forecasts produced by a bagged predictor. -->
<!-- Another approach to improve forecast accuracy is forecast combination. Point forecast combinations usually involve combining multiple forecasts of the same series from different models. See @WanEtAl2023 for a recent comprehensive review. Our FLAP method differs from forecast combination in (a) the forecasts we combine, and (b) in how we combine them. First, rather than combine multiple forecasts of a single series, we combine single forecasts of many different linear combinations of all observed series. Second, our combinations are obtained via projections, and so the final forecasts of a particular series are linear combinations of all series in the collection, including the observed series and all constructed components. Our approach can be used in conjunction with standard forecast combination, as the base forecasts can be obtained from any combination of forecasts. -->
<p>The remainder of the paper is structured as follows. In <a href="#sec-method" class="quarto-xref">Section&nbsp;3</a>, we propose the FLAP method, and highlight its theoretical properties and associated estimation methods. In <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>, we present a simulation example demonstrating its performance and discuss the implications for sources of uncertainty. <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a> examines the performance of FLAP in two empirical applications: forecasting Australian domestic tourism and forecasting macroeconomic variables in the FRED-MD data set. <a href="#sec-conclusion" class="quarto-xref">Section&nbsp;6</a> concludes with some thoughts on future research directions. The methods introduced in this paper are implemented in the <code>flap</code> package <span class="citation" data-cites="flap">[@flap]</span>. This paper is fully reproducible with code and documentation provided at <a href="https://github.com/FinYang/paper-forecast-projection">https://github.com/FinYang/paper-forecast-projection</a>.</p>
</section>
<section id="sec-method" class="level1">
<h1>Forecast Linear Augmented Projection (FLAP)</h1>
<section id="method-and-theoretical-properties" class="level2">
<h2 class="anchored" data-anchor-id="method-and-theoretical-properties">Method and theoretical properties</h2>
<p>In the following, all vectors and matrices are denoted in bold font. We use <span class="math inline">\(\bm{I}_n\)</span> to denote the <span class="math inline">\(n\times n\)</span> identity matrix, and <span class="math inline">\(\bm{O}_{n\times k}\)</span> to denote the <span class="math inline">\(n\times k\)</span> zero matrix.</p>
<p>Let <span class="math inline">\(\bm{y}_t\in\mathbf{R}^m\)</span> be a vector of <span class="math inline">\(m\)</span> observed time series we are interested in forecasting. The FLAP method involves three steps:</p>
<ol type="1">
<li><em>Form components</em>. Form <span class="math inline">\(\bm{c}_t = \bm{\Phi}\bm{y}_t\in\mathbf{R}^p\)</span>, a vector of <span class="math inline">\(p\)</span> linear combinations of <span class="math inline">\(\bm{y}_t\)</span> at time <span class="math inline">\(t\)</span>, where <span class="math inline">\(\bm{\Phi}\in\mathbf{R}^{p\times m}\)</span>. We call <span class="math inline">\(\bm{c}_t\)</span> the components of <span class="math inline">\(\bm{y}_t\)</span> and the component weights <span class="math inline">\(\bm{\Phi}\)</span> are known in the sense that they are chosen by the user of FLAP. Let <span class="math inline">\(\bm{z}_{t} = \big[\bm{y}_t', \bm{c}'_{t}\big]'\)</span> be the concatenation of series <span class="math inline">\(\bm{y}_t\)</span> and components <span class="math inline">\(\bm{c}_{t}\)</span>. We note that <span class="math inline">\(\bm{z}_{t}\)</span> will be constrained in the sense that <span class="math inline">\(\bm{C}\bm{z}_t= \bm{c}_{t} - \bm{\Phi}\bm{y}_t = \bm{0}\)</span> for any <span class="math inline">\(t\)</span> where <span class="math inline">\(\bm{C} = \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]\)</span> is referred to as the constraint matrix.</li>
<li><em>Generate forecasts</em>. Denote as <span class="math inline">\(\hat{\bm{z}}_{t+h}\)</span> the <span class="math inline">\(h\)</span>-step-ahead base forecast of <span class="math inline">\(\bm{z}_{t}\)</span>. The method used to generate forecasts is again selected by the user. This can be univariate or multivariate. In the more general setting where <span class="math inline">\(\bm{z}_t\)</span> are not time series but cross sectional data, any prediction method can be used. In general, the constraints that hold for <span class="math inline">\(\bm{z}_t\)</span> will not hold for <span class="math inline">\(\hat{\bm{z}}_{t+h}\)</span>, i.e <span class="math inline">\(\bm{C}\hat{\bm{z}}_{t+h}\neq \bm{0}\)</span></li>
<li><em>Project the base forecasts</em>. Let <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span> be a set of projected forecasts such that, <span id="eq-z_tilde"><span class="math display">\[
\tilde{\bm{z}}_{t+h} = \bm{M} \hat{\bm{z}}_{t+h}
\tag{1}\]</span></span> with projection matrix <span id="eq-M"><span class="math display">\[
\bm{M} = \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C},
\tag{2}\]</span></span> where <span class="math inline">\(\operatorname{Var}(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) = \bm{W}_h\)</span> is the forecast error covariance matrix. For the proofs of this section, we will assume that <span class="math inline">\(\bm{W}_h\)</span> is known, in practice a plug-in estimate can be used that will be discussed in <a href="#sec-estW" class="quarto-xref">Section&nbsp;3.5</a>.</li>
</ol>
<p>In practice we are interested in forecasts of <span class="math inline">\(\bm{y}_t\)</span> and not the full vector <span class="math inline">\(\bm{z}_t\)</span>. We now introduce some notation to handle this issue. Define the selection matrix <span class="math inline">\(\bm{J}_{n,k} = \big[\bm{I}_n ~~~ \bm{O}_{n\times k}\big]\)</span>, so that <span class="math inline">\(\bm{J}_{n,k}\bm{A}\)</span> selects the first <span class="math inline">\(n\)</span> rows of a matrix <span class="math inline">\(\bm{A}\)</span>. Let <span class="math inline">\(\hat{\bm{y}}_{t+h}\)</span> and <span class="math inline">\(\tilde{\bm{y}}_{t+h}\)</span> denote the first <span class="math inline">\(m\)</span> elements of <span class="math inline">\(\hat{\bm{z}}_{t+h}\)</span> and <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span>, comprising the base and projected forecasts of <span class="math inline">\(\bm{y}_t\)</span> respectively. Similarly, let <span class="math inline">\(\hat{\bm{c}}_{t+h}\)</span> and <span class="math inline">\(\tilde{\bm{c}}_{t+h}\)</span> denote the last <span class="math inline">\(p\)</span> elements of <span class="math inline">\(\hat{\bm{z}}_{t+h}\)</span> and <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span>, comprising the base and projected forecasts of <span class="math inline">\(\bm{c}_t\)</span> respectively. Then the projected forecast of <span class="math inline">\(\bm{y}_t\)</span> can be found by <span id="eq-lcmap"><span class="math display">\[
\tilde{\bm{y}}_{t+h} = \bm{J}\tilde{\bm{z}}_{t+h} = \bm{J}\bm{M} \hat{\bm{z}}_{t+h},
\tag{3}\]</span></span> where <span class="math inline">\(\bm{J} = \bm{J}_{m,p}\)</span>.</p>
<p>We now present some theoretical results regarding the FLAP method, with proofs provided in the Appendix. <a href="#thm-psdvar" class="quarto-xref">Theorem&nbsp;1</a> establishes that the forecasts produced by the FLAP method, <span class="math inline">\(\tilde{\bm{y}}_{t+h}\)</span>, dominate the base forecasts, <span class="math inline">\(\hat{\bm{y}}_{t+h}\)</span>, in the sense that the difference between their forecast error variances is always positive definite. <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a> establishes that the trace of the covariance of the forecast errors of <span class="math inline">\(\tilde{\bm{y}}_{t+h}\)</span> is non-increasing with the number of components <span class="math inline">\(p\)</span>. The conditions needed to make the trace strictly decreasing are discussed in <a href="#thm-pos-condition" class="quarto-xref">Theorem&nbsp;3</a>. In <a href="#thm-minvar" class="quarto-xref">Theorem&nbsp;4</a> we prove that the projection in <a href="#eq-M" class="quarto-xref">Equation&nbsp;2</a> achieves the minimum forecast error variance amongst the class of all projections. Finally, while the theoretical results imply that components could in principle continue to be added to improve forecasts, in practice, larger values of <span class="math inline">\(p\)</span> will make estimation of the plug-in covariance matrix <span class="math inline">\(\bm{W}_h\)</span> unreliable. We explore this issue in a simulation setting and empirically in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a> and <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>, respectively.</p>
</section>
<section id="positive-semi-definiteness-of-error-variance-reduction" class="level2">
<h2 class="anchored" data-anchor-id="positive-semi-definiteness-of-error-variance-reduction">Positive Semi-Definiteness of Error Variance Reduction</h2>
<p>We first provide some intermediate results.</p>
<div id="lem-projectionM" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1</strong></span> Matrix <span class="math inline">\(\bm{M}\)</span> is a projection onto the space where the constraint <span class="math inline">\(\bm{C}\bm{z}_t=\bm{0}\)</span> is satisfied.</p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-lem-projectionm" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-lem-projectionm">Proof of <a href="#lem-projectionM" class="quarto-xref">Lemma&nbsp;1</a></h3>
<p> We have <span class="math display">\[
\begin{aligned}
\bm{M}\bm{M}
&amp; = \bm{I}_{m+p} - 2 \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
&amp; \mbox{}\hspace*{1cm} + \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\\
&amp; =\bm{I}_{m+p} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
&amp; = \bm{M},
\end{aligned}
\]</span> so <span class="math inline">\(\bm{M}\)</span> is a projection matrix. For any <span class="math inline">\(\bm{z}\)</span> such that <span class="math inline">\(\bm{M}\bm{z} = \bm{y}\)</span> for some <span class="math inline">\(\bm{y}\)</span>, we have <span class="math display">\[
\bm{C}\bm{y} = \bm{C}\bm{M}\bm{z} = \bm{C}\bm{z} - \bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{z} = \bm{0}.
\]</span> Thus, <span class="math inline">\(\bm{M}\)</span> projects any vector onto the space where the constraint <span class="math inline">\(\bm{C}\bm{y}=0\)</span> is satisfied.</p>
</section>
<p>Based on the attractive properties of projections, we have the following corollaries.</p>
<div id="cor-corM" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 1</strong></span> &nbsp;</p>
<ol type="1">
<li>The projected forecast <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span> satisfies the constraint <span class="math inline">\(\bm{C}\tilde{\bm{z}}_{t+h}=\bm{0}\)</span>.</li>
<li>For <span class="math inline">\(\bm{z}_{t+h}\)</span> that already satisfies the constraint, the projection does not change its value, i.e., <span class="math inline">\(\bm{M}\bm{z}_{t+h} = \bm{z}_{t+h}\)</span> <span class="citation" data-cites="Rao1974">[@Rao1974, Lemma 2.4]</span>.</li>
<li>If the base forecasts are unbiased such that <span class="math inline">\(\operatorname{E}(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t)\)</span>, then the projected forecasts are also unbiased, i.e., <span class="math inline">\(\operatorname{E}(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t)\)</span>.</li>
</ol>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-cor-corm" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-cor-corm">Proof of <a href="#cor-corM" class="quarto-xref">Corollary&nbsp;1</a></h3>
<p> Items 1 and 2 are trivial application of <a href="#lem-projectionM" class="quarto-xref">Lemma&nbsp;1</a>. To prove 3, we have <span class="math display">\[
\operatorname{E}(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) =
\operatorname{E}(\bm{M}\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{M}\operatorname{E}(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{M}\operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{M}\bm{z}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t).
\]</span></p>
</section>
<p>The assumption of unbiasedness of the base forecasts is not unreasonable in practice, and where it does not hold, a bias correction method can be applied. Note this is not a requirement on model specification. We do not assume the model producing the base forecast is correctly specified like in the DFM literature <span class="citation" data-cites="StoWat2002">[e.g., @StoWat2002]</span>. In fact, the power of FLAP manifests when the models are misspecified, as discussed in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>.</p>
<div id="lem-var" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2</strong></span> The forecast error covariance matrix of the component-augmented projected <span class="math inline">\(h\)</span>-step-ahead forecasts <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span> is <span class="math display">\[
\operatorname{Var}(\bm{z}_{t+h} - \tilde{\bm{z}}_{t+h}) = \bm{M}\bm{W}_h\bm{M}' = \bm{M}\bm{W}_h,
\]</span> and the forecast error covariance matrix of the projected <span class="math inline">\(h\)</span>-step-ahead forecasts <span class="math inline">\(\tilde{\bm{y}}_{t+h}\)</span> is <span class="math display">\[
\operatorname{Var}(\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
\]</span></p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-lem-var" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-lem-var">Proof of <a href="#lem-var" class="quarto-xref">Lemma&nbsp;2</a></h3>
<p> <span class="math display">\[
\operatorname{Var}(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})
= \operatorname{Var}(\bm{M}\hat{\bm{z}}_{t+h} - \bm{M}\bm{z}_{t+h}) \\
= \bm{M}\operatorname{Var}(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h})\bm{M}' \\
= \bm{M}\bm{W}_h\bm{M}'.
\]</span> If we simplify it further, we have <span class="math display">\[
\begin{aligned}
\bm{M}\bm{W}_h\bm{M}'
&amp;= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h(\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})'\\
%&amp;= (\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h)(\bm{I} - \bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h) \\
&amp;= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&amp;\mbox{}\hspace*{1cm} +  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&amp;= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
%&amp;= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h \\
&amp;= \bm{M}\bm{W}_h.
\end{aligned}
\]</span> To get <span class="math inline">\(\operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})\)</span>, we just need to recognise that it is the first <span class="math inline">\(m\times m\)</span> leading principal submatrix of <span class="math inline">\(\operatorname{Var}(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})\)</span>.</p>
</section>
<p><a href="#lem-var" class="quarto-xref">Lemma&nbsp;2</a> is a well-known result in the forecast reconciliation literature <span class="citation" data-cites="DiGir2023">[e.g., @DiGir2023]</span>.</p>
<div id="thm-psdvar" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1 (Positive Semi-Definiteness of Error Variance Reduction)</strong></span> The difference between the forecast error variances of the base and projected component-augmented forecasts, <span class="math display">\[
\begin{aligned}
\operatorname{Var}(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\operatorname{Var}(\bm{z}_{t+h}-\tilde{\bm{z}}_{t+h} ) &amp;=\bm{W}_h-\bm{M}\bm{W}_h\\
&amp;= \bm{W}_h - (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h\\
&amp;=\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h,
\end{aligned}
\]</span> is positive semi-definite. The difference between the forecast error variances of the base and projected forecasts of the original series, <span class="math display">\[
\operatorname{Var}(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\operatorname{Var}(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}',
\]</span> is therefore also positive semi-definite.</p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-thm-psdvar" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-thm-psdvar">Proof of <a href="#thm-psdvar" class="quarto-xref">Theorem&nbsp;1</a></h3>
<p> Trivially, <span class="math inline">\(\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\)</span> and <span class="math inline">\(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'\)</span> are positive semi-definite. Note that <span class="math inline">\(\operatorname{Var}(\hat{\bm{y}}_{t+h} - \bm{z}_{t+h}) -\operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{z}_{t+h})\)</span> is the leading principal submatrix of <span class="math inline">\(\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\)</span>, and the leading principal submatrix of a positive semi-definite matrix is positive semi-definite.</p>
</section>
<p><a href="#thm-psdvar" class="quarto-xref">Theorem&nbsp;1</a> is why FLAP works. The trace of <span class="math inline">\(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'\)</span> is the sum of the reduction in forecast error variances, and is non-negative because the matrix is positive semi-definite. It implies that the forecast error variance can be reduced by simply forecasting the components (the artificially constructed linear combinations of the original data), and mapping the forecasts using matrix <span class="math inline">\(\bm{M}\)</span>. For the improvement to be zero, the trace must be zero. This implies that the entire <span class="math inline">\(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}' = \bm{O}_{m\times m}\)</span> as this is a positive semi-definite matrix, something rarely observed in practice. See <a href="#thm-pos-condition" class="quarto-xref">Theorem&nbsp;3</a> for more discussion.</p>
<p>The following example illustrates the mechanism of the reduction in the forecast error variance.</p>
<div id="exm-identityW" class="theorem example">
<p><span class="theorem-title"><strong>Example 1</strong></span> Suppose <span class="math inline">\(\bm{z}_t\)</span> comprises <span class="math inline">\(m\)</span> original series <span class="math inline">\(\bm{y}_t\)</span> and <span class="math inline">\(p\)</span> components <span class="math inline">\(\bm{c}_t\)</span>. Let <span class="math inline">\(\hat{\bm{z}}_{t+h}\)</span> and <span class="math inline">\(\tilde{\bm{z}}_{t+h}\)</span> be <span class="math inline">\(h\)</span>-step-ahead base and projected forecasts of <span class="math inline">\(\bm{z}_t\)</span>. Assume that their corresponding forecast errors are uncorrelated with unit variance such that <span class="math inline">\(\bm{W}_h = \bm{I}_{m+p}\)</span>. Then <span class="math display">\[
\begin{aligned}
\operatorname{Var}(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\operatorname{Var}(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})
&amp;=
\bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C}\\
&amp; =
\begin{bmatrix} -\bm{\Phi}'\\ \bm{I}_{p} \end{bmatrix}
(\bm{\Phi}\bm{\Phi}' + \bm{I}_p)^{-1}
\begin{bmatrix} -\bm{\Phi}&amp; \bm{I}_{p} \end{bmatrix} ,
\end{aligned}
\]</span> where <span class="math display">\[
\bm{C} = \begin{bmatrix} - \bm{\Phi}&amp; \bm{I}_{p} \end{bmatrix}.
\]</span> Let <span class="math inline">\(\bm{\Phi}\)</span> consist of <span class="math inline">\(p\le m\)</span> orthogonal unit vectors, for example, those obtained from Principal Component Analysis <span class="citation" data-cites="Jol2002">[PCA, @Jol2002]</span>. In this case <span class="math inline">\(\bm{\Phi}\bm{\Phi}' = \bm{I}_p\)</span> and <span class="math display">\[
\operatorname{Var}(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\operatorname{Var}(\bm{z}_{t+h}-\tilde{\bm{z}}_{t+h})
  =  \frac{1}{2}
      \begin{bmatrix}
        \bm{\Phi}'\bm{\Phi} &amp; -\bm{\Phi}'\\
        -\bm{\Phi} &amp; \bm{I}_p
       \end{bmatrix}.
\]</span> Focusing on the forecast error variance reduction of the forecasts of the original series <span class="math inline">\(\bm{y}_t\)</span>, i.e., <span class="math inline">\(\operatorname{tr}(\operatorname{Var}(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\operatorname{Var}(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h})) = \frac{1}{2}\operatorname{tr}(\bm{\Phi}'\bm{\Phi})\)</span>.</p>
<ul>
<li>When <span class="math inline">\(p&lt;m\)</span>, since <span class="math inline">\(\bm{\Phi}'\bm{\Phi}\)</span> is idempotent, <span class="math inline">\(\operatorname{tr}(\bm{\Phi}'\bm{\Phi}) = \operatorname{rank}(\bm{\Phi}'\bm{\Phi}) = p\)</span>. Hence, focusing on the original series the reduction in the total forecast error variance of the FLAP forecasts relative to the base forecasts, is <span class="math inline">\(p/2\)</span>.</li>
<li>When <span class="math inline">\(p=m\)</span>, where all principal components are used, <span class="math inline">\(\bm{\Phi}'\bm{\Phi} = \bm{I}_m\)</span>. This implies a total reduction in the error variance of <span class="math inline">\(m/2\)</span>, and that for each of the <span class="math inline">\(m\)</span> individual series the error variance is halved.</li>
</ul>
<!-- NOT SURE THIS HELPS -- In simplest of cases where we have two original series ($m=2$), using $p=1$ component will reduce the total forecast error variance by $0.5$, while using both components ($p=2$) will reduce the total forecast error variance by $1$; that is a $50\%$ reduction as the original sum of forecast error variances is $2$. -->
<p>If we keep increasing the number of components beyond <span class="math inline">\(m\)</span>, the result in <a href="#thm-psdvar" class="quarto-xref">Theorem&nbsp;1</a> still holds, although <span class="math inline">\(\bm{\Phi}\)</span> can no longer contain orthogonal vectors, and the example here becomes intractable. This is an artificial example as the forecast error variance <span class="math inline">\(\bm{W}_h\)</span> can hardly be an identity in practice. It is likely that the forecast error of a linear combination of series will be correlated to the forecast error of forecasting these series directly. Nonetheless, the aim of the example is to demonstrate how the forecast error variance can be reduced as a result of the component forecasts bringing new information about the original series. The forecast error variance reduction becomes larger as we increase the number of components <span class="math inline">\(p\)</span>. This is not a coincidence but a desirable property of FLAP, as shown in the next section.</p>
</div>
</section>
<section id="monotonicity" class="level2">
<h2 class="anchored" data-anchor-id="monotonicity">Monotonicity</h2>
<p>In the results that follow, we break the base forecast error covariance matrix into smaller blocks. <span class="math display">\[
\bm{W}_h =
\begin{bmatrix}
\bm{W}_{y, h} &amp; \bm{W}_{yc, h} \\
\bm{W}_{yc, h}' &amp; \bm{W}_{c, h}
\end{bmatrix},
\]</span> where <span class="math inline">\(\bm{W}_{y, h}\)</span> is the forecast error covariance matrix of <span class="math inline">\(\hat{\bm{y}}_{t+h}\)</span>, <span class="math inline">\(\bm{W}_{c, h}\)</span> is the forecast error covariance matrix of <span class="math inline">\(\hat{\bm{c}}_{t+h}\)</span>, and <span class="math inline">\(\bm{W}_{yc, h}\)</span> contains error covariances between elements of <span class="math inline">\(\hat{\bm{y}}_{t+h}\)</span> and <span class="math inline">\(\hat{\bm{c}}_{t+h}\)</span>.</p>
<div id="thm-monotone" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2 (Monotonicity)</strong></span> The forecast error variance reductions for each series, i.e., the diagonal elements in the matrix <span class="math display">\[
\operatorname{Var}(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\operatorname{Var}(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'
\]</span> is non-decreasing as <span class="math inline">\(p\)</span> increases. In particular, the sum of forecast error variance reductions <span id="eq-trdvar"><span class="math display">\[
\operatorname{tr}(\operatorname{Var}(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\operatorname{Var}(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h})) = \operatorname{tr}(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}')
\tag{4}\]</span></span> is non-decreasing as <span class="math inline">\(p\)</span> increases.</p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-thm-monotone" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-thm-monotone">Proof of <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a></h3>
<p> Suppose now that we want to include <span class="math inline">\(q\)</span> more components <span class="math inline">\(\bm{c}_{t}^*=\bm{\Phi}^*\bm{y}_t\)</span> in the projection. We define <span class="math inline">\(\bm{z}_{t}^* = \begin{bmatrix}\bm{z}_{t}\\ \bm{c}_{t}^*\end{bmatrix}\)</span>, the constraint matrix <span id="eq-Cstar"><span class="math display">\[
\bm{C}^* =
\begin{bmatrix}
\multicolumn{2}{c}{\bm{C}} &amp; \underset{p\times q}{\bm{0}} \\
-\underset{q\times m}{\bm{\Phi}^*} &amp; \underset{q\times p}{\bm{0}} &amp; \bm{I}_{q}
\end{bmatrix}
=
\begin{bmatrix}
-\underset{p \times m}{\bm{\Phi}}&amp; \bm{I}_{p} &amp;\underset{p\times q}{\bm{0}}\\
-\underset{q\times m}{\bm{\Phi}^*} &amp; \underset{q\times p}{\bm{0}} &amp; \bm{I}_{q},
\end{bmatrix}
=
\begin{bmatrix}
\overline{\bm{C}} \\ \underline{\bm{C}}
\end{bmatrix}
\tag{5}\]</span></span> where <span class="math inline">\(\overline{\bm{C}}\)</span> contains the first <span class="math inline">\(p\)</span> rows of <span class="math inline">\(\bm{C}^*\)</span> and <span class="math inline">\(\underline{\bm{C}}\)</span> contains the remaining <span class="math inline">\(q\)</span> rows of <span class="math inline">\(\bm{C}^*\)</span>, the forecast error variance matrix <span class="math display">\[
\operatorname{Var}(\hat{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*) = \bm{W}_h^* = \begin{bmatrix}\bm{W}_h &amp; \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* &amp; \bm{W}_{c,h}^*\end{bmatrix}.
\]</span> where <span class="math inline">\(\hat{\bm{z}}_{t+h}^*\)</span> is the <span class="math inline">\(h\)</span>-step-ahead base forecasts of <span class="math inline">\(\bm{z}_{t}^*\)</span>: <span class="math display">\[
\hat{\bm{z}}_{t+h}^* = \begin{bmatrix} \hat{\bm{z}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix},
\]</span> and the corresponding <span class="math display">\[
\bm{M}^* = \bm{I} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*.
\]</span></p>
<p>Proving <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a> requires proving the following two items.</p>
<ol type="1">
<li>Including additional components in the mapping without including corresponding component constraints is equivalent to not including these additional components at all.</li>
<li>For a fixed set of components to be included in the mapping, adding constraints will reduce forecast error variance.</li>
</ol>
<p>We start by proving the first statement. Consider the case where we include the additional series <span class="math inline">\(\bm{c}_{t}^*\)</span> without using the additional constraint <span class="math inline">\(\bm{\Phi}^*\)</span>. Defining <span class="math inline">\(\bm{M}^{+}\)</span> only with <span class="math inline">\(\overline{\bm{C}}\)</span>: <span id="eq-Mplus"><span class="math display">\[
\bm{M}^{+} = \bm{I}_{m+p+q} - \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}},
\tag{6}\]</span></span> we have <span class="math inline">\(\tilde{\bm{z}}_{t+h}^+ = \bm{M}^{+}\hat{\bm{z}}_{t+h}^*\)</span>. Further, we obtain <span class="math display">\[
\bm{W}_h^*\overline{\bm{C}}'
= \begin{bmatrix}
      \bm{W}_h &amp; \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* &amp; \bm{W}_{c, h}^*
    \end{bmatrix}
    \begin{bmatrix}
      \bm{C}'\\ \underset{q\times p}{\bm{0}} \\
    \end{bmatrix} \\
= \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
\]</span> and <span class="math display">\[
  \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
= \begin{bmatrix}
      \bm{C} &amp; \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix} \\
=\bm{C}\bm{W}_h\bm{C}',
\]</span> which gives <span class="math display">\[
\begin{aligned}
\bm{M}^{+}
&amp; = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
    (\bm{C}\bm{W}_h\bm{C}')^{-1}
    \begin{bmatrix}
      \bm{C} &amp; \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}\\
%&amp; = \bm{I}_{m+p+q} -
%    \begin{bmatrix}
%      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1} \\
%      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}
%    \end{bmatrix}
%    \begin{bmatrix}
%      \bm{C} &amp; \underset{p\times q}{\bm{0}} \\
%    \end{bmatrix} \\
&amp; = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} &amp; \bm{0} \\
      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} &amp; \bm{0}
    \end{bmatrix},
\end{aligned}
\]</span> and <span class="math display">\[
\begin{aligned}
\tilde{\bm{z}}_{t+h}^+
&amp; = \bm{M}^{+}\hat{\bm{z}}_{t+h}^* \\
&amp; = \left(
      \bm{I}_{m+p+q} -
      \begin{bmatrix}
        \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} &amp; \bm{0} \\
        \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} &amp; \bm{0}
      \end{bmatrix}
    \right)
    \begin{bmatrix} \hat{\bm{z}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix}\\
%&amp; = \begin{bmatrix}
%      \hat{\bm{z}}_{t+h} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}\\
%      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
%    \end{bmatrix} \\
&amp; = \begin{bmatrix}
      (\bm{I}_{n+p} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\hat{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix} \\
&amp; = \begin{bmatrix}
      \bm{M} \hat{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix} \\
&amp; = \begin{bmatrix}
      \tilde{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix}.
\end{aligned}
\]</span> If we only consider the forecast performance relevant to <span class="math inline">\(\bm{y}_{t+h}\)</span>, and define <span class="math inline">\(\bm{J}^* = \bm{J}_{m,p+q}= \begin{bmatrix}\bm{I}_{m} &amp; \bm{0}_{m\times(p+q)} \end{bmatrix}\)</span>, we have <span class="math display">\[
\tilde{\bm{y}}_{t+h}^+ = \bm{J}^*\tilde{\bm{z}}_{t+h}^+ = \bm{J}\tilde{\bm{z}}_{t+h} = \tilde{\bm{y}}_{t+h}.
\]</span> This means adding additional components without imposing the corresponding constraints will yield the same projected forecasts as if these additional components are not added, which implies that the forecast error variance stays the same: <span id="eq-acnc"><span class="math display">\[
\operatorname{Var}(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}) = \operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
\tag{7}\]</span></span> <!-- additional components no constraints --> This finishes the proof of the first statement. Now we move on to proving the second statement. We have the forecast error variance matrices <span class="math display">\[
\begin{aligned}
\operatorname{Var}(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) &amp; = \bm{M}^{+}\bm{W}_h^*
    = (\bm{I}_{m+p+q} -\bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
\text{and}\qquad\qquad
\operatorname{Var}(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*) &amp; = \bm{M}^*\bm{W}_h^*
    = (\bm{I}_{m+p+q} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*)\bm{W}_h^*.
\end{aligned}
\]</span> Taking the difference, we have <span class="math display">\[
\begin{aligned}
\operatorname{Var}(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) - \operatorname{Var}(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)
&amp; = (\bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
&amp; = \bm{W}_h^*(\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*.
\end{aligned}
\]</span> Using block matrix inversion, we have <span class="math display">\[
\begin{aligned}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
&amp; = \begin{bmatrix}
      \overline{\bm{C}}' &amp; \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' &amp; \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' &amp; \underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    \end{bmatrix}^{-1}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
&amp; = \begin{bmatrix}
      \overline{\bm{C}}' &amp; \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      a &amp; b \\
      c &amp; d
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
&amp; =   \overline{\bm{C}}'a \overline{\bm{C}}
    + \overline{\bm{C}}'b \underline{\bm{C}}
    + \underline{\bm{C}}'c\overline{\bm{C}}
    + \underline{\bm{C}}'d\underline{\bm{C}},
\end{aligned}
\]</span> where <span class="math display">\[
\begin{aligned}
a &amp;= (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
     (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
  &amp; \mbox{}\hspace*{1cm}
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' - \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}
    \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \\
  &amp; = (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
b &amp; = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' -
     \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
     \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}\\
  &amp; = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1},\\
c &amp; = - (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
d &amp; = (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}.
\end{aligned}
\]</span> Thus, <span class="math display">\[\begin{align*}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
&amp; =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
&amp; \mbox{}\hspace*{1cm} +
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
&amp; \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
&amp; \mbox{}\hspace*{1cm} -
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}'\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
&amp; \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
&amp;=
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
&amp; \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
&amp; \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
&amp; =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}.
\end{align*}\]</span> Therefore, <span class="math display">\[
\begin{aligned}
\operatorname{Var}(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) - \operatorname{Var}(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)
\hspace*{-4cm} \\
&amp; =
\bm{W}_h^*(
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
-\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*\\
&amp; =
\bm{W}_h^*(
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
)\bm{W}_h^*
\end{aligned}
\]</span> is positive semi-definite. This concludes the proof of the second statement. Combining the results above, we have <span id="eq-addcom"><span class="math display">\[
\begin{aligned}
\operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) -
\operatorname{Var}(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h})
&amp; =
\operatorname{Var}(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}) -
\operatorname{Var}(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}) \\
&amp;=
\bm{J}^*\operatorname{Var}(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*)\bm{J}^{*\prime} -
\bm{J}^*\operatorname{Var}(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)\bm{J}^{*\prime} \\
&amp;=
\bm{J}^*
\bm{W}_h^*
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
\bm{W}_h^*
\bm{J}^{*\prime}
\end{aligned}
\tag{8}\]</span></span> being positive semi-definite. Finally, we have <span class="math display">\[\begin{multline*}
(\operatorname{Var}(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\operatorname{Var}(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}))
-
(\operatorname{Var}(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})) \\
=
    \operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) -
    \operatorname{Var}(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h})
\end{multline*}\]</span> being a positive semi-definite matrix where the diagonal terms are non-negative, whose trace, therefore, is non-negative. This means using a larger number of components in the mapping achieves lower forecast error variances, giving <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a>.</p>
</section>
<p><a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a> is the key result that demonstrates the usefulness of FLAP. It means that we can keep increasing the number of components to reduce forecast error variance, even when the number of components exceeds the number of original series. It requires <span class="math inline">\(\bm{C}\)</span> to be <span class="math inline">\(\big[-\bm{\Phi} ~~~ \bm{I}_{p} \big]\)</span> or <span class="math inline">\(\big[-\bm{\Phi} ~~~ \bm{L}\big]\)</span> where <span class="math inline">\(\bm{L}\)</span> is a lower triangular matrix. This implies that the components can also be constructed from existing components, not only from the original series. This has little significance since a linear combination of components of the original series, is just a linear combination of the original series. This of course assumes that the forecast error covariances are known, something we explore in <a href="#sec-estW" class="quarto-xref">Section&nbsp;3.5</a> and <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>.</p>
<p>Extending the proof of <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a>, we can outline the condition for the reduced sum of forecast error variances to be positive. Denote <span class="math inline">\(\bm{\phi}_i\)</span> as the row vector containing the weights associated with the <span class="math inline">\(i\)</span>th component, so that with <span class="math inline">\(p\)</span> components, the weights matrix is <span class="math inline">\(\bm{\Phi} = \big[\bm{\phi}_1' ~~~ \bm{\phi}_2' ~~~ \dots ~~~ \bm{\phi}_p'\big]'\)</span>. Let <span class="math inline">\(\bm{W}^{(i-1)}_{\tilde{\bm{y}},h}\)</span> denote the error covariance matrix of the projected forecasts of the original series based on the first <span class="math inline">\(i-1\)</span> components, <span class="math inline">\(\bm{w}_{c_{1}\hat{\bm{y}},h}\)</span> denote a vector of covariances of the first component and the base forecasts of the original series, and <span class="math inline">\(\bm{w}_{c_i\tilde{y},h}^{(i-1)}\)</span> denote a vector of covariances of the projected <span class="math inline">\(i\)</span>th component and the projected forecasts of the original series, based on the first <span class="math inline">\(i-1\)</span> components.</p>
<div id="thm-pos-condition" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3 (Positive Forecast Error Variance Reduction Condition)</strong></span> For the first component to achieve a guaranteed reduction of forecast error variance (for the reduced variance matrix in <a href="#thm-psdvar" class="quarto-xref">Theorem&nbsp;1</a> to have positive trace), <span id="eq-pcon"><span class="math display">\[
\bm{\phi}_1\bm{W}_{y,h}\neq\bm{w}_{c_1y,h}.
\tag{9}\]</span></span> For the <span class="math inline">\(i\)</span>th component to have a positive reduction on forecast error variance of the original series, <span id="eq-pconi"><span class="math display">\[
\bm{\phi}_i{\bm{W}}^{(i-1)}_{\tilde{y},h}\neq\bm{w}_{c_i\tilde{y},h}^{(i-1)}.
\tag{10}\]</span></span></p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-thm-pos-condition" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-thm-pos-condition">Proof of <a href="#thm-pos-condition" class="quarto-xref">Theorem&nbsp;3</a></h3>
<p> Denote <span class="math inline">\(\bm{\psi}_i=\begin{bmatrix}-\bm{\phi}_i &amp; \bm{0}_{1\times(i-1)} &amp; 1\end{bmatrix}\)</span> and <span class="math inline">\(\bm{W}_{h}^{(i)}\)</span> to be the base forecast error variance of the original series and the first <span class="math inline">\(i\)</span> components. Starting with the first component, <a href="#eq-trdvar" class="quarto-xref">Equation&nbsp;4</a> becomes <span id="eq-trdvar1"><span class="math display">\[
\operatorname{tr}(\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1'(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}') =
(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}'\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1',
\tag{11}\]</span></span> <span class="math display">\[
\begin{aligned}
\text{where}\qquad\qquad
\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' =&amp;
\begin{bmatrix}
-\bm{\phi}_1 &amp; 1
\end{bmatrix}
\begin{bmatrix}
\bm{W}_{z,h} &amp; \bm{w}'_{c_1z,h} \\
\bm{w}_{c_1z,h} &amp; \bm{W}_{c_1,h}
\end{bmatrix}
\begin{bmatrix}
\bm{I}_m\\ 0
\end{bmatrix} \\
=&amp; -\bm{\phi}_1\bm{W}_{z,h} + \bm{w}_{c_1z,h}.
\end{aligned}
\]</span> <a href="#eq-trdvar1" class="quarto-xref">Equation&nbsp;11</a> is obviously non-negative. For it to be larger than <span class="math inline">\(0\)</span>, we need <span class="math inline">\(\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' \neq 0\)</span>, which gives <span class="math inline">\(\bm{\phi}_1\bm{W}_{z,h} \neq \bm{w}_{c_1z,h}\)</span>.</p>
<p>When it comes to adding the <span class="math inline">\(i\)</span>th component on top of the first <span class="math inline">\(i-1\)</span> components, we define <span class="math display">\[
\overline{\bm{C}}_i =
\begin{bmatrix}
\bm{\psi}_1 &amp; \bm{0}_{1\times i} \\
\bm{\psi}_2 &amp; \bm{0}_{1\times (i-1)} \\
\vdots &amp; \vdots \\
\bm{\psi}_i &amp; 0 \\
\end{bmatrix}
\]</span> and <span class="math display">\[
\bm{M}^+_i = \bm{I}_{m+i} - \bm{W}_h^{(i)} \overline{\bm{C}}_{i-1}'
(\overline{\bm{C}}_{i-1}\bm{W}_h^{(i)}\overline{\bm{C}}_{i-1}')^{-1}
\overline{\bm{C}}_{i-1}
\]</span> analogously to <a href="#eq-Cstar" class="quarto-xref">Equation&nbsp;5</a> and <a href="#eq-Mplus" class="quarto-xref">Equation&nbsp;6</a>. Following <a href="#eq-addcom" class="quarto-xref">Equation&nbsp;8</a>, the additional reduction of forecast error variance when adding the <span class="math inline">\(i\)</span>th component becomes <span class="math display">\[
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
=
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i
\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'.
\]</span> Similar to before, we would want <span class="math inline">\(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}' \neq \bm{0}\)</span>. Note that <span class="math inline">\(\bm{\psi}_i\)</span> concerns the first <span class="math inline">\(m\)</span> rows and the last row of <span class="math inline">\(\bm{M}^{+}_i\bm{W}_h^{(i)}\)</span>, and <span class="math inline">\(\bm{J}_{m,i}'\)</span> concerns the first <span class="math inline">\(m\)</span> columns. Combined with the implication from <a href="#eq-acnc" class="quarto-xref">Equation&nbsp;7</a> that the <span class="math inline">\(m\times m\)</span> leading principal submatrix in equation <span class="math inline">\(\bm{J}_{m,i}\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}'=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'\)</span> is the same, we suppress the straightforward yet tiresome details, and obtain <span class="math display">\[
\bm{\phi}_i\bm{W}_{\tilde{z},h}^{(i-1)} \neq \big[\bm{0}_{1\times m+i-1} ~~~ 1 \big]\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}',
\]</span> where <span class="math inline">\(\bm{W}_{\tilde{z},h}^{(i-1)}=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'\)</span> is the projected forecast error variance of the original series using the first <span class="math inline">\(i-1\)</span> components, and the right hand side of the inequality is simply a one-row matrix consisting of the first <span class="math inline">\(m\)</span> elements in the last row of <span class="math inline">\(\bm{M}^{+}_i\bm{W}_h^{(i)}\)</span>, which can be denoted as <span class="math inline">\(\bm{w}_{\tilde{c}_i\tilde{z},h}^{(i-1)}\)</span> and interpreted as the covariance between the projected forecast of the original series using the first <span class="math inline">\(i-1\)</span> components, and the projected forecast of the <span class="math inline">\(i\)</span>th component using the first <span class="math inline">\(i-1\)</span> components.</p>
</section>
<p>The condition of <a href="#eq-pcon" class="quarto-xref">Equation&nbsp;9</a> demonstrates that for a new component to be beneficial, the information introduced by this new component, reflected in the error covariance, cannot be a linear combination of already existing information.</p>
<p><a href="#thm-pos-condition" class="quarto-xref">Theorem&nbsp;3</a> can potentially provide insights into the selection of component weights and forecast models to satisfy the conditions. We leave this issue to a later article, as practically the conditions in <a href="#thm-pos-condition" class="quarto-xref">Theorem&nbsp;3</a> are either almost always satisfied if the weights are simulated randomly on a continuous scale, or the loss associated with the rare occasions where the conditions are not satisfied is negligible compared to the estimation error imposed by the limited sample size as the number of components increases, as discussed in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a> and <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>.</p>
</section>
<section id="optimality-of-the-projection" class="level2">
<h2 class="anchored" data-anchor-id="optimality-of-the-projection">Optimality of the projection</h2>
<p><a href="#eq-z_tilde" class="quarto-xref">Equation&nbsp;1</a> can be seen as a solution to the optimisation problem: <span class="math display">\[
\underset{\check{\bm{z}}_{T+h}}{\arg\min}
  (\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})
\qquad \text{s.t. } \bm{C}\check{\bm{z}}_{T+h}=0.
\]</span> If we consider the transformed space where all the vectors are first transformed via pre-multiplying by <span class="math inline">\(\bm{W}_h^{-1/2}\)</span>, where <span class="math inline">\(\bm{W}_h^{-1} = (\bm{W}_h^{-1/2})'\bm{W}_h^{-1/2}\)</span>, then this optimisation problem can be interpreted as finding the set of forecasts that are closest to the base forecasts on the transformed space, while satisfying the linear constraints imposed by the components.</p>
<p>Moreover, this is equivalent to the optimisation problem: <span class="math display">\[
\underset{\check{\bm{z}}_{T+h}}{\arg\min} (\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})
\qquad \text{s.t. } \bm{\Phi}\check{\bm{y}}_{T+h}=\check{\bm{c}}_{T+h},
\]</span> where <span class="math inline">\(\check{\bm{c}}_{T+h}\)</span> is the vector of the last <span class="math inline">\(p\)</span> elements of <span class="math inline">\(\check{\bm{z}}_{T+h}\)</span>, corresponding to the forecast of the components as part of the solution. This equivalence is discussed in <span class="citation" data-cites="WicEtAl2019">@WicEtAl2019</span>, where the authors find the solution by minimising the sum of forecast error variance of all series (See <span class="citation" data-cites="AndNar2022">@AndNar2022</span> for a simpler proof). The result is <span id="eq-mint"><span class="math display">\[
\tilde{\bm{z}}_{t+h} = \bm{S}\bm{G} \hat{\bm{z}}_{t+h},
\tag{12}\]</span></span> where <span class="math inline">\(\bm{S} = \begin{bmatrix}\bm{I}_m \\\bm{\Phi}\end{bmatrix}\)</span> contains the constraints, so that <span class="math inline">\(\bm{z}_{t} = \bm{S}\bm{y}_{t}\)</span>, and<span id="eq-G"><span class="math display">\[
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
\tag{13}\]</span></span> In <a href="#eq-mint" class="quarto-xref">Equation&nbsp;12</a>, <span class="math inline">\(\bm{G} \hat{\bm{z}}_{t+h}\)</span> can be viewed as mapping of all the series to a selected few. In the forecast reconciliation context, this is a mapping of all series to the “bottom level”. In our multivariate forecasting context, this is a mapping of all series including the components, to the space of the original series. This leads to the solution <span id="eq-stmap"><span class="math display">\[
\tilde{\bm{y}}_{t+h} = \bm{G}\hat{\bm{z}}_{t+h},
\tag{14}\]</span></span> as equivalent to <a href="#eq-lcmap" class="quarto-xref">Equation&nbsp;3</a>. Recognising that <a href="#eq-mint" class="quarto-xref">Equation&nbsp;12</a> is equivalent to <a href="#eq-z_tilde" class="quarto-xref">Equation&nbsp;1</a>, it is the solution that minimises the sum of forecast error variances of the original series and all the components. We go further in <a href="#thm-minvar" class="quarto-xref">Theorem&nbsp;4</a>, and show that <a href="#eq-mint" class="quarto-xref">Equation&nbsp;12</a> is also the solution to minimise each individual forecast error variance of the original series, and their sum. This can be viewed as a special case of Theorem 3.3 in <span class="citation" data-cites="PanEtAl2021">@PanEtAl2021</span>, or as illustrated by <span class="citation" data-cites="AndNar2022">@AndNar2022</span>, but applied in a different context to forecast reconciliation. The earliest work we can find that noted this interpretation in a non-forecasting context is <span class="citation" data-cites="Lue1969">@Lue1969 [p.85]</span>. We establish a few basic results leading to the optimality of this solution first, also to check that <a href="#lem-projectionM" class="quarto-xref">Lemma&nbsp;1</a>, <a href="#cor-corM" class="quarto-xref">Corollary&nbsp;1</a> and <a href="#lem-var" class="quarto-xref">Lemma&nbsp;2</a> hold under this alternative representation.</p>
<div id="lem-projectionG" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 3</strong></span> The matrix <span class="math inline">\(\bm{S}\bm{G}\)</span> is a projection onto the space where the constraint <span class="math inline">\(\bm{C}\bm{z}_t=\bm{0}\)</span> is satisfied, provided that <span class="math inline">\(\bm{G}\bm{S}=\bm{I}\)</span>.</p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-lem-projectiong" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-lem-projectiong">Proof of <a href="#lem-projectionG" class="quarto-xref">Lemma&nbsp;3</a></h3>
<p> If <span class="math inline">\(\bm{G}\bm{S}=\bm{I}\)</span>, <span class="math inline">\(\bm{S}\bm{G}\)</span> is a projection matrix: <span class="math inline">\(\bm{S}\bm{G}\bm{S}\bm{G} = \bm{S}\bm{G}\)</span>.</p>
<p>For any <span class="math inline">\(\bm{z}\)</span> such that <span class="math inline">\(\bm{S}\bm{G}\bm{z} = \bm{y}\)</span> for some <span class="math inline">\(\bm{y}\)</span>, we have <span class="math inline">\(\bm{C}\bm{y} = \bm{C}\bm{S}\bm{G}\bm{z} = \bm{0}\)</span> because <span class="math inline">\(\bm{C}\bm{S} = \big[-\bm{\Phi} ~~~ \bm{I} \big] \big[\bm{I} ~~~ \bm{\Phi}' \big]' = \bm{0}\)</span>. Similarly to <span class="math inline">\(\bm{M}\)</span>, <span class="math inline">\(\bm{S}\bm{G}\)</span> projects a vector to the same space where <span class="math inline">\(\bm{C}\)</span> is satisfied.</p>
</section>
<div id="cor-corG" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 2</strong></span> Provided that <span class="math inline">\(\bm{G}\bm{S} = \bm{I}\)</span>, the following results hold.</p>
<ol type="1">
<li>The projected forecast in <a href="#eq-stmap" class="quarto-xref">Equation&nbsp;14</a> satisfies the constraint <span class="math inline">\(\bm{C}\tilde{\bm{z}}_{t+h}= \bm{C}\bm{S}\tilde{\bm{y}}_{t+h}= \bm{0}\)</span>.</li>
<li>For <span class="math inline">\(\bm{z}_{t+h}\)</span> that already satisfies the constraint, the mapping does not change its value, i.e., <span class="math inline">\(\bm{G}\bm{z}_{t+h} = \bm{y}_{t+h}\)</span>.</li>
<li>If the base forecasts are unbiased such that <span class="math inline">\(\operatorname{E}(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t)\)</span>, then the projected forecasts in <a href="#eq-stmap" class="quarto-xref">Equation&nbsp;14</a> are also unbiased: <span class="math inline">\(\operatorname{E}(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{y}_{t+h}|\mathcal{I}_t)\)</span>.</li>
</ol>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-cor-corg" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-cor-corg">Proof of <a href="#cor-corG" class="quarto-xref">Corollary&nbsp;2</a></h3>
<p> Item 1 is an direct application of <a href="#lem-projectionG" class="quarto-xref">Lemma&nbsp;3</a>. From <a href="#lem-projectionG" class="quarto-xref">Lemma&nbsp;3</a> and Lemma 2.4 in <span class="citation" data-cites="Rao1974">@Rao1974</span>, we have <span class="math display">\[
\bm{S}\bm{G}\bm{z}_{t+h} = \bm{z}_{t+h} = \bm{S}\bm{y}_{t+h}.
\]</span> Left multiplying by <span class="math inline">\(\bm{G}\)</span> on both sides, we have <span class="math inline">\(\bm{G}\bm{z}_{t+h} = \bm{y}_{t+h}\)</span> and item 2 is proven. To prove Item 3, we have <span class="math display">\[
\operatorname{E}(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) =
\operatorname{E}(\bm{G}\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{G}\operatorname{E}(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{G}\operatorname{E}(\bm{z}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{G}\bm{z}_{t+h}|\mathcal{I}_t) = \operatorname{E}(\bm{y}_{t+h}|\mathcal{I}_t).
\]</span></p>
</section>
<div id="lem-st-var" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 4</strong></span> The covariance matrix of the projected forecasts from <a href="#eq-stmap" class="quarto-xref">Equation&nbsp;14</a> is given by <span class="math display">\[
\operatorname{Var}(\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}) = \bm{G}\bm{W}_h\bm{G}'.
\]</span></p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-lem-st-var" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-lem-st-var">Proof of <a href="#lem-st-var" class="quarto-xref">Lemma&nbsp;4</a></h3>
<p> Let the base and projected forecast errors be given as <span class="math display">\[
\begin{aligned}
\hat{\bm{e}}_{y, t+h} &amp;= \bm{y}_{t+h} - \hat{\bm{y}}_{t+h},\\
\hat{\bm{e}}_{z, t+h} &amp;= \bm{z}_{t+h} - \hat{\bm{z}}_{t+h},\\
\tilde{\bm{e}}_{y, t+h} &amp;= \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h},\\
\text{and}\qquad
\tilde{\bm{e}}_{z, t+h} &amp;= \bm{z}_{t+h} - \tilde{\bm{z}}_{t+h}
= \bm{S}\bm{y}_{t+h} - \bm{S}\tilde{\bm{y}}_{t+h}
= \bm{S} \tilde{\bm{e}}_{y, t+h}. \\
\text{Then we have}\qquad
\tilde{\bm{e}}_{z, t+h}
&amp;= \hat{\bm{e}}_{z, t+h} + \hat{\bm{z}}_{t+h} - \tilde{\bm{z}}_{t+h}\\
&amp;= \hat{\bm{e}}_{z, t+h} + \hat{\bm{z}}_{t+h} - \bm{S}\bm{G}\hat{\bm{z}}_{t+h}\\
&amp;= \hat{\bm{e}}_{z, t+h} + (\bm{I} - \bm{S}\bm{G})(\bm{z}_{t+h} - \hat{\bm{e}}_{z, t+h})\\
\text{and}\qquad
&amp;= \bm{S}\bm{G}\hat{\bm{e}}_{z, t+h} + (\bm{I} - \bm{S}\bm{G})\bm{S}\bm{y}_{t+h}\\
\bm{S} \tilde{\bm{e}}_{y, t+h} &amp;= \bm{S}\bm{G}\hat{\bm{e}}_{z, t+h},
\end{aligned}
\]</span> where the last line comes from <span class="math inline">\(\bm{G}\bm{S} = \bm{I}\)</span>. Left multiplying by <span class="math inline">\(\bm{G}\)</span> on both sides, we have <span class="math display">\[
\bm{G}\bm{S} \tilde{\bm{e}}_{y, t+h} = \bm{G}\bm{S}\bm{G}\hat{\bm{e}}_{z, t+h}
\qquad\text{and}\qquad
\tilde{\bm{e}}_{y, t+h} = \bm{G}\hat{\bm{e}}_{z, t+h},
\]</span> and therefore <span class="math display">\[
\operatorname{Var}(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) =\operatorname{Var}(\tilde{\bm{e}}_{y, t+h})=\operatorname{Var}(\bm{G}\hat{\bm{e}}_{z, t+h})= \bm{G}\operatorname{Var}(\hat{\bm{e}}_{z, t+h})\bm{G}'= \bm{G}\bm{W}_h\bm{G}'.
\]</span></p>
</section>
<p>We are now ready to present the following theorem.</p>
<div id="thm-minvar" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4 (Minimum Variance Unbiased Projected Forecast)</strong></span> The solution to <span id="eq-obj"><span class="math display">\[
\underset{\bm{G}}{\arg\min}\ \operatorname{tr}(\bm{G}\bm{W}_h\bm{G}')
\qquad \text{s.t. } \bm{G}\bm{S} = \bm{I}
\tag{15}\]</span></span> is <a href="#eq-G" class="quarto-xref">Equation&nbsp;13</a>. This problem can be effectively split into independent subproblems such that <span class="math inline">\(\bm{G} = \big[\bm{g}_1 ~~ \bm{g}_2 ~~ \dots ~~ \bm{g}_m\big]'\)</span>, where <span class="math inline">\(\bm{g}_i\)</span> is the solution to the subproblem of the <span class="math inline">\(i\)</span>th series <span id="eq-subobj"><span class="math display">\[
\underset{\bm{g}_i}{\arg\min}\ \bm{g}_i'\bm{W}_h\bm{g}_i
\qquad \text{s.t. } \bm{g}_i'\bm{s}_{j} = \delta_{ij}, \quad j= 1, 2, .\ldots, m,
\tag{16}\]</span></span> where <span class="math inline">\(\bm{s}_j\)</span> is the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(\bm{S}\)</span>, and <span class="math inline">\(\delta_{ij}\)</span> is the Kronecker delta function taking value 1 if <span class="math inline">\(i = j\)</span> and 0 otherwise.</p>
</div>
<p>Proof in Appendix, page .</p>
<section id="proof-of-thm-minvar" class="level3 toappendix">
<h3 class="anchored" data-anchor-id="proof-of-thm-minvar">Proof of <a href="#thm-minvar" class="quarto-xref">Theorem&nbsp;4</a></h3>
<p> This can be proved in a few different ways. We adopt the approach of <span class="citation" data-cites="AndNar2022">@AndNar2022</span> to obtain the solution to <a href="#eq-obj" class="quarto-xref">Equation&nbsp;15</a>, but the procedure from <span class="citation" data-cites="Lue1969">@Lue1969 [p. 85]</span> can also be used, where the problem is divided to <a href="#eq-subobj" class="quarto-xref">Equation&nbsp;16</a> and reconstructed to find the solution to <a href="#eq-obj" class="quarto-xref">Equation&nbsp;15</a>.</p>
<p>There exists a Lagrange multiplier <span class="math inline">\(\bm{\Lambda}\)</span> such that <span class="math display">\[
L(\bm{G}) = \operatorname{tr}(\bm{G}\bm{W}_h\bm{G}') + \operatorname{tr}(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S}))
\]</span> is stationary at an extremum <span class="math inline">\(\bm{G}\)</span> <span class="citation" data-cites="Lue1969">[@Lue1969, p. 243, Theorem 1]</span>. We set the Gateaux differential <span class="citation" data-cites="Lue1969">[@Lue1969, p. 171]</span> to zero for any matrix <span class="math inline">\(\bm{H}\)</span>: <span class="math display">\[
\begin{aligned}
\lim_{\alpha \to 0} \frac{L(\bm{G} + \alpha\bm{H}) - L(\bm{G})}{\alpha} &amp;= 0 \\
\operatorname{tr}(\bm{G}\bm{W}_h\bm{H}') + \operatorname{tr}(\bm{H}\bm{W}_h\bm{G}') - \operatorname{tr}(\bm{\Lambda}'(\bm{H}\bm{S}))
  &amp;=\operatorname{tr}(2\bm{H}\bm{W}_h\bm{G}'-\bm{\Lambda}'\bm{H}\bm{S})\\
  &amp;= \operatorname{tr}(\bm{H}(2\bm{W}_h\bm{G}'-\bm{S}\bm{\Lambda}')) \\
  &amp;=0 \\
2\bm{W}_h\bm{G} &amp;= \bm{S}\bm{\Lambda}' \\
\bm{G}' &amp;= \frac{1}{2}\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'.
\end{aligned}
\]</span> Multiplying <span class="math inline">\(\bm{S}'\)</span> to the left of both sides. we have <span class="math display">\[
\bm{S}'\bm{G}'=\bm{I} = \frac{1}{2}\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'
\qquad\text{and}\qquad
\bm{\Lambda}' = 2 (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
\]</span> because <span class="math inline">\(\bm{G}\bm{S} = \bm{I}\)</span>. Putting it back in, we have <span class="math display">\[
\bm{G}' = \bm{W}_h^{-1}\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
\qquad\text{and}\qquad
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
\]</span></p>
</section>
<p>In other words, the forecast projection method gives optimal projected forecast for a given set of components, in the sense that the unbiased forecast of each series has minimum variance.</p>
</section>
<section id="sec-estW" class="level2">
<h2 class="anchored" data-anchor-id="sec-estW">Estimation of <span class="math inline">\(\bm{W}_h\)</span></h2>
<p>In practice, the base forecast error variance <span class="math inline">\(\bm{W}_h\)</span> is unknown and needs to be estimated. Denote <span class="math inline">\(\hat{\bm{e}}_{t,h} = \bm{z}_{t} - \hat{\bm{z}}_{t|t-h}\)</span> as the <span class="math inline">\(h\)</span>-step-ahead base forecast in-sample residual. The conventional forecast error variance matrix estimator <span class="math display">\[
\widehat{\bm{W}}_h=\frac{1}{T-h-1}\sum^T_{t=h+1}\hat{\bm{e}}_{t,h}\hat{\bm{e}}_{t,h}',
\]</span> albeit unbiased, is not considered a good approximation to the true forecast error variance in a finite sample when <span class="math inline">\((m+p) \approx T-h\)</span>. It is even singular when <span class="math inline">\((m+p)&gt;T-h\)</span>, which makes the quantities discussed in the previous sections impossible to calculate. For this reason, while other shrinkage estimators can be considered, we adopt the covariance shrinkage method of <span class="citation" data-cites="SchStr2005">@SchStr2005</span> and the variance shrinkage method of <span class="citation" data-cites="OpgStr2007">@OpgStr2007</span>. Then, the estimated forecast error variance matrix is guaranteed to be positive definite with few numerical problems. This estimator is denoted as <span class="math inline">\(\widehat{\bm{W}}_h^{shr} = (\hat{w}_{ij,h}^{shr})_{1\leq i,j\leq m+p}\)</span> with the element in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> <span class="math display">\[
\hat{w}_{ij,h}^{shr} = \hat{r}_{ij,h}^{shr}\sqrt{\hat{v}_{i,h}\hat{v}_{j,h}},
\]</span> where <span class="math inline">\(\hat{r}_{ij,h}^{shr} = (1-\hat{\lambda}_{cor})\hat{r}_{ij,h}\)</span> and <span class="math inline">\(\hat{v}_{i,h} = \hat{\lambda}_{var}\hat{w}_{h, median} + (1-\hat{\lambda}_{var})\hat{w}_{i, h}\)</span>, with <span class="math inline">\(\hat{\lambda}_{cor}\)</span> being the shrinkage intensity parameter for the correlation <span class="math display">\[
\hat{\lambda}_{cor} =
\min\left(1,
\frac
{\sum_{i\neq j}\widehat{\operatorname{var}}(\hat{r}_{ij,h})}
{\sum_{i\neq j}\hat{r}_{ij,h}^2}
\right),
\]</span> and <span class="math inline">\(\hat{\lambda}_{var}\)</span> being the shrinkage intensity parameter for the variance <span class="math display">\[
\hat{\lambda}_{var} =
\min\left(1,
\frac
{\sum_{i=1}^{m+p}\widehat{\operatorname{var}}(\hat{w}_{i,h})}
{\sum_{i=1}^{m+p}(\hat{w}_{i, h} - \hat{w}_{h, median})^2}
\right),
\]</span> <span class="math inline">\(\hat{r}_{ij,h}\)</span> the sample correlation of the <span class="math inline">\(h\)</span>-step-ahead forecast error between the <span class="math inline">\(i\)</span>th and the <span class="math inline">\(j\)</span>th series (component) in <span class="math inline">\(\bm{z}_t\)</span>, <span class="math inline">\(\hat{w}_{i, h}\)</span> the <span class="math inline">\(h\)</span>-step-ahead sample base forecast error variance associated with the <span class="math inline">\(i\)</span>th series (the <span class="math inline">\(i\)</span>th diagonal element of <span class="math inline">\(\widehat{\bm{W}}_h\)</span>), and <span class="math inline">\(\hat{w}_{h, median}\)</span> the median of the <span class="math inline">\(h\)</span>-step-ahead sample forecast error variance of the series and components (the median of the diagonal elements of <span class="math inline">\(\widehat{\bm{W}}_h\)</span>). The estimation of <span class="math inline">\(\widehat{\bm{W}}_h^{shr}\)</span> in the following sections are implemented using the package <code>corpcor</code> <span class="citation" data-cites="corpcor">[@corpcor]</span> in R <span class="citation" data-cites="R">[@R]</span>.</p>
<p>Estimating <span class="math inline">\(\widehat{\bm{W}}_h^{shr}\)</span> for each forecast horizon <span class="math inline">\(h\)</span> is desirable but computationally intensive. It involves the calculation of multi-step-ahead in-sample residuals of the forecast models, which is especially challenging for iterative forecasts. Because of this, in practice it is not unreasonable to assume the <span class="math inline">\(h\)</span>-step forecast error variance is proportional to the <span class="math inline">\(1\)</span>-step forecast error variance by a constant <span class="math inline">\(\eta_h\)</span>, as do <span class="citation" data-cites="WicEtAl2019">@WicEtAl2019</span>: <span class="math display">\[
\widehat{\bm{W}}_h^{shr} = \eta_h\widehat{\bm{W}}_1^{shr}.
\]</span> Under this assumption, when <span class="math inline">\(\widehat{\bm{W}}_h^{shr}\)</span> is used in <a href="#eq-M" class="quarto-xref">Equation&nbsp;2</a>, the proportionality constant <span class="math inline">\(\eta_h\)</span> cancels out regardless of the value of <span class="math inline">\(h\)</span>. We can effectively use only the one-step forecast error variance in forecast projection, if we only need point forecasts. We calculate <span class="math inline">\(\widehat{\bm{W}}_h^{shr}\)</span> for each <span class="math inline">\(h\)</span> for the simulation example in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>, but assume this proportionality for the application in <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>.</p>
</section>
</section>
<section id="sec-simulation" class="level1">
<h1>Simulation</h1>
<p>In this section, we illustrate the performance of FLAP in a simulation setting. We generate time series of length <span class="math inline">\(T=400\)</span> from a <span class="math inline">\(m=70\)</span> variable VAR(<span class="math inline">\(3\)</span>) data generating process (GDP). The coefficients for the VAR DGP are estimated from the first <span class="math inline">\(70\)</span> series in the Australian tourism data set used in <a href="#sec-australian-domestic-tourism" class="quarto-xref">Section&nbsp;5.1</a>. The innovations are simulated from a multivariate normal distribution with an identity covariance matrix. The estimation and simulation is performed using the <code>tsDyn</code> package <span class="citation" data-cites="tsDyn">[@tsDyn]</span>.</p>
<p>For each simulated sample we generate <span class="math inline">\(h=1\)</span> to <span class="math inline">\(12\)</span>-step-ahead base forecasts from benchmark models. We implement FLAP with a varying number of components and component construction methods. This process is repeated <span class="math inline">\(220\)</span> times. The improvements of FLAP forecasts over base forecasts is assessed, and the statistical significance of these improvements is evaluated.</p>
<section id="sec-benchmarks" class="level2">
<h2 class="anchored" data-anchor-id="sec-benchmarks">Generating base and FLAP forecasts</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>pa_simulation <span class="ot">&lt;-</span> <span class="cf">function</span>(...) <span class="fu">pa</span>(<span class="st">"simulation"</span>, <span class="st">"projection"</span>, <span class="st">"output"</span>, ...)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">70</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">300</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We generate base forecasts from two benchmarks. The first benchmark is the univariate ARIMA model. For each series, we generate base forecasts from an ARIMA model using the <code>auto.arima()</code> function from the <code>forecast</code> package <span class="citation" data-cites="forecast">[@forecast]</span> with the default settings.</p>
<!-- The function implements an automatic model selection procedure proposed by @HynKha2008. The number of first differences is determined by repeated KPSS tests [@KwiEtAl1992] and the number of seasonal differences is determined by the seasonal strength computed from an STL decomposition [@CleEtAl1990;@fpp3]. The algorithm then chooses different orders of the autoregressive (AR) and moving average (MA) parts by comparing AICc between the corresponding models in a stepwise fashion, up to a maximal order of $5$. Univariate ARIMAs are also used to produce base forecasts of the components used in projection, including the cases where the base forecasts of the series are produced by a different model. -->
<p>The second benchmark is the dymanic factor model (DFM). Following <span class="citation" data-cites="StoWat2002a">@StoWat2002a</span>, base forecasts <span class="math display">\[
\hat{y}_{T+h} = \hat{\alpha}_h + \sum^n_{j=1}\hat{\bm{\beta}}_{hj}'\hat{\bm{F}}_{T-j+1} + \sum^s_{j=1}\hat{\gamma}_{hj}y_{T-j+1},
\]</span> where <span class="math inline">\(\hat{\bm{F}}_t\)</span> is the vector of <span class="math inline">\(k\)</span> estimated factors, and <span class="math inline">\(\hat{y}_t\)</span> is the target series to forecast. The factors are estimated using PCA on demeaned and scaled data. The optimal model is selected for each series based on the Bayesian information criterion (BIC) from models fitted using different combinations of meta-parameters in their corresponding range: <span class="math inline">\(1 \leq k \leq 6\)</span>, <span class="math inline">\(1 \leq n \leq 3\)</span> and <span class="math inline">\(1 \leq s \leq 3\)</span>. We note that the DFM produces direct forecasts in the sense that a different model is fitted for each forecast horizon <span class="math inline">\(h\)</span>, in contrast to indirect or iterative forecasts generated by the ARIMA models.</p>
<!-- ## Generating FLAP forecasts {#sec-flap} -->
<p>We use several sets of weights to construct linear components for the FLAP method. The types of components are listed in <a href="#tbl-comp" class="quarto-xref">Table&nbsp;1</a>. Principal components from PCA are established using the <code>prcomp</code> function in the <code>stats</code> package <span class="citation" data-cites="R">[@R]</span>. We generate random components using weights simulated from a standard normal (Norm) distribution and a uniform (Unif) distribution with range <span class="math inline">\((-1, 1)\)</span>. We normalise the weights of all randomly generated components into unit vectors to maintain some level of consistency, with <span class="math inline">\(\bm{\phi}_i/\sqrt{\sum_j(\phi_{ij}^2)}\)</span> where <span class="math inline">\(\phi_{ij}\)</span> is the <span class="math inline">\(j\)</span>th value in the weight vector of the <span class="math inline">\(i\)</span>th component. The total number of components <span class="math inline">\(p\)</span> is selected to be either <span class="math inline">\(m=70\)</span> or <span class="math inline">\(300\)</span>.</p>
<div id="tbl-comp" class="quarto-float anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1: Component construction methods for FLAP
</figcaption>
<div aria-describedby="tbl-comp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 83%">
</colgroup>
<thead>
<tr class="header">
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>PCA</td>
<td>Principal components from PCA.</td>
</tr>
<tr class="even">
<td>Norm</td>
<td>The weights of all components are simulated from a standard normal distribution.</td>
</tr>
<tr class="odd">
<td>Unif</td>
<td>The weights of all components are simulated from a uniform distribution</td>
</tr>
<tr class="even">
<td>PCA+Norm</td>
<td><span class="math inline">\(m\)</span> principal components from PCA are complemented with random components whose weights are simulated from a standard normal distribution.</td>
</tr>
<tr class="odd">
<td>PCA+Unif</td>
<td><span class="math inline">\(m\)</span> principal components from PCA are complemented with random components whose weights are simulated from a uniform distribution.</td>
</tr>
<tr class="even">
<td>Ortho</td>
<td>A random orthonormal matrix is generated using package <code>pracma</code> <span class="citation" data-cites="pracma">[@pracma]</span> as the weight matrix.</td>
</tr>
<tr class="odd">
<td>Ortho+Norm</td>
<td>A random orthonormal matrix is generated using package <code>pracma</code>, forming the weights of the first <span class="math inline">\(m\)</span> components. Weights of the additional components are simulated from a standard normal distribution.</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</section>
<section id="forecast-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="forecast-evaluation">Forecast evaluation</h2>
<p>We employ the Friedman test <span class="citation" data-cites="Fri1937 Fri1939">[@Fri1937; @Fri1939]</span> along with post-hoc Nemenyi tests <span class="citation" data-cites="Nem1963 HolEtAl2013">[@Nem1963;@HolEtAl2013]</span> using the <code>tsutils</code> package <span class="citation" data-cites="tsutils">[@tsutils]</span> to compare forecast performance between different methods. The analysis involves the use of Multiple Comparisons with the Best (MCB) plot introduced by <span class="citation" data-cites="KonEtAl2005">@KonEtAl2005</span> to visualise the comparison. The MSE of each series over different samples is calculated, and the MSEs of all the series are treated as observations in the Nemenyi test. The average ranks are plotted in <a href="#fig-simulation-mcb-series" class="quarto-xref">Figure&nbsp;1</a> for forecast horizons <span class="math inline">\(1\)</span>, <span class="math inline">\(6\)</span> and <span class="math inline">\(12\)</span>. The methods using FLAP are labelled “Model – Component Weights – Number of Components”. The benchmarks are labelled “Model – Benchmark”. These points are marked with triangles. The shaded region is the confidence interval of the best-performing method. Methods outside the shaded region are significantly worse than the best model.</p>
<p>In <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a>, we plot the out-of-sample MSE values as a function of the number of components <span class="math inline">\(p\)</span> included in the FLAP method. Here we also include the performance of the DGP VAR model (VAR – DGP), and an estimated VAR model with the correct specification (VAR – Est.), as well as their corresponding FLAP generated forecasts. We do not include FLAP forecasts using components generated from a uniform distribution or random orthonormal matrices, as they are visually identical to the methods with random weights generated from a standard normal distribution. The vertical black line indicates when <span class="math inline">\(p=m=70\)</span>, the number of original series.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ?tsutils::nemenyi</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># dir(pa_simulation())</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>mse_arima_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_arima_series.qs"</span>))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>mse_dfm_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_dfm_series.qs"</span>))</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>mse_var_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_var_series.qs"</span>))</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_arima_normal_series.qs"</span>))</span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_ortho_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_arima_ortho_normal_series.qs"</span>))</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_arima_series.qs"</span>))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_uniform_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_arima_pca_uniform_series.qs"</span>))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_uniform_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_arima_uniform_series.qs"</span>))</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_proj_dfm_series.qs"</span>))</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series_m <span class="ot">&lt;-</span> mse_proj_arima_normal_series[[m]]</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_ortho_normal_series_m <span class="ot">&lt;-</span> mse_proj_arima_ortho_normal_series[[m]]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_series_m <span class="ot">&lt;-</span> mse_proj_arima_series[[m]]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_uniform_series_m <span class="ot">&lt;-</span> mse_proj_arima_pca_uniform_series[[m]]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_uniform_series_m <span class="ot">&lt;-</span> mse_proj_arima_uniform_series[[m]]</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_series_m <span class="ot">&lt;-</span> mse_proj_dfm_series[[m]]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series_p <span class="ot">&lt;-</span> mse_proj_arima_normal_series[[p]]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_ortho_normal_series_p <span class="ot">&lt;-</span> mse_proj_arima_ortho_normal_series[[p]]</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_series_p <span class="ot">&lt;-</span> mse_proj_arima_series[[p]]</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_uniform_series_p <span class="ot">&lt;-</span> mse_proj_arima_pca_uniform_series[[p]]</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_uniform_series_p <span class="ot">&lt;-</span> mse_proj_arima_uniform_series[[p]]</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_series_p <span class="ot">&lt;-</span> mse_proj_dfm_series[[p]]</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>name_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_arima_series =</span> <span class="st">"ARIMA-Benchmark"</span>,</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_dfm_series =</span> <span class="st">"DFM-Benchmark"</span>,</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mse_var_series = "VAR-Base",</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_normal_series_m =</span> <span class="st">"ARIMA-Norm-m"</span>,</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_ortho_normal_series_m =</span> <span class="st">"ARIMA-Ortho-m"</span>,</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_series_m =</span> <span class="st">"ARIMA-PCA-m"</span>,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mse_proj_arima_pca_uniform_series_m = "ARIMA-PCA+Unif-m",</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_uniform_series_m =</span> <span class="st">"ARIMA-Unif-m"</span>,</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_dfm_series_m =</span> <span class="st">"DFM-PCA-m"</span>,</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_normal_series_p =</span> <span class="st">"ARIMA-Norm-300"</span>,</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_ortho_normal_series_p =</span> <span class="st">"ARIMA-Ortho+Norm-300"</span>,</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_series_p =</span> <span class="st">"ARIMA-PCA+Norm-300"</span>,</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_pca_uniform_series_p =</span> <span class="st">"ARIMA-PCA+Unif-300"</span>,</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_uniform_series_p =</span> <span class="st">"ARIMA-Unif-300"</span>,</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_dfm_series_p =</span> <span class="st">"DFM-PCA+Norm-300"</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>mse_series_mat_ls <span class="ot">&lt;-</span> <span class="fu">lst</span>(<span class="sc">!!!</span><span class="fu">syms</span>(<span class="fu">names</span>(name_vec))) <span class="sc">%&gt;%</span></span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list2array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aperm</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>  <span class="fu">array2list</span>()</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="co"># tsutils::nemenyi(mse_series_mat_ls[[1]], plottype = "vmcb")</span></span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>mcb_df_ls <span class="ot">&lt;-</span>  mse_series_mat_ls <span class="sc">%&gt;%</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(tsutils<span class="sc">::</span>nemenyi, <span class="at">plottype =</span> <span class="st">"none"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(\(x) <span class="fu">as.data.frame</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> name_vec[name]) <span class="sc">%&gt;%</span></span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">paste</span>(name, <span class="fu">format</span>(<span class="fu">round</span>(value, <span class="dv">2</span>), <span class="at">width =</span> <span class="dv">5</span>, <span class="at">nsmall =</span> <span class="dv">2</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">col =</span></span>
<span id="cb9-59"><a href="#cb9-59" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb9-60"><a href="#cb9-60" aria-hidden="true" tabindex="-1"></a>                       u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>l) <span class="sc">|</span></span>
<span id="cb9-61"><a href="#cb9-61" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb9-62"><a href="#cb9-62" aria-hidden="true" tabindex="-1"></a>                       l[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u)</span>
<span id="cb9-63"><a href="#cb9-63" aria-hidden="true" tabindex="-1"></a>           ))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Registered S3 method overwritten by 'quantmod':
  method            from
  as.zoo.data.frame zoo </code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plot_mcb_series <span class="ot">&lt;-</span> mcb_df_ls <span class="sc">%&gt;%</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(<span class="at">.id =</span> <span class="st">"h"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">h =</span> <span class="fu">as.integer</span>(h)) <span class="sc">%&gt;%</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">paste0</span>(<span class="fu">sprintf</span>(<span class="st">"%002d"</span>, h), name)) <span class="sc">%&gt;%</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">factor</span>(name,<span class="fu">unique</span>(name), <span class="at">ordered =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(h) <span class="sc">%&gt;%</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_mcb</span>(<span class="st">"Benchmark"</span>) <span class="sc">+</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="st">"h"</span>, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">labeller =</span> label_both, <span class="at">ncol =</span> <span class="dv">1</span>,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>             <span class="at">strip.position =</span> <span class="st">"right"</span>) <span class="sc">+</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_y_discrete</span>(<span class="at">labels =</span> \(x) <span class="fu">substr</span>(x, <span class="dv">3</span>, <span class="fl">1e4</span>))</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>plot_mcb_series</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-simulation-mcb-series" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simulation-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-simulation-mcb-series-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simulation-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Average ranks of <span class="math inline">\(1\)</span>-, <span class="math inline">\(6\)</span>- and <span class="math inline">\(12\)</span>-step-ahead MSE of different model and component specifications in the simulation. The methods using FLAP are labelled as “Model – Component Weights – Number of Components”. The two benchmark models generating base forecasts are labelled “Model – Benchmark”. Their MSEs are marked with triangles. The shaded region is the confidence interval of the best performing method. Methods outside the shaded region are significantly worse than the best method.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>  qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse.qs"</span>)),</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>  qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_simulation</span>(<span class="st">"mse_exp.qs"</span>))</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>plot_mse <span class="ot">&lt;-</span> mse <span class="sc">%&gt;%</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(model <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"arima"</span>, <span class="st">"dfm"</span>, <span class="st">"var"</span>, <span class="st">"true"</span>),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>         Phi <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"PCA_normal"</span>, <span class="st">"normal"</span>) <span class="sc">|</span> <span class="fu">is.na</span>(Phi),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>         h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>  <span class="co"># {print(distinct(., model, Phi))}</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> value,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>             <span class="at">colour =</span> model,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>)))<span class="sc">+</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> m) <span class="sc">+</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">data =</span> \(df) <span class="fu">filter</span>(df, <span class="sc">!</span>proj),</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">yintercept =</span> value,</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>                 <span class="at">colour =</span> model,</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>                 <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>))) <span class="sc">+</span></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>  <span class="co"># facet_wrap("h", scales = "free", labeller = label_both) +</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="st">"h"</span>, <span class="at">labeller =</span> label_both) <span class="sc">+</span></span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"MSE"</span>) <span class="sc">+</span></span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Component"</span>,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"dashed"</span>,</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"solid"</span>,</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"longdash"</span></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"PCA+Norm."</span>,</span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"No Proj."</span>,</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"Norm."</span></span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    )) <span class="sc">+</span></span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Model"</span>,</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> cb_palette_grey[<span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">2</span>)],</span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>      <span class="st">"arima"</span> <span class="ot">=</span> <span class="st">"ARIMA"</span>,</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>      <span class="st">"dfm"</span> <span class="ot">=</span> <span class="st">"DFM"</span>,</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>      <span class="st">"true"</span> <span class="ot">=</span> <span class="st">"VAR - DGP"</span>,</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a>      <span class="st">"var"</span> <span class="ot">=</span> <span class="st">"VAR - Est."</span>)) <span class="sc">+</span></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.background =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.background =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="at">mult =</span> <span class="dv">0</span>))</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>plot_mse</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-simulation-line" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-simulation-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-simulation-line-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-simulation-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Out-of-sample MSE for base and FLAP forecasts as the number of components <span class="math inline">\(p\)</span> increases, for forecast horizons <span class="math inline">\(1\)</span>, <span class="math inline">\(6\)</span> and <span class="math inline">\(12\)</span>. “VAR – DGP” indicates the performance of the true data generating VAR model. “VAR – Est.” indicates the performance of the VAR model with the same structure as the true model but with estimated parameters. The solid horizontal lines show the MSE for the base forecasts (with no projection) while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of <span class="math inline">\(p=m=70\)</span>, the number of series.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="evaluating-base-forecasts" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-base-forecasts">Evaluating base forecasts</h3>
<p>Having simulated data from a VAR DGP, we expect the DFM to capture the correlations between series, something not possible with univariate ARIMA models. Hence, we expect the DFM benchmark to perform better than the ARIMA benchmark. This is indeed the case. <a href="#fig-simulation-mcb-series" class="quarto-xref">Figure&nbsp;1</a> shows that the base DFM forecasts are significantly better than base ARIMA forecasts, with the exception for <span class="math inline">\(h=1\)</span>, where the difference are statistically insignificant but only marginally.</p>
<p>This is also observed in <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a>. The solid horizontal line representing the MSE of the base DFM forecasts is always much lower than the horizontal solid line for the base ARIMA forecasts.</p>
</section>
<section id="evaluating-flap-forecasts" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-flap-forecasts">Evaluating FLAP forecasts</h3>
<p>The most important observation from <a href="#fig-simulation-mcb-series" class="quarto-xref">Figure&nbsp;1</a> are the statistically significant improvements of the FLAP forecasts over their corresponding benchmark base forecasts. Note that we are referring here to forecasts corresponding to the same model, i.e., FLAP ARIMA forecasts compared to base ARIMA forecasts and FLAP DFM forecasts compared to base DFM forecasts. The average ranks of the FLAP forecasts are better than the corresponding base forecasts for all forecast horizons, and the differences are all statistically significant. The only exception is for the PCA-related FLAP forecasts using only <span class="math inline">\(m=70\)</span> components for forecast horizon <span class="math inline">\(6\)</span> and <span class="math inline">\(12\)</span>.</p>
<p>The number of components seems to be important. The best-performing models all comprise the maximum of <span class="math inline">\(300\)</span> components. For <span class="math inline">\(h=1\)</span>-step-ahead forecasts, the differences between the methods with <span class="math inline">\(300\)</span> components are not significantly significant, regardless of the model that generated the base forecasts or the method used to construct the components.</p>
<p>Indeed, <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a> shows that the MSEs for the FLAP ARIMA and DFM forecasts, represented by the dashed lines, decrease monotonically from the base, as the number of components increases. This supports <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a>, demonstrating that increasing the number of components in FLAP can reduce the forecast error variance. Of course this is feasible in this ideal setting where each time series has <span class="math inline">\(400\)</span> observations and we use no more than <span class="math inline">\(300\)</span> components in FLAP. The relatively large number of observations coupled with the relatively simple VAR DGP, has potentially eased the challenge of estimation. This continuous reduction in forecast error variance is not always achievable with real data, as demonstrated in <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>, especially with FRED-MD dataset in <a href="#sec-fred-md" class="quarto-xref">Section&nbsp;5.2</a>.</p>
<p>Worth noting from <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a> is the performance of the FLAP ARIMA forecasts relative to DMF forecasts as the number of components <span class="math inline">\(p\)</span> increases. The MSE of ARIMA FLAP forecasts becomes lower than the DFM base forecasts for <span class="math inline">\(p&lt;70\)</span> for <span class="math inline">\(h=1\)</span>, for <span class="math inline">\(70&lt;p&lt;100\)</span> for <span class="math inline">\(h=6\)</span> and for a larger <span class="math inline">\(p\)</span> for <span class="math inline">\(h=12\)</span>. Hence, FLAP is able to enhance univariate ARIMA forecasts with shared information between series by capturing these cross-correlations in the components.</p>
<p>Interestingly, for <span class="math inline">\(h=1\)</span>, the MSEs of FLAP ARIMA and FLAP DFM forecasts converge to the same value as <span class="math inline">\(p\)</span> reaches <span class="math inline">\(300\)</span>, no matter how the components are constructed. For the longer forecast horizons although the MSEs again converge they do not reach the same value for <span class="math inline">\(p=300\)</span>, although they are closer for <span class="math inline">\(h=6\)</span> than <span class="math inline">\(h=12\)</span>. The reason for this is not the contribution of FLAP, but the starting point of the two sets of base forecasts. As the forecast horizon increases the ARIMA base forecasts MSEs increase, while the MSEs for DFM base forecasts remain at very similar levels. Hence, it seems that the direct forecasts from the DFM model, and hence fitting a different model for each forecast horizon <span class="math inline">\(h\)</span>, is advantageous for the longer forecast horizons, in this setting of forecasting data generated from a simple VAR DGP. This is in contrast to the iterative nature of ARIMA generated forecasts.</p>
<p>GA: NOT SURE THAT THIS IS NOT REFLECTED ABOVENote that the same forecasts of the components, generated from univariate ARIMA models of these components, are used for both the FLAP ARIMA and the FLAP DFM methods. This implies that there is valuable information in the series that is not captured by either the ARIMA model or DFM, but is captured by the components. As the number of components increases, the information captured by the components overpowers the information captured by the base models, dominating the performance of the FLAP forecasts. Once again, this emphasises the importance of the components and FLAP. In this extreme case, the simple model is as good as the more complicated model after projection, while the forecast model itself is not as valuable as FLAP.</p>
</section>
</section>
<section id="sec-component-construction" class="level2">
<h2 class="anchored" data-anchor-id="sec-component-construction">Component construction</h2>
<p>The construction of components is obviously important in the proposed FLAP. However, the simulation results show that the method of constructing components may not be as important as one may have expected. In <a href="#fig-simulation-mcb-series" class="quarto-xref">Figure&nbsp;1</a>, the main difference that can be observed is between using a combination of PCA and random weights, versus purely using random weights. The distribution that generates the random weights has limited effect. in <a href="#fig-simulation-mcb-series" class="quarto-xref">Figure&nbsp;1</a>, with the same number of components and the same base ARIMA model, the MSEs are not significantly different, regardless of distribution from which the weights are simulated. The same conclusion can be drawn when PCA is used. As long as principal components are used, the performance is not different whether the additional components are generated from a normal distribution (PCA+Norm) or a uniform distribution (PCA+Unif).</p>
<p>When the weights are randomly simulated, the distribution is of limited importance. Therefore, in <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a>, we look only at the inclusion of PCA and randomly generated components whose weights are from a standard normal distribution. When <span class="math inline">\(p\)</span> is relatively small, the MSE decreases at a faster rate when PCA is used. Comparing the two dashed lines of the FLAP ARIMA forecasts, The performance of FLAP forecasts without PCA reaches and exceeds the performance with PCA before the number of components reaches <span class="math inline">\(m\)</span>, and stays in the lead thereafter, although the gap seems to diminish with large <span class="math inline">\(p\)</span>. This difference of PCA comes from the variances of principal components being maximised and ranked from largest to smallest. Because the performance of using random orthonormal weight matrix is the same as using only random normal weights, the difference of PCA is not from the orthogonality of the components. This might suggest the use of simple random weights if one is prepared to include a relatively large number of components in FLAP and to use PCA only when the number of components is small. However, as we will see in <a href="#sec-empirical-applications" class="quarto-xref">Section&nbsp;5</a>, this is not the case with real data. In this case, PCA seems to be the preferred approach even when the number of components is large.</p>
<p>Different constructions of components remain an important aspect of FLAP. One important future direction would be to find alternative and optimal components, as we do not limit the structure of the weight matrix in <a href="#sec-method" class="quarto-xref">Section&nbsp;3</a>. This should be studied together with the selection of the forecast model since both the weight matrix <span class="math inline">\(\bm{\Phi}\)</span> and the base forecast error variance <span class="math inline">\(\bm{W}_h\)</span> can affect the projection simultaneously in <a href="#eq-M" class="quarto-xref">Equation&nbsp;2</a>. This is likely to be an extension of the forecast combination literature, focusing on the properties of the base forecasts, and the diversity and robustness of the forecast model and components. Examples of studies on this issue in the forecast combination literature include <span class="citation" data-cites="BatDua1995">@BatDua1995</span>, <span class="citation" data-cites="KanEtAl2022">@KanEtAl2022</span> and <span class="citation" data-cites="LicWin2020">@LicWin2020</span>.</p>
</section>
<section id="sources-of-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="sources-of-uncertainty">Sources of uncertainty</h2>
<p>At the bottom of each panel in <a href="#fig-simulation-line" class="quarto-xref">Figure&nbsp;2</a>, the best forecasts come from the true DGP VAR model (the solid green line). The true VAR model FLAP forecasts do not improve on the base forecasts, as the uncertainty comes from the intrinsic error in the DGP that cannot be reduced. The second best forecasts come from the estimated VAR model, as the uncertainty, apart from the intrinsic error, only comes from the estimation error, not the model misspecification error like ARIMA and DFM, which are both misspecified in this simulation example. The gap between the estimated VAR and the true VAR becomes bigger for a longer forecast horizon, because VAR produces iterative forecasts, and estimation error accumulates as <span class="math inline">\(h\)</span> becomes larger.</p>
<p>Forecast projection shows little, if any, improvement over the estimated VAR. This means that FLAP cannot reduce estimation error, which is termed as the parameter uncertainty in <span class="citation" data-cites="PetEtAl2018">@PetEtAl2018</span>. On the other hand, it shows significant improvement over misspecified base models. This implies that the uncertainty FLAP can reduce is mainly the model misspecification error, referred to as the model uncertainty in <span class="citation" data-cites="PetEtAl2018">@PetEtAl2018</span>. This is similar to bagging as bagging also reduces variance by controlling model uncertainty. <span class="citation" data-cites="PetEtAl2018">@PetEtAl2018</span> also examine the data uncertainty, which is not studied here. It is not clear how data uncertainty translates in a projection problem. However, the uncertainty in how the components are constructed, unique in FLAP problems, is discussed in <a href="#sec-component-construction" class="quarto-xref">Section&nbsp;4.3</a> and awaits future research.</p>
</section>
</section>
<section id="sec-empirical-applications" class="level1">
<h1>Empirical applications</h1>
<p>Here we apply the FLAP method to two real data examples and draw most of the same conclusions we did from the simulations, with a few key differences.</p>
<section id="sec-australian-domestic-tourism" class="level2">
<h2 class="anchored" data-anchor-id="sec-australian-domestic-tourism">Australian domestic tourism</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pa_visnights <span class="ot">&lt;-</span> <span class="cf">function</span>(...) <span class="fu">pa</span>(<span class="st">"tourism"</span>, <span class="st">"projection"</span>, <span class="st">"output"</span>, ...)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">77</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="dv">200</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The Australian Tourism Data Set compiled from the National Visitor Survey by Tourism Research Australia contains the total number of nights spent by Australians away from home. We refer to these as visitor nights. The monthly visitor nights are recorded for <span class="math inline">\(m=77\)</span> regions around Australia, covering the period January 1998 to December 2019. To evaluate the forecast performance, we conduct expanding window time series cross-validation <span class="citation" data-cites="fpp3">[@fpp3]</span>. The first <span class="math inline">\(T = 84\)</span> observations are used as the first training set. The following <span class="math inline">\(12\)</span> months are used as the test set for evaluation. We repeat the evaluation for the rest of the data, by expanding each training sample by one observation at a time. This generates <span class="math inline">\(169\)</span> forecasts for each of the forecast horizons from <span class="math inline">\(1\)</span> to <span class="math inline">\(12\)</span> for evaluation.</p>
<p>The base forecasts for both the series and the components are produced using univariate ETS models selected and fitted using the <code>ets()</code> function in the <code>forecast</code> package <span class="citation" data-cites="forecast HynKha2008">[@forecast; @HynKha2008]</span>. See <span class="citation" data-cites="fpp2">@fpp2 [chap. 7]</span> for more details. <!-- In an ETS model, the trend term describes the direction of the long-term tendency, the seasonal term describes the periodically recurring pattern with a fixed periodicity, and the error term measures the uncertainty. The trend and seasonal terms are optional. If a trend term exists, we allow it to be additive or additive damped; if a seasonality term exists, it can be addictive or multiplicative; the error can be additive or multiplicative. Excluding models with numerical instabilities, we choose the model with the smallest AICc among the $15$ possible models. --></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># ?tsutils::nemenyi</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># dir(pa_visnights())</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>mse_ets_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_visnights</span>(<span class="st">"mse_ets_series.qs"</span>))</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_visnights</span>(<span class="st">"mse_proj_ets_normal_series.qs"</span>))</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_pca_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_visnights</span>(<span class="st">"mse_proj_ets_pca_normal_series.qs"</span>))</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_pca_normal_series_1 <span class="ot">&lt;-</span> mse_proj_ets_pca_normal_series[[<span class="dv">1</span>]]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_normal_series_1 <span class="ot">&lt;-</span> mse_proj_ets_normal_series[[<span class="dv">1</span>]]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_pca_normal_series_2 <span class="ot">&lt;-</span> mse_proj_ets_pca_normal_series[[<span class="dv">2</span>]]</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_normal_series_2 <span class="ot">&lt;-</span> mse_proj_ets_normal_series[[<span class="dv">2</span>]]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_pca_normal_series_m <span class="ot">&lt;-</span> mse_proj_ets_pca_normal_series[[m]]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_normal_series_m <span class="ot">&lt;-</span> mse_proj_ets_normal_series[[m]]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_pca_normal_series_p <span class="ot">&lt;-</span> mse_proj_ets_pca_normal_series[[p]]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>mse_proj_ets_normal_series_p <span class="ot">&lt;-</span> mse_proj_ets_normal_series[[p]]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>name_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_ets_series =</span> <span class="st">"ETS-Benchmark"</span>,</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_pca_normal_series_1 =</span> <span class="st">"ETS-PCA-1"</span>,</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_normal_series_1 =</span> <span class="st">"ETS-Norm-1"</span>,</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_pca_normal_series_2 =</span> <span class="st">"ETS-PCA-2"</span>,</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_normal_series_2 =</span> <span class="st">"ETS-Norm-2"</span>,</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_pca_normal_series_m =</span> <span class="st">"ETS-PCA-m"</span>,</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_normal_series_m =</span> <span class="st">"ETS-Norm-m"</span>,</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_pca_normal_series_p =</span> <span class="st">"ETS-PCA+Norm-200"</span>,</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_ets_normal_series_p =</span> <span class="st">"ETS-Norm-200"</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>mse_series_mat_ls <span class="ot">&lt;-</span> <span class="fu">lst</span>(<span class="sc">!!!</span><span class="fu">syms</span>(<span class="fu">names</span>(name_vec))) <span class="sc">%&gt;%</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list2array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aperm</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>  <span class="fu">array2list</span>()</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a><span class="co"># tsutils::nemenyi(mse_series_mat_ls[[1]], plottype = "vmcb")</span></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>mcb_df_ls <span class="ot">&lt;-</span>  mse_series_mat_ls <span class="sc">%&gt;%</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(tsutils<span class="sc">::</span>nemenyi, <span class="at">plottype =</span> <span class="st">"none"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(\(x) <span class="fu">as.data.frame</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> name_vec[name]) <span class="sc">%&gt;%</span></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">paste</span>(name, <span class="fu">format</span>(<span class="fu">round</span>(value, <span class="dv">2</span>), <span class="at">width =</span> <span class="dv">5</span>, <span class="at">nsmall =</span> <span class="dv">2</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">col =</span></span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>                       u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>l) <span class="sc">|</span></span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a>                       l[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u)</span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a>           ))</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-49"><a href="#cb14-49" aria-hidden="true" tabindex="-1"></a>plot_mcb_series <span class="ot">&lt;-</span> mcb_df_ls <span class="sc">%&gt;%</span></span>
<span id="cb14-50"><a href="#cb14-50" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(<span class="at">.id =</span> <span class="st">"h"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb14-51"><a href="#cb14-51" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">h =</span> <span class="fu">as.integer</span>(h)) <span class="sc">%&gt;%</span></span>
<span id="cb14-52"><a href="#cb14-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb14-53"><a href="#cb14-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">factor</span>(name,<span class="fu">unique</span>(name), <span class="at">ordered =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb14-54"><a href="#cb14-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(h) <span class="sc">%&gt;%</span></span>
<span id="cb14-55"><a href="#cb14-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_mcb</span>(<span class="st">"Benchmark"</span>) <span class="sc">+</span></span>
<span id="cb14-56"><a href="#cb14-56" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="st">"h"</span>, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">labeller =</span> label_both, <span class="at">ncol =</span> <span class="dv">1</span>,</span>
<span id="cb14-57"><a href="#cb14-57" aria-hidden="true" tabindex="-1"></a>             <span class="at">strip.position =</span> <span class="st">"right"</span>)</span>
<span id="cb14-58"><a href="#cb14-58" aria-hidden="true" tabindex="-1"></a>plot_mcb_series</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-visnights-mcb-series" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-visnights-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-visnights-mcb-series-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-visnights-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Average ranks of <span class="math inline">\(1\)</span>-, <span class="math inline">\(6\)</span>- and <span class="math inline">\(12\)</span>-step-ahead cross-validation MSE of different model and component specifications on the visitor nights data. The methods using forecast projection are named as “Model – Component Weights – Number of Components”. The base models are named as “Model – Benchmark” and these points are marked with triangles. The shaded region is the confidence interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_visnights</span>(<span class="st">"mse.qs"</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>plot_mse <span class="ot">&lt;-</span> mse <span class="sc">%&gt;%</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> value,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>))) <span class="sc">+</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> m) <span class="sc">+</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">data =</span> \(df) <span class="fu">filter</span>(df, <span class="sc">!</span>proj),</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">yintercept =</span> value,</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>                 <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>))) <span class="sc">+</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>  <span class="co"># facet_wrap("h", scales = "free", labeller = label_both) +</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="st">"h"</span>, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">labeller =</span> label_both) <span class="sc">+</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"MSE"</span>) <span class="sc">+</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Component"</span>,</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"dashed"</span>,</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"solid"</span>,</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"longdash"</span></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"PCA+Norm."</span>,</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"No Proj."</span>,</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"Norm."</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    )) <span class="sc">+</span></span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.background =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.background =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="at">mult =</span> <span class="dv">0</span>))</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>plot_mse</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-visnights-line" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-visnights-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-visnights-line-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-visnights-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Out-of-sample MSE of base and FLAP forecasts as the number of components <span class="math inline">\(p\)</span> increases, for forecast horizons <span class="math inline">\(1\)</span>, <span class="math inline">\(6\)</span> and <span class="math inline">\(12\)</span>, using the visitor nights data. The solid horizontal lines show the MSE for the base forecasts while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of <span class="math inline">\(p=m\)</span>, the number of series.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The MCB and MSE plots for <span class="math inline">\(h=1, 6, 12\)</span> (as described in <strong>?@sec-flap</strong>) are shown in <a href="#fig-visnights-mcb-series" class="quarto-xref">Figure&nbsp;3</a> and <a href="#fig-visnights-line" class="quarto-xref">Figure&nbsp;4</a>. The FLAP forecasts in general outperforms the base forecasts, which is consistent with <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>. In <a href="#fig-visnights-mcb-series" class="quarto-xref">Figure&nbsp;3</a>, the base forecasts are always ranked last. Even FLAP with only one component are significantly better than the base forecasts for <span class="math inline">\(h=6\)</span> and <span class="math inline">\(12\)</span>.</p>
<p>We highlight two different observations from <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>. First, in <a href="#fig-visnights-line" class="quarto-xref">Figure&nbsp;4</a>, the MSE of FLAP forecasts does not always decrease as the number of components increases — see for example for <span class="math inline">\(h=1\)</span> with <span class="math inline">\(p&gt;150\)</span>. This can also be seen from <a href="#fig-visnights-mcb-series" class="quarto-xref">Figure&nbsp;3</a>, where the two best methods are not significantly different, even though they have very different numbers of components (<span class="math inline">\(p=77\)</span> and <span class="math inline">\(p=200\)</span>). Choosing the number of components is a tradeoff between the increasing estimation error as the dimension of forecast error variance <span class="math inline">\(\bm{W}_h\)</span> increases, and the additional benefit brought by the information embedded in the new components, depending on the complexity of the DGP. For the visitor nights data set, the benefit of components above the estimation error diminishes after the number of components reaches <span class="math inline">\(p=m=77\)</span>.</p>
<p>Second, unlike <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a>, using principal components is significantly better than simply using random components with normal weights. In <a href="#fig-visnights-mcb-series" class="quarto-xref">Figure&nbsp;3</a>, this is observed by comparing the performance of PCA related FLAP and the performance of FLAP with only random components, with a relatively large number of components, for example, <span class="math inline">\(m=77\)</span> or <span class="math inline">\(300\)</span>. This is also clear from <a href="#fig-visnights-line" class="quarto-xref">Figure&nbsp;4</a>, where the reduction in terms of sample MSE from using PCA is always in the lead, even after the <span class="math inline">\(m\)</span> principal components are exhausted and random components with weights generated from a normal distribution are added.</p>
<p>PCA aims to find components along the direction where the data varies the most. The first principal components accounts for the maximum variance in the data. Subsequent principal components are orthogonal to the previous ones and capture the maximum remaining variance. Based on this variance maximisation and ranking of PCA, we propose two potential explanations for its superior performance.</p>
<ol type="1">
<li>Optimality: By maximising and ranking the variance of PCs from largest to smallest, we ensure that the projection utilises components containing significantly more information (as measured by variance) compared to randomly weighted components.</li>
<li>Diversity: Actively seeking PCs with the highest variance results in the incorporation of a more diverse set of components into the projection.</li>
</ol>
</section>
<section id="sec-fred-md" class="level2">
<h2 class="anchored" data-anchor-id="sec-fred-md">FRED-MD</h2>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>pa_fredmd <span class="ot">&lt;-</span> <span class="cf">function</span>(...) <span class="fu">pa</span>(<span class="st">"fred-md"</span>, <span class="st">"projection"</span>, <span class="st">"output"</span>, ...)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="dv">122</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The FRED-MD <span class="citation" data-cites="McCNg2016">[@McCNg2016]</span> is a popular monthly data set of macroeconomic variables, and shares similar properties with the <span class="citation" data-cites="StoWat2002">@StoWat2002</span> data. We downloaded and transformed the data set using the <code>fbi</code> package <span class="citation" data-cites="fbi">[@fbi]</span>. The period we use for this exercise is from January 1959 to September 2023, containing <span class="math inline">\(777\)</span> observations. Following <span class="citation" data-cites="McCNg2016">@McCNg2016</span>, we replace observations that deviate from the sample median by more than 10 interquartile ranges (which are recognised as outliers), with missing values. We then drop any series with more than 5% observations missing. This left us with <span class="math inline">\(m=122\)</span> series. We fill in the missing values using the expectation-maximization (EM) algorithm described in <span class="citation" data-cites="StoWat2002a">@StoWat2002a</span> with <span class="math inline">\(8\)</span> factors. The number <span class="math inline">\(8\)</span> is identified by <span class="citation" data-cites="McCNg2016">@McCNg2016</span>, albeit with a different time span. In order to relate to the theoretical forecast error variance reduction, we use MSE as the error measure, instead of other scaled or percentage error measures. To reliably calculate MSE over series with different scales, we demean the series and scale them to have variance <span class="math inline">\(1\)</span>. The MSEs are calculated on this standardised scale without back-transformation.</p>
<p>We evaluate the performance of forecasts using time series cross-validation. Starting with <span class="math inline">\(300\)</span> observations in the first training set and the following <span class="math inline">\(12\)</span> observations as the test set, we repeat the evaluation for the rest of the data with the size of the training set increasing by <span class="math inline">\(1\)</span> in each iteration. This provides us with <span class="math inline">\(466\)</span> forecasts for each of the forecast horizons from <span class="math inline">\(1\)</span> to <span class="math inline">\(12\)</span> for evaluation. We generates base forecasts from ARIMA models using <code>auto.arima()</code> function from the <code>forecast</code> package <span class="citation" data-cites="forecast">[@forecast]</span> with the default settings, and DFMs. The ranges of the meta-parameters in the DFMs are <span class="math inline">\(1\leq k \leq 8\)</span> (since <span class="math inline">\(8\)</span> factors are identified and used to fill in the missing values), <span class="math inline">\(1 \leq n \leq 3\)</span> and <span class="math inline">\(0 \leq s \leq 6\)</span>. For more details see <a href="#sec-benchmarks" class="quarto-xref">Section&nbsp;4.1</a>. The MCB and MSE plots for <span class="math inline">\(h=1, 6, 12\)</span> are shown in <a href="#fig-fred-md-mcb-series" class="quarto-xref">Figure&nbsp;5</a> and <a href="#fig-fred-md-line" class="quarto-xref">Figure&nbsp;6</a>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>mse_arima_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse_arima_series.qs"</span>))</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>mse_dfm_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse_dfm_series.qs"</span>))</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse_proj_arima_normal_series.qs"</span>))</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse_proj_arima_pca_normal_series.qs"</span>))</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_pca_normal_series <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse_proj_dfm_pca_normal_series.qs"</span>))</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_normal_series_1 <span class="ot">&lt;-</span> mse_proj_arima_pca_normal_series[[<span class="dv">1</span>]]</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series_1 <span class="ot">&lt;-</span> mse_proj_arima_normal_series[[<span class="dv">1</span>]]</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_pca_normal_series_1 <span class="ot">&lt;-</span> mse_proj_dfm_pca_normal_series[[<span class="dv">1</span>]]</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_normal_series_2 <span class="ot">&lt;-</span> mse_proj_arima_pca_normal_series[[<span class="dv">2</span>]]</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series_2 <span class="ot">&lt;-</span> mse_proj_arima_normal_series[[<span class="dv">2</span>]]</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_pca_normal_series_2 <span class="ot">&lt;-</span> mse_proj_dfm_pca_normal_series[[<span class="dv">2</span>]]</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_pca_normal_series_m <span class="ot">&lt;-</span> mse_proj_arima_pca_normal_series[[m]]</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>mse_proj_arima_normal_series_m <span class="ot">&lt;-</span> mse_proj_arima_normal_series[[m]]</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>mse_proj_dfm_pca_normal_series_m <span class="ot">&lt;-</span> mse_proj_dfm_pca_normal_series[[m]]</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>name_vec <span class="ot">&lt;-</span> <span class="fu">c</span>(</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_arima_series =</span> <span class="st">"ARIMA-Benchmark"</span>,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_dfm_series =</span> <span class="st">"DFM-Benchmark"</span>,</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_pca_normal_series_1 =</span> <span class="st">"ARIMA-PCA-1"</span>,</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_normal_series_1 =</span> <span class="st">"ARIMA-Norm-1"</span>,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_dfm_pca_normal_series_1 =</span> <span class="st">"DFM-PCA-1"</span>,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mse_proj_arima_pca_normal_series_2 = "ARIMA-PCA-2",</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mse_proj_arima_normal_series_2 = "ARIMA-Norm-2",</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>  <span class="co"># mse_proj_dfm_pca_normal_series_2 = "DFM-PCA-2",</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_pca_normal_series_m =</span> <span class="st">"ARIMA-PCA-m"</span>,</span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_arima_normal_series_m =</span> <span class="st">"ARIMA-Norm-m"</span>,</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a>  <span class="at">mse_proj_dfm_pca_normal_series_m =</span> <span class="st">"DFM-PCA-m"</span></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>mse_series_mat_ls <span class="ot">&lt;-</span> <span class="fu">lst</span>(<span class="sc">!!!</span><span class="fu">syms</span>(<span class="fu">names</span>(name_vec))) <span class="sc">%&gt;%</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list2array</span>() <span class="sc">%&gt;%</span></span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aperm</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">2</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>  <span class="fu">array2list</span>()</span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a><span class="co"># tsutils::nemenyi(mse_series_mat_ls[[1]], plottype = "vmcb")</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>mcb_df_ls <span class="ot">&lt;-</span>  mse_series_mat_ls <span class="sc">%&gt;%</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(tsutils<span class="sc">::</span>nemenyi, <span class="at">plottype =</span> <span class="st">"none"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lapply</span>(\(x) <span class="fu">as.data.frame</span>(x) <span class="sc">%&gt;%</span></span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> name_vec[name]) <span class="sc">%&gt;%</span></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">paste</span>(name, <span class="fu">format</span>(<span class="fu">round</span>(value, <span class="dv">2</span>), <span class="at">width =</span> <span class="dv">5</span>, <span class="at">nsmall =</span> <span class="dv">2</span>))) <span class="sc">%&gt;%</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>           <span class="fu">mutate</span>(<span class="at">col =</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>                       u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>l) <span class="sc">|</span></span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>                    (u[[<span class="fu">which.min</span>(value)]] <span class="sc">&gt;=</span>u <span class="sc">&amp;</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>                       l[[<span class="fu">which.min</span>(value)]] <span class="sc">&lt;=</span>u)</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>           ))</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a>plot_mcb_series <span class="ot">&lt;-</span> mcb_df_ls <span class="sc">%&gt;%</span></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_rows</span>(<span class="at">.id =</span> <span class="st">"h"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">h =</span> <span class="fu">as.integer</span>(h)) <span class="sc">%&gt;%</span></span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">name =</span> <span class="fu">factor</span>(name,<span class="fu">unique</span>(name), <span class="at">ordered =</span> <span class="cn">TRUE</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(h) <span class="sc">%&gt;%</span></span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot_mcb</span>(<span class="st">"Benchmark"</span>) <span class="sc">+</span></span>
<span id="cb17-58"><a href="#cb17-58" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="st">"h"</span>, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">labeller =</span> label_both, <span class="at">ncol =</span> <span class="dv">1</span>,</span>
<span id="cb17-59"><a href="#cb17-59" aria-hidden="true" tabindex="-1"></a>             <span class="at">strip.position =</span> <span class="st">"right"</span>)</span>
<span id="cb17-60"><a href="#cb17-60" aria-hidden="true" tabindex="-1"></a>plot_mcb_series</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-fred-md-mcb-series" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fred-md-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-fred-md-mcb-series-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fred-md-mcb-series-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Average ranks of <span class="math inline">\(1\)</span>-, <span class="math inline">\(6\)</span>- and <span class="math inline">\(12\)</span>-step-ahead cross-validation MSE of different model and component specifications on the FRED-MD data. The methods using forecast projection are named as “Model – Component Weights – Number of Components”. The base models are named as “Model – Benchmark” and these points are marked with triangles. The shaded region is the confidence interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.
</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>mse <span class="ot">&lt;-</span> qs<span class="sc">::</span><span class="fu">qread</span>(<span class="fu">pa_fredmd</span>(<span class="st">"mse.qs"</span>))</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>plot_mse <span class="ot">&lt;-</span> mse <span class="sc">%&gt;%</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(<span class="sc">!</span>model <span class="sc">%in%</span> <span class="st">"ets"</span>,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>         <span class="sc">!</span><span class="fu">grepl</span>(<span class="st">"ets"</span>, Phi),</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>         h <span class="sc">%in%</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">12</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> value,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>             <span class="at">colour =</span> model,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>             <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>))) <span class="sc">+</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> m) <span class="sc">+</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">data =</span> \(df) <span class="fu">filter</span>(df, <span class="sc">!</span>proj),</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">yintercept =</span> value,</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">colour =</span> model,</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">linetype =</span> <span class="fu">paste</span>(proj, Phi, <span class="at">sep =</span> <span class="st">"."</span>))) <span class="sc">+</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># facet_wrap("h", scales = "free", labeller = label_both) +</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="st">"h"</span>, <span class="at">scales =</span> <span class="st">"free"</span>, <span class="at">labeller =</span> label_both) <span class="sc">+</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">"MSE"</span>) <span class="sc">+</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_linetype_manual</span>(</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Component"</span>,</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"dashed"</span>,</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"solid"</span>,</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"longdash"</span></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.PCA_normal"</span> <span class="ot">=</span> <span class="st">"PCA+Norm."</span>,</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a>      <span class="st">"FALSE.NA"</span> <span class="ot">=</span> <span class="st">"No Proj."</span>,</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>      <span class="st">"TRUE.normal"</span> <span class="ot">=</span> <span class="st">"Norm."</span></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    )) <span class="sc">+</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">name =</span> <span class="st">"Model"</span>,</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">values =</span> cb_palette_grey[<span class="fu">c</span>(<span class="dv">7</span>, <span class="dv">6</span>)],</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    <span class="at">labels =</span> <span class="fu">c</span>(</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>      <span class="st">"arima"</span> <span class="ot">=</span> <span class="st">"ARIMA"</span>,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>      <span class="st">"dfm"</span> <span class="ot">=</span> <span class="st">"DFM"</span>)) <span class="sc">+</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">plot.background =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>        <span class="at">legend.background =</span> <span class="fu">element_blank</span>()) <span class="sc">+</span></span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">expand =</span> <span class="fu">expansion</span>(<span class="at">mult =</span> <span class="dv">0</span>))</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a>plot_mse</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="fig-fred-md-line" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-fred-md-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="free-lunch_files/figure-html/fig-fred-md-line-1.png" class="img-fluid figure-img" width="672">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-fred-md-line-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Out-of-sample MSE for base and FLAP forecasts as the number of components <span class="math inline">\(p\)</span> increases, for forecast horizons <span class="math inline">\(1\)</span>, <span class="math inline">\(6\)</span> and <span class="math inline">\(12\)</span>, using the FRED-MD data. The solid horizontal lines show the MSE for the base forecasts while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of <span class="math inline">\(p=m\)</span> the number of series.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The first observation worth noting in <a href="#fig-fred-md-mcb-series" class="quarto-xref">Figure&nbsp;5</a> is the performance of base ARIMA forecasts exceeds that of the base DFM. This difference is statistically significant for <span class="math inline">\(h=6\)</span>. The best models at all forecast horizons are once again FLAP forecasts using components from PCA. These are significantly better than the base forecasts at <span class="math inline">\(h=1\)</span> and <span class="math inline">\(6\)</span>. The fact that the FLAP with PCA is statistically significantly better than with components with random weights, which can be seen from both the rankings in <a href="#fig-fred-md-mcb-series" class="quarto-xref">Figure&nbsp;5</a> and the MSE in <a href="#fig-fred-md-line" class="quarto-xref">Figure&nbsp;6</a>, reaffirms our findings from the tourism data set discussed in <a href="#sec-australian-domestic-tourism" class="quarto-xref">Section&nbsp;5.1</a>.</p>
<p>In <a href="#fig-fred-md-line" class="quarto-xref">Figure&nbsp;6</a>, FLAP forecasts for both ARIMA and DFM seem to be worse than the base forecasts at the beginning, when the number of components included in FLAP is small. However, this improves as <span class="math inline">\(p\)</span> becomes larger and FLAP forecasts outperform the base forecasts gradually. The tradeoff between the benefit of including more components and the difficulty of estimation in a large dimension is once again observed, however seems to be more extreme compared to the tourism example. In this case, the MSEs start to increase once <span class="math inline">\(p\)</span> becomes larger than <span class="math inline">\(m\)</span>. <!-- The projected forecast even worsens to the same level as the base forecast for ARIMA at $h=6$ and DFM at $h=12$ when $p=300$.  --> As we have seen in <a href="#sec-simulation" class="quarto-xref">Section&nbsp;4</a> and <a href="#sec-australian-domestic-tourism" class="quarto-xref">Section&nbsp;5.1</a>, <span class="math inline">\(m\)</span> is not always a clear cut-off point. Where the performance of FLAP turns should be jointly determined by the number of series <span class="math inline">\(m\)</span>, the sample size <span class="math inline">\(T\)</span>, the component construction method, and the DGP. In the case of FRED-MD, it signals the importance of PCA, as <span class="math inline">\(m\)</span> is the point that the component changes from PCA to random normal weighted linear combinations, implying PCA can exploit the information in the data while random weights cannot. This is more obvious for <span class="math inline">\(h=1\)</span> and <span class="math inline">\(h=6\)</span>, as PCA works when <span class="math inline">\(p&lt;m\)</span>, but random normal weights do not seem to work from the beginning.</p>
</section>
</section>
<section id="sec-conclusion" class="level1">
<h1>Conclusion</h1>
<p>The proposed forecast linear augmented projection (FLAP) method has been shown to be a simple but effective way to reduce forecast error variance of any multivariate forecasting problem. It simply involves augmenting the data with linear combinations, forecasting these and then projecting the augmented vector of forecasts. We have shown theoretically that FLAP will continue to reduce forecast error covariance as more components are added, assuming we that the forecast error covariance matrix is known. In practice, a plug in estimate of this covariance matrix can be used, and in both simulated and empirical data we demonstrate that a simple shrinkage estimator does indeed lead to improvements in forecast accuracy. Regarding the construction of components we find that PCA in practice achieves significant improvements in forecast accuracy. Another appealing property of FLAP is that the projection step can even compensate for a poor choice of base forecasting model. This is particularly attractive since it makes FLAP robust against model misspecification in the base forecasting step.</p>
<p>One outstanding issue is to find alternatives to PCA to select component weights. For example, <span class="citation" data-cites="Goerg2013-dg">@Goerg2013-dg</span> proposed “forecastable components” that are optimal in the sense of minimising the forecast error variance of the components, while <span class="citation" data-cites="Matteson2011-jf">@Matteson2011-jf</span> proposed “dynamic orthogonal components” that reduce a multivariate time series to a set of uncorrelated univariate time series. It would be interesting to explore whether these components (or other similar suggestions) can be used effectively in FLAP. Also, another route to improving FLAP my be found by optimizing <a href="#eq-obj" class="quarto-xref">Equation&nbsp;15</a> over <span class="math inline">\(\bm{\Phi}\)</span> and <span class="math inline">\(\bm{G}\)</span> rather than just <span class="math inline">\(\bm{G}\)</span>.</p>
<p>Finally, while FLAP is motivated by the forecast reconciliation literature, the focus is very much on multivariate time series with no constraints. However, it would be possible to use both forecast reconciliation and forecast projection together. This may be particularly useful when there are relationships between series that are not captured in the known hierarchical structure.</p>
</section>
<section id="acknowledgements" class="level1 unnumbered">
<h1 class="unnumbered">Acknowledgements</h1>
<p>We thank Daniele Girolimetto for contributing to the initial proof of <a href="#thm-monotone" class="quarto-xref">Theorem&nbsp;2</a> in his unpublished work.</p>
<div style="page-break-after: always;"></div>
</section>
<section id="references" class="level1 unnumbered">
<h1 class="unnumbered">References</h1>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This argument, as well as the terminology direct and indirect forecasts, is inspired by <span class="citation" data-cites="HolEtAl2021">@HolEtAl2021</span> who discuss this in the forecast reconciliation setting.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>