---
title: "Free Lunch Multivariate Forecasting: reducing forecast variance using linear combinations"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author:
 - familyname: Yang
   othernames: Yangzhuoran Fin
   address: Monash University\newline Melbourne, Australia
   email: Fin.Yang@monash.edu
 - familyname: Hyndman
   othernames: Rob J.
   address: Monash University\newline Melbourne, Australia
 - familyname: Athanasopoulos
   othernames: George
   address: Monash University\newline Melbourne, Australia
 - familyname: Panagiotelis
   othernames: Anastasios
   address: University of Sydney\newline Sydney, Australia
pdf-engine: pdflatex
cite-method: biblatex
biblio-style: authoryear-comp
bibliography:
  - references-key.bib
  - references-pkg.bib
blind: false
cover: true
toc: false
keep-tex: true
fig-height: 5
fig-width: 8
number-sections: true
execute:
  echo: false
  warning: false
  message: false
  cache: false
editor_options:
  chunk_output_type: console
filters:
  - latex-environment
  - abstract-section
environments: toappendix
header-includes:
  - \usepackage[bibliography=common]{apxproof}
  - \def\Var{\operatorname{Var}}
  - \def\E{\operatorname{E}}
  - \def\tr{\operatorname{tr}}
format: wp-pdf
---

# Abstract

Abstract to be written.

```{r setup}
#| cache: false
knitr::read_chunk("free-lunch.R")
```
```{r path}
```

```{r library}
```

# Introduction {#sec-introduction}

We introduce a new method for improving the accuracy of any multivariate time series method, often substantially. This is done without introducing any new data, or any new information. Thus, we call it a "free lunch" method: it is a simple addition to any existing multivariate forecasting method that can improve its accuracy.

The method is based on the idea that the forecasts of linear combinations of the series should be consistent with the forecasts of the series themselves. For example, suppose we have two observed series $z_{t,1}$ and $z_{t,2}$, and we also construct the combination $c_t = 2 z_{t,1} -  z_{t,1}$, then the forecasts of $c_t$ should satisfy the same linear constraint: $\hat{c}_t = 2 \hat{z}_{t,1} - \hat{z}_{t,2}$. If they do not, then we can improve the forecasts of all three series by adjusting them to be consistent. While the forecasts of $z_t$ may be of no interest in themselves, they can be used to improve the forecasts of $x_t$ and $y_t$.

The idea can be extended to any number of linear combinations, and can be applied to any number of multivariate series. It does not depend on the forecasting method being used, and works well even if all series are forecast using univariate models. In fact, when used with univariate models, this allows cross-correlations between the series to be implicitly captured in the forecasts.

We call these linear combinations of the observed time series "components", and we call the original forecasts of the observed series "base forecasts". Our free-lunch method is to adjust the base forecasts to be consistent with the forecasts of the components by projecting all forecasts onto the space where the linear constraints are satisfied. We show (theoretically and empirically) that this method leads to significant reduction in the forecast variance, without introducing any bias.

Our free-lunch method has close connections to forecast reconciliation in the hierarchical forecasting literature. See @AthEtAl2023a for a recent review of the area. In particular, the projection formulation is inspired by the minimum trace [MinT, @WicEtAl2019] solution of the forecast reconciliation problem. Forecast reconciliation is a method to modify forecasts using projection so that they conform to a specific hierarchical, grouped or temporal structure. Notably, @WicEtAl2019, @AthEtAl2017, and  @DiGir2023 have shown that forecast reconciliation can reduce forecast variance theoretically and empirically, in cross-sectional settings, temporal settings, and in cross-temporal settings. @PanEtAl2021 have provided insight into the geometric interpretation of the projection used in forecast reconciliation. However, forecast reconciliation cannot be directly applied in a general multivariate time series unless the series satisfy some linear constraints such as a hierarchical structure. In contrast, our method can be applied to any multivariate time series. It can also be used in conjunction with forecast reconciliation to further reduce forecast variance.

The idea bears some similarity to bootstrap aggregation or "bagging" [@Bre1996;@BerEtAl2016], where the final prediction is produced from an ensemble of predictions made on bootstrapped data. Bagging can reduce prediction variance without increasing bias [@HasEtAl2003], by mitigating model uncertainty [@PetEtAl2018], and it does so without introducing any new data (just bootstrapped versions of the existing data). Our method also reduces forecast variance without introducing new data, but using linear combinations of the existing data, rather than bootstrapped versions of the data. A second difference is that bagging is model dependent: it is a procedure applied to enhance the models that produce the forecasts, where the same models are fitted repeatedly. Our method is model independent: it linearly transforms a set of forecasts, regardless of which models they come from. As a result, the two methods can be used in conjunction: the projections can be applied to forecasts produced by a bagged predictor.

Another approach to improve forecast accuracy is forecast combination. Point forecast combinations usually involve combining multiple forecasts of the same series from different models. See @WanEtAl2023 for a recent comprehensive review. Our free-lunch method differs from forecast combination in (a) the forecasts we combine, and (b) in how we combine them. First, rather than combine multiple forecasts of a single series, we combine single forecasts of many different linear combinations of all observed series. Second, our combinations are obtained via projections, and so the final forecasts of a particular series are linear combinations of all series in the collection, including the observed series and all constructed components. Our approach can be used in conjunction with standard forecast combination, as the base forecasts can be obtained from any combination of forecasts.

In a broad sense, forecast reconciliation, bagging, and our proposed free-lunch method can all be viewed as forms of forecast combination with different objects to be combined. @PetSpi2021 overview a group of methods utilising combination techniques which they call "the wisdom of data" including bagging, theta method [@AssNik2000], temporal aggregation [@KouEtAl2014; @AthEtAl2017], forecasting with sub-seasonal series [FOSS, @LiEtAl2022c] and forecast combination with multiple starting points [@DisPet2015]. These differ in transformations, series to forecast, forecasting models, and combination weights. Similarly, our free-lunch method aims to exploit information in the data, with a focus on the shared information that can be captured by linear combinations of the series.

Finally, the use of components bears a resemblance to Dynamic Factor Models (DFMs), specifically those used in a forecasting setting where the components (factors) are estimated using Principal Component Analysis (PCA) [@StoWat2002a; @StoWat2002; @StoWat2012], and their extension in the machine learning literature [@DeEtAl2019]. DFMs assume that the multivariate time series possesses common components and the dynamics of the observed series are governed by the dynamics of these unobserved components, typically assumed to follow a Vector AutoRegressive (VAR) model. In contrast, our method makes no assumptions on the parametric form of the dynamics. In fact, the forecasts from a DFM can be further improved by applying the free-lunch method to the DFM forecasts. This is demonstrated in @sec-simulation and @sec-empirical-applications, where we also show how the performance of projected forecasts from univariate ARIMA models is comparable to base DFM forecasts, with the help of components.

The rest of the paper is structured as follows. In @sec-method, we propose the free lunch forecast projection method, and highlight its theoretical properties and associated estimation methods. In @sec-simulation, we present a simulation example demonstrating its performance and discuss the implications for sources of uncertainty. @sec-empirical-applications examines the performance of free-lunch forecast projection in two empirical applications: forecasting Australian domestic tourism and forecasting macroeconomic variables in the FRED-MD data set. @sec-conclusion concludes with some thoughts on future research directions.

# Free-Lunch Forecast Projection {#sec-method}

## Definitions and properties

We use $\bm{I}_n$ to denote the $n\times n$ identity matrix, and $\bm{O}_{n\times k}$ to denote the $n\times k$ zero matrix. Define the selection matrix $\bm{J}_{n,k} = \big[\bm{I}_n ~~~ \bm{O}_{n\times k}\big]$, so that $\bm{J}_{n,k}\bm{A}$ picks out the first $n$ rows of a matrix $\bm{A}$.

Let $\bm{z}_t\in\mathbf{R}^m$ be a vector of $m$ observed time series at time $t$, and let $\bm{c}_t = \bm{\Phi}\bm{z}_t\in\mathbf{R}^p$ be a vector of $p$ linear combinations of $\bm{z}_t$ at time $t$, where $\bm{\Phi}\in\mathbf{R}^{p\times m}$ is a matrix of coefficients. We call $\bm{c}_t$ the components of $\bm{z}_t$.  Let $\bm{y}_{t} = \big[\bm{z}_t', \bm{c}'_{t}\big]'$ be the collection of series $\bm{z}_t$ and components $\bm{c}_{t}$, and let $\hat{\bm{y}}_{t+h}$ denote the $h$-step-ahead base forecast of $\bm{y}_{t}$. The forecast variance covariance matrix is $\Var(\hat{\bm{y}}_{t+h}-\bm{y}_{t+h}) = \bm{W}_h$.

We project the base forecasts onto the space where the constraints are imposed:
$$
\tilde{\bm{y}}_{t+h} = \bm{M} \hat{\bm{y}}_{t+h}
$$ {#eq-y_tilde}
with projection matrix
$$
\bm{M} = \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C},
$$ {#eq-M}
where $\bm{C} = \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]$ defines the $p\times(m+p)$ constraint matrix such that $\bm{C}\bm{y}_t= \bm{c}_{t} - \bm{\Phi}\bm{z}_t = \bm{0}$ for any $t$.

Let $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$ denote the first $p$ elements of $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, comprising the base and projected forecasts of $\bm{z}_t$ respectively. Similarly, let $\hat{\bm{c}}_{t+h}$ and $\tilde{\bm{c}}_{t+h}$ denote the last $p$ elements of $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, comprising the base and projected forecasts of $\bm{c}_t$ respectively. Then the projected forecast of $\bm{z}_t$ can be found by
$$
\tilde{\bm{z}}_{t+h} = \bm{J}\tilde{\bm{y}}_{t+h} = \bm{J}\bm{M} \hat{\bm{y}}_{t+h},
$$ {#eq-lcmap}
where $\bm{J} = \bm{J}_{m,p}$.

We are ready to present a few immediate results, with proofs provided in the Appendix.

::: {#lem-coherent}

The projected forecast $\tilde{\bm{y}}_{t+h}$ satisfies the constraint
$$
\bm{C}\tilde{\bm{y}}_{t+h}= \tilde{\bm{c}}_{t+h} - \bm{\Phi}\tilde{\bm{z}}_{t+h} = \bm{0}.
$$
:::
::: {.toappendix}
::: {.proof}
#### Proof of @lem-coherent
$$
\begin{aligned}
\bm{C}\tilde{\bm{y}}_{t+h} &=
\bm{C}\bm{M}\hat{\bm{y}}_{t+h} \\
&= \bm{C}(\bm{I}_{n+q} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\hat{\bm{y}}_{t+h}\\
&= \bm{C}\hat{\bm{y}}_{t+h} - \bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}\\
&= \bm{0}
\end{aligned}
$$
:::
:::

::: {#lem-self-proj}
The mapping matrix $\bm{M}$ projected a vector onto the space where the constraint $\bm{C}$ is satisfied. For $\bm{y}_{t+h}$ that already satisfies the constraint, the projection does not change its value:
$$
\bm{M}\bm{y}_{t+h} = \bm{y}_{t+h}.
$$
:::
::: {.toappendix}
::: {.proof}
#### Proof of @lem-self-proj
$$
\bm{M}\bm{y}_{t+h} = (\bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{y}_{t+h} = \bm{y}_{t+h}
$$
since $\bm{C}\bm{y}_{t+h} = \bm{0}$.
:::
:::


::: {#lem-unbiased}
If the base forecasts are unbiased such that
$$
\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t),
$$
then the projected forecasts are also unbiased:
$$
\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t).
$$
:::
::: {.toappendix}
::: {.proof}
#### Proof of @lem-unbiased
$$
\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) =
\E(\bm{M}\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{M}\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t)
$$
:::
:::

The only requirement for the current theory of forecast projection to work is the given base forecasts need to be unbiased. This is not a strict requirement as unbiasedness can be achieved by most of the common forecasting models with an intercept. Even if there are transformations applied to the data set before the models are fitted, bias correction can be applied as suggested by @PanEtAl2021. Note this is not a requirement on model specification: we do not assume the model producing the base forecast is correctly specified like in the DFM literature (e.g. @StoWat2002). In fact, the power of forecast projection manifests when the models are misspecified, as discussed in @sec-simulation.

::: {#lem-var}
The forecast variance covariance matrix of the component-constrained projected $h$-step-ahead  forecasts $\tilde{\bm{y}}_{t+h}$ is $\bm{M}\bm{W}_h$, i.e.
$$
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) = \bm{M}\bm{W}_h\bm{M}' = \bm{M}\bm{W}_h,
$$
and the forecast variance covariance matrix of the projected $h$-step-ahead forecasts $\tilde{\bm{z}}_{t+h}$ is
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-var
$$
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
= \Var(\bm{M}\hat{\bm{y}}_{t+h} - \bm{M}\bm{y}_{t+h}) \\
= \bm{M}\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h})\bm{M}' \\
= \bm{M}\bm{W}_h\bm{M}'.
$$
If we simplify it further, we have
$$
\begin{aligned}
\bm{M}\bm{W}_h\bm{M}'
&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h(\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})'\\
%&= (\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h)(\bm{I} - \bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h) \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&\mbox{}\hspace*{1cm} +  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
%&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h \\
&= \bm{M}\bm{W}_h.
\end{aligned}
$$
To get $\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})$, we just need to recognise that it is the first $m\times m$ leading principal submatrix of $\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})$.

:::
:::

@lem-var is a well known results (e.g. @DiGir2023) but was rarely focused on.

::: {#thm-psdvar}

#### Positive Semi-Definiteness of Variance Reduction

The difference between the forecast variance covariance matrix of the base forecast and the projected forecast
$$
\begin{aligned}
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) &=\bm{W}_h-\bm{M}\bm{W}_h\\
&= \bm{W}_h - (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h\\
&=\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h
\end{aligned}
$$
is positive semi-definite.
The difference between the forecast variance of $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$
$$
\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'
$$
is therefore positive semi-definite.

:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-psdvar
Trivially,
$\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$ and $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ are positive semi-definite. Note that $\Var(\hat{\bm{z}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{y}_{t+h})$ is the leading principal submatrix of $\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$, and the leading principal submatrix of a positive semi-definite matrix is positive semi-definite.
:::
:::

@thm-psdvar is why forecast projection works. The trace of $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ is the sum of forecast variances that can be reduced by forecast projection. Because the matrix is positive semi-definite, such trace is nonnegative. It means we can reduce the forecast variance by simply forecasting the components - the artificially constructed linear combination of the original data, and mapping the forecasts using matrix $\bm{M}$. For the improvement to be zero, the trace needs to be zero, and because the matrix is positive semi-definite, this implies that the entire $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ is a zero matrix, which rarely happens in practice. See @thm-pos-condition for more discussions.

We give a simple example to show how the variance reduction works.

::: {#exm-identityW}

#### Identity $W_h$

Let
$$
\bm{W}_h = \bm{I}_{m+p},
$$
that is, let $\bm{y}_t$ consist of $m$ original series and $p$ components whose forecasts are all uncorrelated with each other with variance $1$, then
$$
\begin{aligned}
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
&=
\bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C}\\
& =
\begin{bmatrix} -\bm{\Phi}'\\ \bm{I}_{p} \end{bmatrix}
(\bm{\Phi}\bm{\Phi}' + \bm{I})^{-1}
\begin{bmatrix} -\bm{\Phi}& \bm{I}_{p} \end{bmatrix} ,
\end{aligned}
$$
where
$$
\bm{C} = \begin{bmatrix} -\underset{p \times m}{\bm{\Phi}}& \underset{p \times p}{\bm{I}_{p}} \end{bmatrix}.
$$
Let $\bm{\Phi}$ consists of unit vectors that are orthogonal to each other, for example, those obtained from Principal Component Analysis [PCA, see @Jol2002 among others]. That is,
$$
\bm{\Phi}\bm{\Phi}' = \bm{I}_p\text{ when } p\le m
$$
and
$$
\bm{\Phi}'\bm{\Phi} = \bm{I}_m\text{ when } p= m,
$$
then
$$
\begin{aligned}
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
& =
\frac{1}{2}
\begin{bmatrix}
\bm{\Phi}'\bm{\Phi} & -\bm{\Phi}'\\
-\bm{\Phi} & \bm{I}_p
\end{bmatrix}\text{ when } p\le m. \\
& =
\frac{1}{2}
\begin{bmatrix}
\bm{I}_m  &-\bm{\Phi}'\\
-\bm{\Phi} & \bm{I}_p
\end{bmatrix}\text{ when } p=m.
\end{aligned}
$$
We only focus on the forecast variance of the original series, which is $\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) = \frac{1}{2}\tr(\bm{\Phi}'\bm{\Phi})$.
When $p<m$, since $\bm{\Phi}'\bm{\Phi}$ is idempotent (i.e. $(\bm{\Phi}'\bm{\Phi})(\bm{\Phi}'\bm{\Phi})= \bm{\Phi}'\bm{\Phi}\bm{\Phi}'\bm{\Phi} = \bm{\Phi}'\bm{\Phi}$), we have $\tr(\bm{\Phi}'\bm{\Phi}) = \operatorname{rank}(\bm{\Phi}'\bm{\Phi}) = p$. The reduction on the forecast variance of the original series is $p/2$.
When $p=m$, we have $\tr(\bm{\Phi}'\bm{\Phi}) = \tr(\bm{I}_m) = m$, and the reduction is $m/2$.
If we have two series ($m=2$) to begin with, using $1$ component ($p=1$) in the mapping will reduce the sum of forecast variance by $0.5$, and using $2$ components ($p=m=2$) will reduce the sum of forecast variance by $1$, that is a $50\%$ reduction when the original sum of forecast variance is only $2$.

If we keep increasing the number of components, the result in @thm-psdvar still holds, but $\bm{\Phi}$ can no longer contain orthogonal vectors, and the example here becomes intractable. This is an artificial example in the sense that the forecast variance $\bm{W}_h$ can hardly be identity in practice, as the forecasts of a linear combination of two series are likely to be correlated with the forecasts of these series. Nonetheless, we can see how the forecast variance can be reduced as a result of uncorrelatedness between the forecast of components and the series, representing the new information brought by the components.
:::

The variance reduction becomes larger as we increase the number of component $p$ from $1$ to $2$ in @exm-identityW. In fact, this is not a coincidence but rather a much desired property of forecast projection, as shown in the next section.

## Monotonicity

In the results that follow, we break the base forecast variance covariance matrix into smaller blocks:
$$
\bm{W}_h =
\begin{bmatrix}
\bm{W}_{z, h} & \bm{W}_{zc, h} \\
\bm{W}_{cz, h} & \bm{W}_{c, h}
\end{bmatrix},
$$
where $\bm{W}_{z, h}$ is the forecast variance covariance matrix of $\hat{\bm{z}}_{t+h}$,  $\bm{W}_{c, h}$ is the forecast variance covariance matrix of $\hat{\bm{c}}_{t+h}$,
and $\bm{W}_{zc, h}$ ($\bm{W}_{cz, h}$) consists of covariance between elements of $\hat{\bm{z}}_{t+h}$ and $\hat{\bm{c}}_{t+h}$ ($\hat{\bm{c}}_{t+h}$ and $\hat{\bm{z}}_{t+h}$).

::: {#thm-monotone}

#### Monotonicity
The sum of reduced forecast variance
$$
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) = \tr(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}')
$$ {#eq-trdvar}
is non-decreasing as $p$ increases.^[We thank Daniele Girolimetto for contributing to the initial proof (of the second statement in the proof) in his unpublished work.]
:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-monotone
Suppose now that we want to include $q$ more components $\bm{c}_{t}^*=\bm{\Phi}^*\bm{z}_t$ in the projection. We define $\bm{y}_{t}^* = \begin{bmatrix}\bm{y}_{t}\\ \bm{c}_{t}^*\end{bmatrix}$, the constraint matrix
$$
\bm{C}^* =
\begin{bmatrix}
\multicolumn{2}{c}{\bm{C}} & \underset{p\times q}{\bm{0}} \\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q}
\end{bmatrix}
=
\begin{bmatrix}
-\underset{p \times m}{\bm{\Phi}}& \bm{I}_{p} &\underset{p\times q}{\bm{0}}\\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q},
\end{bmatrix}
=
\begin{bmatrix}
\overline{\bm{C}} \\ \underline{\bm{C}}
\end{bmatrix}
$$ {#eq-Cstar}
where $\overline{\bm{C}}$ contains the first $p$ rows of $\bm{C}^*$ and $\underline{\bm{C}}$ contains the remaining $q$ rows of $\bm{C}^*$,
the forecast variance covariance matrix
$$
\Var(\hat{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*) = \bm{W}_h^* = \begin{bmatrix}\bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c,h}^*\end{bmatrix}.
$$
where $\hat{\bm{y}}_{t+h}^*$ is the $h$-step-ahead base forecasts of $\bm{y}_{t}^*$:
$$
\hat{\bm{y}}_{t+h}^* = \begin{bmatrix} \hat{\bm{y}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix},
$$
and the corresponding
$$
\bm{M}^* = \bm{I} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*.
$$

Proving @thm-monotone requires proving the following two items.

1. Including additional components in the mapping without including corresponding component constraints is equivalent to not including these additional components at all.
1. For a fixed set of components to be included in the mapping, adding constraints will reduce forecast variance.

We start by proving the first statement. Consider the case where we include the additional series $\bm{c}_{t}^*$ without using the additional constraint $\bm{\Phi}^*$. Defining $\bm{M}^{+}$ only with $\overline{\bm{C}}$:
$$
\bm{M}^{+} = \bm{I}_{m+p+q} - \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}},
$$ {#eq-Mplus}
we have $\tilde{\bm{y}}_{t+h}^+ = \bm{M}^{+}\hat{\bm{y}}_{t+h}^*$. Furthermore, we have
$$
\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c}^*
    \end{bmatrix}
    \begin{bmatrix}
      \bm{C}'\\ \underset{q\times p}{\bm{0}} \\
    \end{bmatrix} \\
 = \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
$$
and
$$
  \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix} \\
 =\bm{C}\bm{W}_h\bm{C}',
$$
which gives
$$
\begin{aligned}
\bm{M}^{+}
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
    (\bm{C}\bm{W}_h\bm{C}')^{-1}
    \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}\\
%& = \bm{I}_{m+p+q} -
%    \begin{bmatrix}
%      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1} \\
%      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}
%    \end{bmatrix}
%    \begin{bmatrix}
%      \bm{C} & \underset{p\times q}{\bm{0}} \\
%    \end{bmatrix} \\
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
    \end{bmatrix},
\end{aligned}
$$
and
$$
\begin{aligned}
\tilde{\bm{y}}_{t+h}^+
& = \bm{M}^{+}\hat{\bm{y}}_{t+h}^* \\
& = \left(
      \bm{I}_{m+p+q} -
      \begin{bmatrix}
        \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
        \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
      \end{bmatrix}
    \right)
    \begin{bmatrix} \hat{\bm{y}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix}\\
%& = \begin{bmatrix}
%      \hat{\bm{y}}_{t+h} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}\\
%      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
%    \end{bmatrix} \\
& = \begin{bmatrix}
      (\bm{I}_{n+p} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\hat{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \bm{M} \hat{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \tilde{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix}.
\end{aligned}
$$
If we only consider the forecast performance relevant to $\bm{z}_{t+h}$, and define $\bm{J}^* = \bm{J}_{m,p+q}= \begin{bmatrix}\bm{I}_{m} & \bm{0}_{m\times(p+q)} \end{bmatrix}$, we have
$$
\tilde{\bm{z}}_{t+h}^+ = \bm{J}^*\tilde{\bm{y}}_{t+h}^+ = \bm{J}\tilde{\bm{y}}_{t+h} = \tilde{\bm{z}}_{t+h}.
$$
This means adding additional components without imposing the corresponding constraints will yield the same projected forecasts as if these additional components are not added, which implies that the forecast variance stays the same:
$$
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}) = \Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$ {#eq-acnc}
<!-- additional components no constraints -->
This finishes the proof of the first statement. Now we move on to proving the second statement. We have the forecast variance matrices
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) & = \bm{M}^{+}\bm{W}_h^*
    = (\bm{I}_{m+p+q} -\bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
\text{and}\qquad\qquad
\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*) & = \bm{M}^*\bm{W}_h^*
    = (\bm{I}_{m+p+q} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*)\bm{W}_h^*.
\end{aligned}
$$
Taking the difference, we have
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) - \Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)
& = (\bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
& = \bm{W}_h^*(\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*.
\end{aligned}
$$
Using block matrix inversion, we have
$$
\begin{aligned}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    \end{bmatrix}^{-1}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& =   \overline{\bm{C}}'a \overline{\bm{C}}
    + \overline{\bm{C}}'b \underline{\bm{C}}
    + \underline{\bm{C}}'c\overline{\bm{C}}
    + \underline{\bm{C}}'d\underline{\bm{C}},
\end{aligned}
$$
where
$$
\begin{aligned}
a &= (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
     (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
  & \mbox{}\hspace*{1cm}
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' - \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}
    \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \\
  & = (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
b & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' -
     \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
     \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}\\
  & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1},\\
c & = - (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
d & = (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}.
\end{aligned}
$$
Thus,
$$
\begin{aligned}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}'\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
&=
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}.
\end{aligned}
$$
Therefore,
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) - \Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)
\hspace*{-4cm} \\
& =
\bm{W}_h^*(
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
-\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*\\
& =
\bm{W}_h^*(
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
)\bm{W}_h^*
\end{aligned}
$$
is positive semi-definite. This concludes the proof of the second statement.
Combining the results above, we have
$$
\begin{aligned}
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) -
\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h})
& =
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}) -
\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}) \\
&=
\bm{J}^*\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*)\bm{J}^{*\prime} -
\bm{J}^*\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)\bm{J}^{*\prime} \\
&=
\bm{J}^*
\bm{W}_h^*
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
\bm{W}_h^*
\bm{J}^{*\prime}
\end{aligned}
$$ {#eq-addcom}
being positive semi-definite.
Finally, we have
\begin{multline*}
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}))
-
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) \\
= \tr(
    \Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) -
    \Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h})
)
\end{multline*}
being the trace of a positive semi-definite matrix, which is non-negative. This means using a larger number of components in the mapping achieves a lower sum of forecast variance, giving @thm-monotone.
:::
:::

@thm-monotone is the key result that justifies the usefulness of forecast projection by providing a practical way to increase its power. It means we can keep increasing the number of components to reduce forecast variance, even when the number of components exceeds the number of original series, assuming we know the true base forecast variance covariance of all the series and components. It requires $\bm{C}$ to be $\begin{bmatrix} -\bm{\Phi}&\bm{I}_{p} \end{bmatrix}$ or $\begin{bmatrix} -\bm{\Phi}& {\bm{L}} \end{bmatrix}$ where $\bm{L}$ is a lower triangular matrix (the upper right corner needs to be all $0$s). This implies that the components can also be constructed from existing components, not only from the original series. This has little significance since a linear combination of components (which are linear combinations themselves) of the original series, is just a linear combination of the original series.

Extending the proof of @thm-monotone, we can outline the condition for the reduced variance to be positive. That is, if the new component satisfies these relationships with the previous (projected) forecasts, the projected forecast variance with the new component is smaller than the base forecast variance or the projected variance with only the previous components. Denote $\bm{\phi}_i$ as the row vector containing the weights associated with the $i$th component, so that with $p$ components, the weights matrix is $\bm{\Phi} = \begin{bmatrix}\bm{\phi}_1' & \bm{\phi}_2' & \dots & \bm{\phi}_p'\end{bmatrix}'$.

::: {#thm-pos-condition}

#### Positive Variance Reduction Condition

For the first component to have a guaranteed reduction of forecast variance (for the reduced variance matrix in @thm-psdvar to have positive trace), the following condition must be satisfied:
$$
\bm{\phi}_1\bm{W}_{z,h}\neq\bm{W}_{c_1z,h},
$$ {#eq-pcon}
where $\bm{W}_{c_{1}z,h}$ denotes the base forecast covariance of the first components and the original series.
For the $i$th component to have a positive reduction on forecast variance of the original series, the following condition is satisfied:
$$
\bm{\phi}_i{\bm{W}}^{(i-1)}_{\tilde{z},h}\neq\bm{W}_{\tilde{c}_i\tilde{z},h}^{(i-1)},
$$ {#eq-pconi}
where $\bm{W}^{(i-1)}_{\tilde{z},h}$ denotes the projected forecast variance of the original series using the first $i-1$ components, and $\bm{W}_{\tilde{c}_i\tilde{z},h}^{(i-1)}$ denotes the covariance between the projected forecast of the original series using the first $i-1$ components and the projected forecast of the $i$th component using the first $i-1$ components. See the proof in the Appendix for an algebraic definition.

:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-pos-condition

Denote $\bm{\psi}_i=\begin{bmatrix}-\bm{\phi}_i & \bm{0}_{1\times(i-1)} & 1\end{bmatrix}$ and $\bm{W}_{h}^{(i)}$ to be the base forecast variance of the original series and the first $i$ components. Starting with the first component, @eq-trdvar becomes
$$
\tr(\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1'(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}') =
(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}'\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1',
$$ {#eq-trdvar1}
$$
\begin{aligned}
\text{where}\qquad\qquad
\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' =&
\begin{bmatrix}
-\bm{\phi}_1 & 1
\end{bmatrix}
\begin{bmatrix}
\bm{W}_{z,h} & \bm{W}_{zc_1,h} \\
\bm{W}_{c_1z,h} & \bm{W}_{c_1,h}
\end{bmatrix}
\begin{bmatrix}
\bm{I}_m\\ 0
\end{bmatrix} \\
=& -\bm{\phi}_1\bm{W}_{z,h} + \bm{W}_{c_1z,h}.
\end{aligned}
$$
@eq-trdvar1 is obviously non-negative. For it to be larger than $0$, we need $\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' \neq 0$, which gives $\bm{\phi}_1\bm{W}_{z,h} \neq \bm{W}_{c_1z,h}$.

When it comes to adding the $i$th component on top of the first $i-1$ components, we define
$$
\overline{\bm{C}}_i =
\begin{bmatrix}
\bm{\psi}_1 & \bm{0}_{1\times i} \\
\bm{\psi}_2 & \bm{0}_{1\times (i-1)} \\
\vdots & \vdots \\
\bm{\psi}_i & 0 \\
\end{bmatrix}
$$
and
$$
\bm{M}^+_i = \bm{I}_{m+i} - \bm{W}_h^{(i)} \overline{\bm{C}}_{i-1}'
(\overline{\bm{C}}_{i-1}\bm{W}_h^{(i)}\overline{\bm{C}}_{i-1}')^{-1}
\overline{\bm{C}}_{i-1}
$$
analogously to @eq-Cstar and @eq-Mplus.
Following @eq-addcom, the additional reduction of forecast variance when adding the $i$th component becomes
$$
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
=
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i
\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'.
$$
Similar to before, we would want $\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}' \neq \bm{0}$. Note that $\bm{\psi}_i$ concerns the first $m$ rows and the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, and $\bm{J}_{m,i}'$ concerns the first $m$ columns. Combined with the implication from @eq-acnc that the $m\times m$ leading principal submatrix in equation $\bm{J}_{m,i}\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}'=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the same, we suppress the straightforward yet tiresome details, and obtain
$$
\bm{\phi}_i\bm{W}_{\tilde{z},h}^{(i-1)} \neq \big[\bm{0}_{1\times m+i-1} ~~~ 1 \big]\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}',
$$
where $\bm{W}_{\tilde{z},h}^{(i-1)}=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the projected forecast variance of the original series using the first $i-1$ components, and the right hand side of the inequality is simply a one-row matrix consisting of the first $m$ elements in the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, which can be denoted as $\bm{W}_{\tilde{c}_i\tilde{z},h}^{(i-1)}$ and interpreted as the covariance between the projected forecast of the original series using the first $i-1$ components, and the projected forecast of the $i$th component using the first $i-1$ components.
:::
:::

If we look at each element of the terms in @eq-pcon, we can interpret the condition in the following way: for at least one series, the forecast covariance between the new component and this series is not exactly a linear combination, defined by $\bm{\phi}_1$, of the forecast variance of this series and the forecast covariance between this series and all other series. For the $i$th component in @eq-pconi, the interpretation is similar, where the linear combination is defined by $\bm{\phi}_i$ and the forecast variance is replaced by the projected forecast variance and covariance associated with the first $i-1$ components. This interpretation constitutes a measure of forecast information. For the new component to be beneficial, the information brought by this new component, measured as the covariance, cannot be a combination of already existing information.

@thm-pos-condition can potentially provide insights into the selection of component weights and forecast models to satisfy the conditions. We leave this issue in a later article, as practically the conditions in @thm-pos-condition are either almost always satisfied if the weights are simulated randomly on a continuous scale, or the loss associated with the rare occasions where the conditions are not satisfied is neglectable compared to the estimation error imposed by the limited sample size as the number of components increases, as discussed in @sec-simulation and @sec-empirical-applications.


## Alternative Interpretations

@eq-y_tilde can be seen as a solution to the optimisation problem:
$$
\begin{aligned}
\underset{\check{\bm{y}}_{T+h}}{\arg\min} (\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h}) &\\
\text{s.t. } \bm{C}\check{\bm{y}}_{T+h}=0&.
\end{aligned}
$$
In this case, the projection can be interpreted as finding the set of forecast that is closest (on the transformed space) to the base forecast that satisfies the linear constraints imposed by the components.

Moreover, this is equivalent to the optimisation problem:
$$
\begin{aligned}
\underset{\check{\bm{y}}_{T+h}}{\arg\min} (\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h}) &\\
\text{s.t. } \bm{\Phi}\check{\bm{z}}_{T+h}=\check{\bm{c}}_{T+h}&,
\end{aligned}
$$
where $\check{\bm{c}}_{T+h}$ is the vector of the last $p$ elements of $\check{\bm{y}}_{T+h}$, corresponding to the forecast of the components as part of the solution. This equivalence is discussed in @WicEtAl2019, where the authors find the solution by minimising the sum of forecast variance of all series (See @AndNar2022 for a corrected proof). The result is the MinT solution
$$
\tilde{\bm{y}}_{t+h} = \bm{S}\bm{G} \hat{\bm{y}}_{t+h},
$$ {#eq-mint}
where
$\bm{S} = \begin{bmatrix}\bm{I}_m \\\bm{\Phi}\end{bmatrix}$ contains the constraints in a different order from $\bm{C}$, so we also have $\bm{y}_{t} = \bm{S}\bm{z}_{t}$, and
$$
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$ {#eq-G}
In @eq-mint, $\bm{G} \hat{\bm{y}}_{t+h}$ can be viewed as mapping all the series to a selected few. In the forecast reconciliation context, this is mapping series at all levels to the bottom level series; in our multivariate forecasting context, this is mapping all series including the given number of components, to the original series. The structure matrix $\bm{S}$ in the forecast reconciliation context is to map the forecast at the bottom level to the entire hierarchical structure; in our context, this is not necessary, as our focus is on the forecast of the original series only, regardless of the forecasting performance on the components. Thus, our solution in the multivariate forecast context in @eq-lcmap is equivalent to finding the $\bm{G}$ in
$$
\tilde{\bm{z}}_{t+h} = \bm{G}\hat{\bm{y}}_{t+h}.
$$ {#eq-stmap}
Recognising @eq-y_tilde is equivalent to @eq-mint, the solution in @eq-G is the solution that minimises the sum of variance of original series and all the components. Here we show in @thm-minvar that @eq-G is also the solution to minimise each individual variance (and the sum) of the original series only, which is optimal in @eq-stmap. This can be viewed as a special case of Theorem 3.3 of @PanEtAl2021, or as illustrated by @AndNar2022, but applied in a different context from forecast reconciliation. The early work we can find that noted this interpretation in a non-forecasting context can go back as far as @Lue1969 [p. 85]. We establish a few basic results leading to the optimality of this solution first, also to check that
@lem-coherent-[-@lem-var]
<!-- [@lem-coherent; -@lem-var] -->
<!-- [@lem-coherent; -@lem-self-proj; -@lem-unbiased; and @lem-var] -->
hold under this alternative representation. It may not be immediately obvious that the constraints imposed by the components are satisfied by looking at @eq-stmap only, but this can be seen from @eq-mint and @lem-coherent can be easily checked without referencing to a specific $\bm{G}$.
<!-- Lemma: component constraint -->

::: {#lem-stcoherent}
The projected forecast in @eq-stmap satisfies the constraint such that
$$\bm{C}\tilde{\bm{y}}_{t+h}= \bm{C}\bm{S}\tilde{\bm{z}}_{t+h}= \bm{0}.$$
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-stcoherent
$$
\bm{C}\tilde{\bm{y}}_{t+h}
= \bm{C}\bm{S}\tilde{\bm{z}}_{t+h}
= \begin{bmatrix}-\bm{\Phi} & \bm{I} \end{bmatrix}
\begin{bmatrix}\bm{I} \\ \bm{\Phi} \end{bmatrix}\tilde{\bm{z}}_{t+h}
= (-\bm{\Phi} + \bm{\Phi})\tilde{\bm{z}}_{t+h}
= \bm{0}.
$$
:::
:::

::: {#lem-st-self-proj}
For $\bm{y}_{t+h}$ that already satisfies the constraint, the projection does not change its value:
$$
\bm{G}\bm{y}_{t+h} = \bm{z}_{t+h}.
$$
:::
::: {.toappendix}
::: {.proof}
#### Proof of @lem-st-self-proj

$$
\bm{G}\bm{y}_{t+h} =(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\bm{y}_{t+h}=(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{z}_{t+h}= \bm{z}_{t+h}.
$$
:::
:::




<!-- Lemma: unbiased constraint -->
::: {#lem-st-unb-const}

If the base forecasts are unbiased such that
$$
\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t),
$$
then the mapping in @eq-stmap are also unbiased:
$$
\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t),
$$
provided
$$
\bm{G}\bm{S} = \bm{I}.
$$
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-st-unb-const
If $\bm{G}\bm{S} = \bm{I}$, then
$$
\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) =
\E(\bm{G}\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{G}\bm{S}\bm{z}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t).
$$
:::
:::
<!-- Lemma: Var -->
::: {#lem-st-var}

The forecast variance covariance matrix of the mapped forecasts from @eq-stmap is given by
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{G}\bm{W}_h\bm{G}'.
$$

:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-st-var
Let the base and projected forecast errors be given as
$$
\begin{aligned}
\hat{\bm{e}}_{z,t+h} &= \bm{z}_{t+h} - \hat{\bm{z}}_{t+h},\\
\hat{\bm{e}}_{y,t+h} &= \bm{y}_{t+h} - \hat{\bm{y}}_{t+h},\\
\tilde{\bm{e}}_{z,t+h} &= \bm{z}_{t+h} - \tilde{\bm{z}}_{t+h},\\
\text{and}\qquad
\tilde{\bm{e}}_{y,t+h} &= \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}
= \bm{S}\bm{z}_{t+h} - \bm{S}\tilde{\bm{z}}_{t+h}
= \bm{S} \tilde{\bm{e}}_{z,t+h}. \\
\text{Then we have}\qquad
\tilde{\bm{e}}_{y,t+h}
&= \hat{\bm{e}}_{y,t+h} + \hat{\bm{y}}_{t+h} - \tilde{\bm{y}}_{t+h}\\
&= \hat{\bm{e}}_{y,t+h} + \hat{\bm{y}}_{t+h} - \bm{S}\bm{G}\hat{\bm{y}}_{t+h}\\
&= \hat{\bm{e}}_{y,t+h} + (\bm{I} - \bm{S}\bm{G})(\bm{y}_{t+h} - \hat{\bm{e}}_{y,t+h})\\
\text{and}\qquad
&= \bm{S}\bm{G}\hat{\bm{e}}_{y,t+h} + (\bm{I} - \bm{S}\bm{G})\bm{S}\bm{z}_{t+h}\\
\bm{S} \tilde{\bm{e}}_{z,t+h}&= \bm{S}\bm{G}\hat{\bm{e}}_{y,t+h},
\end{aligned}
$$
where the last line comes from $\bm{G}\bm{S} = \bm{I}$.
Left multiply $\bm{G}$ to both sides, we have\pagebreak[3]
$$
\bm{G}\bm{S} \tilde{\bm{e}}_{z,t+h} = \bm{G}\bm{S}\bm{G}\hat{\bm{e}}_{y,t+h}
\qquad\text{and}\qquad
\tilde{\bm{e}}_{z,t+h} = \bm{G}\hat{\bm{e}}_{y,t+h},
$$
and therefore
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) =\Var(\tilde{\bm{e}}_{z,t+h})=\Var(\bm{G}\hat{\bm{e}}_{y,t+h})= \bm{G}\Var(\hat{\bm{e}}_{y,t+h})\bm{G}'= \bm{G}\bm{W}_h\bm{G}'.
$$

:::
:::


Putting them together, we have the following theorem.

::: {#thm-minvar}

#### Minimum Variance Unbiased Projected Forecast

The solution to
$$
\begin{aligned}
\underset{\bm{G}}{\arg\min}\ \bm{G}\bm{W}_h\bm{G}'&\\
\text{s.t. } \bm{G}\bm{S} = \bm{I}&
\end{aligned}
$$ {#eq-obj}
is @eq-G.

Recognising $\bm{G}$ is of dimension $m\times (m+p)$, this problem can be effectively split into independent subproblems such that
$$
\bm{G} = \begin{bmatrix}\bm{g}_1 & \bm{g}_2 &\dots &\bm{g}_n \end{bmatrix}',
$$
where $\bm{g}_i$ is the solution to the subproblem of the $i$th series
$$
\begin{aligned}
\underset{\bm{g}_i}{\arg\min}\ \bm{g}_i'\bm{W}_h\bm{g}_i&\\
\text{s.t. } \bm{g}_i'\bm{s}_{j} = \delta_{ij}, &\quad j= 1, 2, .\ldots, m,
\end{aligned}
$$ {#eq-subobj}
where $\delta_{ij}=\left\{\begin{array}{ll} 1 & i=j\\0 &j\neq j \end{array}\right.$ is the Kronecker delta function.

:::

::: {.toappendix}
::: {.proof}
#### Proof of @thm-minvar

This can be proved in a few different ways. We adopt the approach of @AndNar2022 to obtain the solution to @eq-obj, but the procedure from @Lue1969 [p. 85] can also be used, where the problem is divided to @eq-subobj and reconstructed to find the solution to @eq-obj.

There exists a Lagrange multiplier $\bm{\Lambda}$ such that
$$
L(\bm{G}) = \tr(\bm{G}\bm{W}_h\bm{G}') + \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S}))
$$
is stationary at an extremum $\bm{G}$ [@Lue1969, p. 243, Theorem 1]. We set the Gateaux differential [@Lue1969, p. 171] to zero for any matrix $\bm{H}$:
$$
\begin{aligned}
\lim_{\alpha \to 0} \frac{L(\bm{G} + \alpha\bm{H}) - L(\bm{G})}{\alpha} &= 0 \\
\tr(\bm{G}\bm{W}_h\bm{H}') + \tr(\bm{H}\bm{W}_h\bm{G}') - \tr(\bm{\Lambda}'(\bm{H}\bm{S}))
  &=\tr(2\bm{H}\bm{W}_h\bm{G}'-\bm{\Lambda}'\bm{H}\bm{S})\\
  &= \tr(\bm{H}(2\bm{W}_h\bm{G}'-\bm{S}\bm{\Lambda}')) \\
  &=0 \\
2\bm{W}_h\bm{G} &= \bm{S}\bm{\Lambda}' \\
\bm{G}' &= \frac{1}{2}\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'.
\end{aligned}
$$
Multiplying $\bm{S}'$ to the left of both sides. we have
$$
\bm{S}'\bm{G}'=\bm{I} = \frac{1}{2}\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'
\qquad\text{and}\qquad
\bm{\Lambda}' = 2 (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
$$
because $\bm{G}\bm{S} = \bm{I}$. Putting it back in, we have
$$
\bm{G}' = \bm{W}_h^{-1}\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
\qquad\text{and}\qquad
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$
:::
:::
In other words, the forecast projection method gives optimal projected forecast for a given set of component, in the sense that the unbiased forecast of each series has minimum variance, which is a step further from the collective optimum where the sum of the variances is minimised.

## Estimation of $\bm{W}_h$

In practice, the base forecast variance $\bm{W}_h$ is unknown and needs to be estimated. Denote $\hat{\bm{e}}_{t,h} = \bm{y}_{t} - \hat{\bm{y}}_{t|t-h}$ as the $h$-step-ahead base forecast residual. The conventional forecast variance covariance matrix estimator
$$
\widehat{\bm{W}_h}=\frac{1}{T-1}\sum^T_{i=1}\hat{\bm{e}}_{t,h}\hat{\bm{e}}_{t,h}',
$$
albeit unbiased, is not considered a good approximation to the true forecast variance in a finite sample when $(m+p) \approx T$. It is even singular when $(m+p)>T$, which makes the quantities discussed in the previous sections impossible to calculate. For this reason, we adopt the variance shrinkage method by @SchStr2005, which is treated as the MinT(Shrink) method by @WicEtAl2019, and the covariance shrinkage method by @OpgStr2007. The estimated forecast variance matrix is guaranteed to be positive definite with few numerical problems. This estimator is denoted as $\widehat{\bm{W}}_h^{shr} = (\hat{w}_{ij,h}^{shr})_{1\leq i,j\leq m+p}$ whose elements are
$$
\hat{w}_{ij,h}^{shr} = \hat{r}_{ij,h}^{shr}\sqrt{\hat{v}_{i,h}\hat{v}_{j,h}}
$$
with
$$
\hat{r}_{ij,h}^{shr} = (1-\hat{\lambda}_{cor})\hat{r}_{ij,h}
$$
and
$$
\hat{v}_{i,h} = \hat{\lambda}_{var}\hat{w}_{h, median} + (1-\hat{\lambda}_{var})\hat{w}_{i, h},
$$
with $\hat{\lambda}_{cor}$ being the shrinkage intensity parameter for the correlation:
$$
\hat{\lambda}_{cor} =
\min(1,
\frac
{\sum_{i\neq j}\widehat{\operatorname{var}}(\hat{r}_{ij,h})}
{\sum_{i\neq j}\hat{r}_{ij,h}^2}
),
$$
and $\hat{\lambda}_{var}$ being the shrinkage intensity parameter for the variance:
$$
\hat{\lambda}_{var} =
\min(1,
\frac
{\sum_{i=1}^{m+p}\widehat{\operatorname{var}}(\hat{w}_{i,h})}
{\sum_{i=1}^{m+p}(\hat{w}_{i, h} - \hat{w}_{h, median})^2}
),
$$
where $\hat{r}_{ij,h}$ is the sample correlation of the $h$-step-ahead forecast error between the $i$th and the $j$th series (component) in $\bm{y}_t$, $\hat{w}_{i, h}$ is the $h$-step-ahead sample base forecast variance associated with the $i$th series (the $i$th diagonal element of $\widehat{\bm{W}_h}$) and $\hat{w}_{h, median}$ is the median of the $h$-step-ahead sample forecast variance of the series and components (the median of the diagonal elements of $\widehat{\bm{W}_h}$). The estimation of $\widehat{\bm{W}}_h^{shr}$ in the following sections are implemented using the package `corpcor` [@corpcor] in R [@R].

Estimating $\widehat{\bm{W}}_h^{shr}$ for each forecast horizon $h$ is desirable but computationally intensive. It involves the calculation of multi-step-ahead in-sample residuals of the forecast models, which is especially challenging for iterative forecasts. Because of this, in practice it is not unreasonable to assume the $h$-step forecast variance is proportional to the $1$-step forecast variance by a constant $\eta_h$, as do @WicEtAl2019:
$$
\widehat{\bm{W}}_h^{shr} = \eta_h\widehat{\bm{W}}_1^{shr}.
$$
Under this assumption, when $\widehat{\bm{W}}_h^{shr}$ is used in @eq-M, the proportionality constant $\eta_h$ cancels out regardless of the value of $h$. We can effectively use only the one-stop forecast variance in forecast projection, if only the point forecasts are concerned. We calculate $\widehat{\bm{W}}_h^{shr}$ for each $h$ for the simulation example in @sec-simulation, but assumes this proportionality for the application in @sec-empirical-applications.


# Simulation {#sec-simulation}

## Benchmarks

```{r simulation}
```

In this section, we illustrate the performance of forecast projection in a simulation example. In each sample, we simulate $T=400$ observations from a VAR($3$) process with $m=70$ variables. The coefficients of the VAR model are estimated from the first $70$ series in the Australian tourism data set used in @sec-australian-domestic-tourism. The innovation in the VAR model is simulated from a multivariate normal distribution with an identity variance covariance matrix. The estimation and simulation are done using package `tsDyn` [@tsDyn]. We simulate $220$ such samples and the forecast is evaluated on each sample.

The first benchmark we use is the univariate ARIMA model. For each series, we fit an ARIMA model using the `auto.arima()` function from the `forecast` package [@forecast]. The function implements an automatic model selection procedure proposed by @HynKha2008. The number of first differences is determined by repeated KPSS tests [@KwiEtAl1992] and the number of seasonal differences is determined by the seasonal strength computed from an STL decomposition [@CleEtAl1990]. The algorithm then chooses different orders of the autoregressive (AR) and moving average (MA) parts by comparing AICc between the corresponding models in a stepwise fashion, up to a maximal order of $5$. Univariate ARIMAs are also used to produce base forecasts of the components used in projection, regardless of the base model.

Another base model is the DFM following @StoWat2002a:
$$
\hat{y}_{T+h} = \hat{\alpha}_h + \sum^n_{j=1}\hat{\bm{\beta}}_{hj}'\hat{\bm{F}}_{T-j+1} + \sum^s_{j=1}\hat{\gamma}_{hj}y_{T-j+1},
$$
where $\hat{\bm{F}}_t$ is the vector of $k$ estimated factors, and $\hat{y}_t$ is the target series to forecast. The factors are estimated using PCA on demeaned and scaled data. The optimal model is selected for each series based on the Bayesian information criterion (BIC) from models fitted using different combinations of meta-parameters in their corresponding range: $1 \leq k \leq 6$, $1 \leq n \leq 3$ and $1 \leq s \leq 3$. Note here DFM produces direct forecasts in the sense that a different model is fitted for each forecast horizon $h$, compared to indirect forecast or iterative forecast.

We use different weighting methods to construct the components. The types of components are listed below. For the ones randomly simulated from a distribution, we normalise them into unit vectors to maintain some level of consistency with $\bm{\phi}_i/\sqrt{\sum_j(\phi_{ij}^2)}$ where $\phi_{ij}$ is the $j$th value in the weight vector of the $i$ components.

PCA+Norm
: The $m$ principal components in PCA are taken first, implemented with the `prcomp` function in package `stats` [@R]. Weights of the additional components are simulated from a standard normal distribution before normalised to unit vectors.

PCA+Unif
: The $m$ principal components in PCA are taken first, and the weights of the additional components are simulated from a uniform distribution with minimum $-1$ and maximum $1$ before normalised to unit vectors.

Norm
: The weights of components are simulated from a standard normal distribution before normalised to unit vectors.

Unif
: The weights of components are simulated from a uniform distribution with minimum $-1$ and maximum $1$ before normalised to unit vectors.

Ortho+Norm
: A random orthonormal matrix is generated using package `pracma` [@pracma] as the weights of the first $m$ components. Weights of the additional components are simulated from a standard normal distribution before normalised to unit vectors.

We employ the Friedman test [@Fri1937; @Fri1939] along with post-hoc Nemenyi tests [@Nem1963; See @HolEtAl2013 for details] to compare forecast performance between different methods. The analysis involves the use of Multiple Comparisons with the Best (MCB) plot introduced by @KonEtAl2005 to visualise the comparison. The mean squared error (MSE) of each series over different samples is calculated, and the MSEs of all the series are treated as observations in the Nemenyi test. Our objective is to assess whether there are statistically significant differences between the projected and base forecasts. The average ranks are plotted in @fig-simulation-mcb-series for forecast horizons $1$, $6$ and $12$. The methods using forecast projection are named "{Model}-{Comp. Weights}-{No. Comp.}". The maximum number of components is chosen to be $300$. The base models are named "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best-performing model. Methods outside the shaded region are significantly worse than the best model. We also plot the specific MSE values by the number of components $p$ in @fig-simulation-line. Here we include the performance of the true data generating process (DGP) VAR model (VAR - GDP), the estimated VAR model with the correct specification (VAR - Est.), and their projections. We do not include methods involving uniform distribution and random orthonormal matrices as they are visually identical to the methods with normal distribution. The vertical black line indicates the number of series $m=70$. We group our findings in the following categories.
```{r fig-simulation-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead MSE of different model and component specifications in the simulation. The methods using forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
#| fig-height: 8
```

```{r fig-simulation-line}
#| fig-cap: 'MSE of different forecast models and component construction methods by the number of components $p$ used in forecast projection in the simulation, for forecast horizons $1$, $6$ and $12$. "VAR - DGP" indicates the performance of the true data generating VAR model. "VAR - Est." indicates the performance of the VAR model with the same structure as the true model and estimated parameter values. The vertical black line indicates the location of $p=m$ the number of series.'
```

## Projection over base forecast

The first thing we note is the overall performance difference between the projected forecasts and the base forecast. In @fig-simulation-mcb-series, the average ranks of the projected forecasts are better than the corresponding base forecast at all forecast horizons, and the differences are all significant except the PCA-related ones with only $m=70$ components for forecast horizon $6$ and $12$. Note here we need to compare forecasts with the same model: the projected forecast of ARIMA to base ARIMA and the projected forecast of DFM to base DFM. The number of components seems important: the best-performing models are all with $300$ components. Between the one-step-ahead forecasts, the methods with $300$ components are not significantly different from each other, regardless of the forecast model or how we construct the components.

Indeed, from @fig-simulation-line we can see the MSEs for model ARIMA and DFM keep decreasing from the base forecast, as the number of components increases. This confirms @thm-monotone that the more components we include, the more variance reduction we can achieve. This is only obvious in this ideal setting where we have $400$ observations in each group while we only use at most $300$ components in the projection. This ample number of observations and the simple DGP can ease the challenge of estimation. This continued reduction in variance is not always achievable with real data, as we can see in @sec-empirical-applications, especially with FRED-MD in @sec-fred-md.

## Base forecast model

If we compare ARIMA and DFM, under a VAR DGP, we expect DFM to pick up the correlation between series and not univariate ARIMA, so DFM should have better performance over ARIMA. This is indeed the case. Looking at the base forecasts in @fig-simulation-mcb-series, base DFM is significantly better than base ARIMA, except for $h=1$ where it is close to significant. This is also obvious in @fig-simulation-line. The horizontal line representing the MSE of base DFM is always far below the horizontal line for base ARIMA.

With the help of forecast projection, a simple model like ARIMA can achieve comparable performance to more sophisticated models like DFM. In @fig-simulation-mcb-series, all projected ARIMA forecasts except the ones having $m$ PCs are significantly better than base DFM at $h=1$, and all projected ARIMA with $300$ components are significantly better than base DFM at $h=6$. In @fig-simulation-line, The valid and long-dashed lines of projected ARIMA with corresponding component construction go down monotonically as the number of components $p$ increases and reaches the MSE of base DFM at some point: at or below $m$ for $h=1$, at or above $m$ for $h=6$, and around $p=300$ for $h=12$. This is because forecast projection utilises shared information between series by capturing them in the components, making up for the overlooked correlation in univariate ARIMA models.

Interestingly enough, at $h=1$, while the MSE of projected DFM also goes down as $p$ increases, the MSE of the projected ARIMA and the MSE of the projected DFM seems to converge to the same value as $p$ reaches $300$, no matter how the components are constructed. Note here the same forecasts of the components, coming from univariate ARIMA of these components, are used for both the projected ARIMA and the projected DFM. This implies that much information in the series is not captured by ARIMA or DFM, but is captured by the components. As the number of components becomes high enough, the information captured by the components overpowers the information captured by the base models, dominating the performance of the projected forecasts. Once again, this emphasises the importance of the components and forecast projection. In this extreme case, the simple model is as good as the more complicated model after projection, because the forecast model itself is not as valuable as forecast projection.

This observation is not as obvious as the forecast horizon increases. This is because while ARIMA produces forecasts iteratively, DFM is a direct forecast model. With this simple DGP, the performance of DFM can be well maintained with larger $h$ since a different model is fitted for each $h$. This can be seen as the MSE of the base DFM does not change much with different $h$, but the MSE of base ARIMA keeps increasing as $h$ increases.

## Component construction {#sec-component-construction}

The construction of components is obviously important in forecast projection, but might not be as important as expected in this simulation example. In @fig-simulation-mcb-series, the main difference that can be observed exists between using a combination of PCA and random weights, and purely using random weights. The distribution that generates the random weights is less relevant: in @fig-simulation-mcb-series, with the same number of components and the same base ARIMA model, the MSEs are not significantly different, regardless of whether the weights are simulated from a normal distribution (Norm) or a uniform distribution (Unif), or a combination of random orthonormal weights and random normal weights (Ortho+Norm). The same conclusion can be found when PCA is used. As long as PCA is used, the performance is not different whether the additional components are simulated from a normal distribution (PCA+Norm) or a uniform distribution (PCA+Unif).

Because the distribution is less important, in @fig-simulation-line, we only look at the inclusion of PCA with distribution set to normal. When $p$ is smaller, the MSE drops faster when PCA is used, but the speed decreases as $p$ increases. The performance of forecasts without PCA reaches and exceeds the performance with PCA before the number of components reaches $m$, and stays in the lead thereafter, although the gap seems to diminish with large $p$. This difference of PCA comes from the variances of principal components being maximised and ranked from largest to smallest, not from the orthogonality of the components, because the performance of using random orthonormal weight matrix is the same as using only random normal weights as discussed before. This might suggest the use of simple random weights if one is going to include a lot of components in the projection and to use PCA only when the number of components is small, but as we can see in @sec-empirical-applications, this is not the case with real data, and PCA is the preferred components even when the number of components is large.

Different constructions of components remain an important aspect of forecast projection. One important future direction would be to find alternative and optimal components, as we do not limit the structure of the weight matrix in @sec-method. This should be studied together with the selection of the forecast model since both the weight matrix $\bm{\Phi}$ and the base forecast variance $\bm{W}_h$ are operatable and affect the projection simultaneously in @eq-M. This is likely to be an extension of the forecast combination literature, focusing on the properties of the base forecast, and the diversity and robustness of the forecast model and components. Examples of studies on this issue in the forecast combination literature include @BatDua1995, @KanEtAl2022 and @LicWin2020.

## Sources of uncertainty

At the bottom of each panel in @fig-simulation-line, the best forecasts come from the true DGP VAR model (the dashed green line that is partially covered by the solution green line). The forecast projection on the true model does not improve its forecast (the solid green line) as expected, as the uncertainty comes from the intrinsic error in the DGP that cannot be reduced. The second best forecast is from the estimated VAR model, as the uncertainty, apart from the intrinsic error, only comes from estimation error, not model misspecification error like ARIMA and DFM, which are both misspecified in this simulation example. The gap between the estimated VAR and the true VAR becomes bigger for a longer forecast horizon, because VAR produces iterative forecasts, and estimation error accumulates as $h$ becomes larger.

Forecast projection shows little, if any, improvement over the estimated VAR. This means forecast projection cannot reduce estimation error or the parameter uncertainty described in @PetEtAl2018. On the other hand, it shows significant improvement over misspecified base models. This implies that the uncertainty it can reduce is mainly the model misspecification error, or the model uncertainty in @PetEtAl2018, similar to how bagging reduces variance. The data uncertainty in @PetEtAl2018 is not examined and is less translatable to forecast projection, but it is partially the uncertainty associated with how the components are constructed as discussed in @sec-component-construction.


# Empirical applications {#sec-empirical-applications}

Here we apply forecast projection to two real data examples and draw most of the same conclusions in simulation, with a few key differences.

## Australian domestic tourism {#sec-australian-domestic-tourism}

```{r visnights}
```

The Australian Tourism Data Set compiled from the National Visitor Survey by Tourism Research Australia contains the total number of nights spent by Australians away from home, which we will refer to as visitor nights in what follows. The visitor nights are recorded monthly for each of the $m=77$ regions, covering the period from January 1998 to December 2019. To measure the performance of forecast projection, we conduct time series cross-validation. The first $T = 84$ observations are kept as the training sample of the first evaluation, and the following $12$ periods are taken as the test set on which the error is calculated. We repeat the evaluation for the rest of the data, with each training sample including one more observation than the previous one, and each test set shifting one period to the future.

The base forecasts of both the series and the components are produced by univariate ETS models selected and fitted using the `ets()` function in the `forecast` package [@forecast; @HynKha2008]. In an ETS model, different term treats different patterns of a time series: the trend term treats the direction of the long-term tendency, the seasonality term treats the periodically recurring pattern with a fixed periodicity, and the error term measures the uncertainty. There does not need to be a trend term or seasonality term. If a trend term exists, we allow it to be additive or addictive damped. If a seasonality term exists, it can be addictive or multiplicative. The error can be additive or multiplicative. Excluding models with numerical instabilities, we choose the model with the smallest AICc among the $15$ models.

```{r fig-visnights-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the visitor nights data. The methods using forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```

```{r fig-visnights-line}
#| fig-cap: "MSE of different component construction methods by the number of components $p$ used in forecast projection with ETS models on the visitor nights data, for forecast horizons $1$, $6$ and $12$. The vertical black line indicates the location of $p=m$ the number of series."
```

The MCB plot and the MSE plot can be found in @fig-visnights-mcb-series and @fig-visnights-line. Most of the findings are consistent with @sec-simulation. From @fig-visnights-mcb-series, the base forecast is always ranked last. Even projections with only one component are significantly better than the base forecast for $h=6$ and $12$.

We highlight two differences. Firstly, from @fig-visnights-line, the MSE of projection does not always go down as the number of components increases, especially for $h=1$. This can also be seen from @fig-visnights-mcb-series, where the two best methods are not significantly different, even though they have very different numbers of components $m=77$ and $m=200$. Intuitively, choosing the number of components is a tradeoff between the increasing estimation error as the dimension of forecast variance $\bm{W}_h$ increases, and the additional benefit brought by the information embedded in the new components, depending on the complexity of the DGP. For the visitor nights data set, the benefit of components above the estimation error diminishes after the number reaches about $m=77$.

Secondly, unlike @sec-simulation, using PCA as components is significantly better than simply using random normal weights, for the same large enough number of components (@fig-visnights-mcb-series). This is also clear from @fig-visnights-line, where the reduction of variance from using PCA is always in the lead, even after the $m$ PCAs are exhausted and random normal weighted components are added. As discussed in @sec-component-construction, the rationale awaits future research, but we propose two potential explanations for this superior performance, related to the variance maximisation and ranking of PCA: 1. Optimality: By maximizing and ranking the variance of PCs from largest to smallest, we ensure that the projection utilizes components containing significantly more information (as measured by variance) compared to randomly weighted components; 2. Diversity: Actively seeking PCs with the highest variance results in the incorporation of a more diverse set of components into the projection.

## FRED-MD {#sec-fred-md}

```{r fred-md}
```

The FRED-MD [@McCNg2016] data set is a popular monthly data set for macroeconomic variables. It shares similar properties with the @StoWat2002 (and others) data. We download and transform the data set using the `fbi` [@fbi] package. The period we use for this exercise is from January 1959 to September 2023, containing $777$ observations. Following @McCNg2016, we replace observations that deviate from the sample median by more than 10 interquartile ranges, which are recognised as outliers, with missing values. We then drop any series with more than $5\%$ observations missing. This left us with $m=122$ series. We fill in the missing values using the expectation-maximization (EM) algorithm described in @StoWat2002a with $8$ factors. The number $8$ is identified by @McCNg2016, albeit with a different time span. As the theory shows a reduction in forecast variance, we want to use MSE as the error measure, instead of other scaled or percentage error measures. To reliably calculate MSE over series with different scales, we demean the series to have mean $0$ and scale the series to have variance $1$. The MSEs are calculated on this standardised scale without back-transformation.

Similar to the visitor nights data, we evaluate the performance of forecasts using time series cross-validation. Starting with $300$ observations in the first training set and the following $12$ observations as the test set, we repeat the evaluation for the rest of the data with the size of the training set increasing $1$ in each turn. The base models are the univariate ARIMA model and the DFM model, as described in @sec-simulation. The ranges of the meta-parameters in the DFM models are $1\leq k \leq 8$ (since $8$ factors are identified and used to fill in the missing values), $1 \leq n \leq 3$ and $0 \leq s \leq 6$. The MCB plot and the MSE plot can be found in @fig-fred-md-mcb-series and @fig-fred-md-line.

```{r fig-fred-md-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the FRED-MD data. The methods using forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```
```{r fig-fred-md-line}
#| fig-cap: "MSE of different forecast models and component construction methods by the number of components $p$ used in forecast projection on the FRED-MD data, for forecast horizons $1$, $6$ and $12$. The vertical black line indicates the location of $p=m$ the number of series."
```

The first thing we note is the performance of base ARIMA exceeds that of the base DFM. This difference is significant at $h=6$ (@fig-fred-md-mcb-series). In terms of forecast projection, the best models at all forecast horizons are still forecast projections with PCA, and they are significantly better than the base models at $h=1$ and $6$. The fact that the projection with PCA is better than random weights, which can be seen from both the rankings in @fig-fred-md-mcb-series and the MSE in @fig-fred-md-line, reaffirms our finding about the difference between PCA and random weights in @sec-australian-domestic-tourism.

In @fig-fred-md-line, the forecast projection seems to be worse than the base models at the beginning, when the number of components is small, but improves with larger $p$ and outperforms the base models gradually. As naturally the tradeoff between the benefit of new components and the difficulty of estimation of a large dimension still happens here, it seems to be more extreme: the MSEs start to increase as $p$ increases, once $p$ becomes larger than $m$. The projected forecast even worsens to the same level as the base forecast for ARIMA at $h=6$ and DFM at $h=12$ when $p=300$. The turning point seems to be at $m$, but as we have seen in @sec-simulation and @sec-australian-domestic-tourism, $m$ is not the clear cut-off point. Where the performance of forecast projection turns should be jointly determined by the number of series $m$, the sample size $T$, the component construction method, and the DGP. In the case of FRED-MD, it signals the importance of PCA, as $m$ is the point that the component changes from PCA to random normal weighted linear combinations, implying PCA can exploit the information in the data while random weights cannot. This is more obvious for $h=1$ and $h=6$, as PCA works when $p<m$, but random normal weights do not seem to work from the beginning.


# Conclusion {#sec-conclusion}

In this article, we propose a new approach called forecast projection that can reduce forecast variance on top of given forecasts without requiring any additional information, by projecting the forecasts of the series and the forecasts of their linear combinations in a specific way. We prove in theory, that the forecast projection can reduce forecast variance with the help of the components, and it can be reduced monotonically by including more components, assuming we know the base forecast variance matrix. Furthermore, we show that for a given number of components, within the class of linear projection, the proposed method is the best in the sense that it minimises the forecast variance of the series. To handle the difficulty of estimating the forecast variance matrix, we suggest using a shrinkage estimator, which shrinks variances toward their median and covariances toward zero.

We illustrate the proposed forecast projection outperforms base forecasts significantly and confirm the theoretical results in simulation and two empirical applications on the Australian domestic tourism data set and the FRED-MD data set. We find using PCA to construct components can achieve satisfactory variance reduction, and leave the issue of finding alternative optimal components to a later article. We recognise, in certain ideal cases, that the usage of forecast projection is even more important than the choice of base forecast model, emphasising the relative importance of forecast projection. We discuss that the source of the variance reduction is the reduction of model misspecification error and the forecast projection has little impact on estimation error.


\pagebreak
# References {.unnumbered}
