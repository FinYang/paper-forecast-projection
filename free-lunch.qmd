---
title: "Free Lunch Multivariate Forecast Projection: reducing forecast variance using linear combinations"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author:
 - familyname: Yang
   othernames: Yangzhuoran Fin
   address: Monash University\newline Melbourne, Australia
   email: Fin.Yang@monash.edu
 - familyname: Hyndman
   othernames: Rob J.
   address: Monash University\newline Melbourne, Australia
 - familyname: Athanasopoulos
   othernames: George
   address: Monash University\newline Melbourne, Australia
 - familyname: Panagiotelis
   othernames: Anastasios
   address: University of Sydney\newline Sydney, Australia
pdf-engine: pdflatex
cite-method: biblatex
biblio-style: authoryear-comp
bibliography:
  - references-key.bib
  - references-pkg.bib
blind: false
cover: true
toc: false
keep-tex: true
fig-height: 5
fig-width: 8
number-sections: true
execute:
  echo: false
  warning: false
  message: false
  cache: false
editor_options:
  chunk_output_type: console
filters:
  - latex-environment
  - abstract-section
environments: toappendix
header-includes:
  - \usepackage{todonotes}
  - \usepackage[bibliography=common]{apxproof}
  - \def\Var{\operatorname{Var}}
  - \def\E{\operatorname{E}}
  - \def\tr{\operatorname{tr}}
format: wp-pdf
---

# Abstract

We introduce a novel forecast projection method designed to reduce forecast variance of arbitrary multivariate forecasts without introducing bias. This method offers a "free lunch" by requiring no additional data or information. The free-lunch method adjusts the forecasts of multivariate time series to be consistent with the forecasts of linear combinations (components) of the series by projecting all forecasts onto the space where the linear constraints are satisfied. The forecast variance can be reduced monotonically by including more components. For a given number of components, the proposed method achieves maximum forecast variance reduction among linear projections. Substantial variance reduction is observed in simulation and two applications on the Australian domestic tourism data set and the FRED-MD data set, validating theoretical findings. Notably, forecast projection with Principal Component Analysis (PCA) as the component construction method demonstrates effective variance reduction. We observe the source of the reduction is the reduction of model misspecification error.

```{r setup}
#| cache: false
knitr::read_chunk("free-lunch.R")
```
```{r path}
```

```{r library}
```

# Introduction {#sec-introduction}

We introduce a new method for improving the accuracy of any multivariate time series method, often substantially. This is done without introducing any new data, or any new information. Thus, we call it a "free lunch" method [@NFLT]: it is a simple addition to any existing multivariate forecasting method that can improve its accuracy.

The method is based on the idea that the forecasts of linear combinations of the series should be consistent with the forecasts of the series themselves. For example, suppose we have two observed series $z_{t,1}$ and $z_{t,2}$, and we also construct the combination $c_t = 2 z_{t,1} -  z_{t,1}$, then the forecasts of $c_t$ should satisfy the same linear constraint: $\hat{c}_t = 2 \hat{z}_{t,1} - \hat{z}_{t,2}$. If they do not, then we can improve the forecasts of all three series by adjusting them to be consistent. While the forecasts of $c_t$ may be of no interest in themselves, they can be used to improve the forecasts of $z_{t,1}$ and $z_{t,2}$.

The idea can be extended to any number of linear combinations, and can be applied to any number of multivariate series. It does not depend on the forecasting method being used, and works well even if all series are forecast using univariate models. In fact, when used with univariate models, this allows cross-correlations between the series to be implicitly captured in the forecasts.

We call these linear combinations of the observed time series "components", and we call the original forecasts of the observed series "base forecasts". Our free-lunch method is to adjust the base forecasts to be consistent with the forecasts of the components by projecting all forecasts onto the space where the linear constraints are satisfied. We show (theoretically and empirically) that this method leads to significant reduction in the forecast variance, without introducing any bias.

Our free-lunch method has close connections to forecast reconciliation in the hierarchical forecasting literature. See @AthEtAl2023a for a recent review of the area. In particular, the projection formulation is inspired by the minimum trace [MinT, @WicEtAl2019] solution of the forecast reconciliation problem. Forecast reconciliation is a method to modify forecasts using projection so that they conform to a specific hierarchical, grouped or temporal structure. Notably, @WicEtAl2019, @AthEtAl2017, and  @DiGir2023 have shown that forecast reconciliation can reduce forecast variance theoretically and empirically, in cross-sectional settings, temporal settings, and in cross-temporal settings. @PanEtAl2021 have provided insight into the geometric interpretation of the projection used in forecast reconciliation. However, forecast reconciliation cannot be directly applied in a general multivariate time series unless the series satisfy some linear constraints such as a hierarchical structure. In contrast, our method can be applied to any multivariate time series. It can also be used in conjunction with forecast reconciliation to further reduce forecast variance.

The idea bears some similarity to bootstrap aggregation or "bagging" [@Bre1996;@BerEtAl2016], where the final prediction is produced from an ensemble of predictions made on bootstrapped data. Bagging can reduce prediction variance without increasing bias [@HasEtAl2003], by mitigating model uncertainty [@PetEtAl2018], and it does so without introducing any new data (just bootstrapped versions of the existing data). Our method also reduces forecast variance without introducing new data, but using linear combinations of the existing data, rather than bootstrapped versions of the data. A second difference is that bagging is model dependent: it is a procedure applied to enhance the models that produce the forecasts, where the same models are fitted repeatedly. Our method is model independent: it linearly transforms a set of forecasts, regardless of which models they come from. As a result, the two methods can be used in conjunction: the projections can be applied to forecasts produced by a bagged predictor.

Another approach to improve forecast accuracy is forecast combination. Point forecast combinations usually involve combining multiple forecasts of the same series from different models. See @WanEtAl2023 for a recent comprehensive review. Our free-lunch method differs from forecast combination in (a) the forecasts we combine, and (b) in how we combine them. First, rather than combine multiple forecasts of a single series, we combine single forecasts of many different linear combinations of all observed series. Second, our combinations are obtained via projections, and so the final forecasts of a particular series are linear combinations of all series in the collection, including the observed series and all constructed components. Our approach can be used in conjunction with standard forecast combination, as the base forecasts can be obtained from any combination of forecasts.

In a broad sense, forecast reconciliation, bagging, and our proposed free-lunch method can all be viewed as forms of forecast combination with different objects to be combined. @PetSpi2021 overview a group of methods utilising combination techniques which they call "the wisdom of data" including bagging, theta method [@AssNik2000], temporal aggregation [@KouEtAl2014; @AthEtAl2017], forecasting with sub-seasonal series [FOSS, @LiEtAl2022c] and forecast combination with multiple starting points [@DisPet2015]. These differ in transformations, series to forecast, forecasting models, and combination weights. Similarly, our free-lunch method aims to exploit information in the data, with a focus on the shared information that can be captured by linear combinations of the series.

Finally, the use of components bears a resemblance to Dynamic Factor Models (DFMs), specifically those used in a forecasting setting where the components (factors) are estimated using Principal Component Analysis (PCA) [@StoWat2002a; @StoWat2002; @StoWat2012], and their extension in the machine learning literature [@DeEtAl2019]. DFMs assume that the multivariate time series possesses common components and the dynamics of the observed series are governed by the dynamics of these unobserved components, typically assumed to follow a Vector AutoRegressive (VAR) model. In contrast, our method makes no assumptions on the parametric form of the dynamics. In fact, the forecasts from a DFM can be further improved by applying the free-lunch method to the DFM forecasts. This is demonstrated in @sec-simulation and @sec-empirical-applications, where we also show how the performance of projected forecasts from univariate ARIMA models is comparable to base DFM forecasts, with the help of components.

The rest of the paper is structured as follows. In @sec-method, we propose the free lunch forecast projection method, and highlight its theoretical properties and associated estimation methods. In @sec-simulation, we present a simulation example demonstrating its performance and discuss the implications for sources of uncertainty. @sec-empirical-applications examines the performance of free-lunch forecast projection in two empirical applications: forecasting Australian domestic tourism and forecasting macroeconomic variables in the FRED-MD data set. @sec-conclusion concludes with some thoughts on future research directions.

# Free-Lunch Forecast Projection {#sec-method}

## Definitions and properties

We use $\bm{I}_n$ to denote the $n\times n$ identity matrix, and $\bm{O}_{n\times k}$ to denote the $n\times k$ zero matrix. Define the selection matrix $\bm{J}_{n,k} = \big[\bm{I}_n ~~~ \bm{O}_{n\times k}\big]$, so that $\bm{J}_{n,k}\bm{A}$ picks out the first $n$ rows of a matrix $\bm{A}$.

Let $\bm{z}_t\in\mathbf{R}^m$ be a vector of $m$ observed time series at time $t$, and let $\bm{c}_t = \bm{\Phi}\bm{z}_t\in\mathbf{R}^p$ be a vector of $p$ linear combinations of $\bm{z}_t$ at time $t$, where $\bm{\Phi}\in\mathbf{R}^{p\times m}$ is a matrix of coefficients. We call $\bm{c}_t$ the components of $\bm{z}_t$.  Let $\bm{y}_{t} = \big[\bm{z}_t', \bm{c}'_{t}\big]'$ be the collection of series $\bm{z}_t$ and components $\bm{c}_{t}$, and let $\hat{\bm{y}}_{t+h}$ denote the $h$-step-ahead base forecast of $\bm{y}_{t}$. The forecast covariance matrix is $\Var(\hat{\bm{y}}_{t+h}-\bm{y}_{t+h}) = \bm{W}_h$.

We project the base forecasts onto the space where the constraints are imposed:
$$
\tilde{\bm{y}}_{t+h} = \bm{M} \hat{\bm{y}}_{t+h}
$$ {#eq-y_tilde}
with projection matrix
$$
\bm{M} = \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C},
$$ {#eq-M}
where $\bm{C} = \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]$ defines the $p\times(m+p)$ constraint matrix such that $\bm{C}\bm{y}_t= \bm{c}_{t} - \bm{\Phi}\bm{z}_t = \bm{0}$ for any $t$.

Let $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$ denote the first $p$ elements of $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, comprising the base and projected forecasts of $\bm{z}_t$ respectively. Similarly, let $\hat{\bm{c}}_{t+h}$ and $\tilde{\bm{c}}_{t+h}$ denote the last $p$ elements of $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$, comprising the base and projected forecasts of $\bm{c}_t$ respectively. Then the projected forecast of $\bm{z}_t$ can be found by
$$
\tilde{\bm{z}}_{t+h} = \bm{J}\tilde{\bm{y}}_{t+h} = \bm{J}\bm{M} \hat{\bm{y}}_{t+h},
$$ {#eq-lcmap}
where $\bm{J} = \bm{J}_{m,p}$.

We are ready to present a few immediate results, with proofs provided in the Appendix.

::: {#lem-projectionM}
The matrix $\bm{M}$ is a projection onto the space where the constraint $\bm{C}$ is satisfied.
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-projectionM

We have
$$
\begin{aligned}
\bm{M}\bm{M}
& = \bm{I}_{m+p} - 2 \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
& \mbox{}\hspace*{1cm} + \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\\
& =\bm{I}_{m+p} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
& = \bm{M},
\end{aligned}
$$
so $\bm{M}$ is a projection matrix.
For any $\bm{y}$ such that $\bm{M}\bm{y} = \bm{x}$ for some $\bm{x}$, we have
$$
\bm{C}\bm{x} = \bm{C}\bm{M}\bm{y} = \bm{C}\bm{y} - \bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{y} = \bm{0}.
$$
Thus, $\bm{M}$ projects any vector onto the space where the constraint $\bm{C}$ is satisfied.
:::
:::

Based on the attractive properties of projections, we have the following corollaries.

::: {#cor-corM}
1. The projected forecast $\tilde{\bm{y}}_{t+h}$ satisfies the constraint $\bm{C}$.
1. For $\bm{y}_{t+h}$ that already satisfies the constraint, the projection does not change its value: $\bm{M}\bm{y}_{t+h} = \bm{y}_{t+h}$ [@Rao1974, Lemma 2.4].
1. If the base forecasts are unbiased such that $\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t)$, then the projected forecasts are also unbiased: $\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t)$.
:::

::: {.toappendix}
::: {.proof}
#### Proof of @cor-corM

Items 1 and 2 are trivial application of @lem-projectionM. To prove 3, we have
$$
\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) =
\E(\bm{M}\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{M}\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t).
$$
:::
:::

Note that the unbiasedness of the base forecasts is true for most common forecasting models, and can otherwise be easily achieved with a simple bias correction as suggested by @PanEtAl2021. Note this is not a requirement on model specification: we do not assume the model producing the base forecast is correctly specified like in the DFM literature [e.g., @StoWat2002]. In fact, the power of forecast projection manifests when the models are misspecified, as discussed in @sec-simulation.

::: {#lem-var}
The forecast covariance matrix of the component-constrained projected $h$-step-ahead  forecasts $\tilde{\bm{y}}_{t+h}$ is $\bm{M}\bm{W}_h$, i.e.
$$
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) = \bm{M}\bm{W}_h\bm{M}' = \bm{M}\bm{W}_h,
$$
and the forecast covariance matrix of the projected $h$-step-ahead forecasts $\tilde{\bm{z}}_{t+h}$ is
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-var
$$
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
= \Var(\bm{M}\hat{\bm{y}}_{t+h} - \bm{M}\bm{y}_{t+h}) \\
= \bm{M}\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h})\bm{M}' \\
= \bm{M}\bm{W}_h\bm{M}'.
$$
If we simplify it further, we have
$$
\begin{aligned}
\bm{M}\bm{W}_h\bm{M}'
&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h(\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})'\\
%&= (\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h)(\bm{I} - \bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h) \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&\mbox{}\hspace*{1cm} +  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
%&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h \\
&= \bm{M}\bm{W}_h.
\end{aligned}
$$
To get $\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})$, we just need to recognise that it is the first $m\times m$ leading principal submatrix of $\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})$.

:::
:::

@lem-var is a well-known result in the forecast reconciliation literature [e.g., @DiGir2023].

::: {#thm-psdvar}

#### Positive Semi-Definiteness of Variance Reduction

The difference between the forecast covariance matrices of the base and projected forecasts,
$$
\begin{aligned}
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) &=\bm{W}_h-\bm{M}\bm{W}_h\\
&= \bm{W}_h - (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h\\
&=\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h,
\end{aligned}
$$
is positive semi-definite.
The difference between the forecast variance of $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$,
$$
\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}',
$$
is therefore positive semi-definite.

:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-psdvar
Trivially,
$\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$ and $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ are positive semi-definite. Note that $\Var(\hat{\bm{z}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{y}_{t+h})$ is the leading principal submatrix of $\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$, and the leading principal submatrix of a positive semi-definite matrix is positive semi-definite.
:::
:::

@thm-psdvar is why forecast projection works. The trace of $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ is the sum of the reduction in forecast variances, and is non-negative because the matrix is positive semi-definite. It means we can reduce the forecast variance by simply forecasting the components (the artificially constructed linear combinations of the original data), and mapping the forecasts using matrix $\bm{M}$. For the improvement to be zero, the trace needs to be zero, and because the matrix is positive semi-definite, this implies that the entire $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ is a zero matrix, which rarely happens in practice. See @thm-pos-condition for more discussions.

We give a simple example to show how the variance reduction works.

::: {#exm-identityW}

####

Suppose $\bm{y}_t$ comprises $m$ original series and $p$ components whose forecasts are all uncorrelated with each other and have  variance $1$. Then $\bm{W}_h = \bm{I}_{m+p}$, and
$$
\begin{aligned}
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
&=
\bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C}\\
& =
\begin{bmatrix} -\bm{\Phi}'\\ \bm{I}_{p} \end{bmatrix}
(\bm{\Phi}\bm{\Phi}' + \bm{I})^{-1}
\begin{bmatrix} -\bm{\Phi}& \bm{I}_{p} \end{bmatrix} ,
\end{aligned}
$$
where
$$
\bm{C} = \begin{bmatrix} - \bm{\Phi}& \bm{I}_{p} \end{bmatrix}.
$$
Let $\bm{\Phi}$ consist of orthogonal unit vectors, for example, those obtained from Principal Component Analysis [PCA, @Jol2002]. That is,
$$
\bm{\Phi}\bm{\Phi}' = \bm{I}_p\text{ when } p\le m
\qquad\text{and}\qquad
\bm{\Phi}'\bm{\Phi} = \bm{I}_m\text{ when } p= m.
$$
Then
$$
\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})
  =  \frac{1}{2}
      \begin{bmatrix}
        \bm{\Phi}'\bm{\Phi} & -\bm{\Phi}'\\
        -\bm{\Phi} & \bm{I}_p
       \end{bmatrix}.
$$
We only focus on the forecast variance reduction for the original series, which is $\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) = \frac{1}{2}\tr(\bm{\Phi}'\bm{\Phi})$.
When $p<m$, since $\bm{\Phi}'\bm{\Phi}$ is idempotent, we have $\tr(\bm{\Phi}'\bm{\Phi}) = \operatorname{rank}(\bm{\Phi}'\bm{\Phi}) = p$. The reduction in the forecast variance of the original series is $p/2$.
When $p=m$, we have $\tr(\bm{\Phi}'\bm{\Phi}) = \tr(\bm{I}_m) = m$, which means the reduction on the sum is $m/2$ and the reduction on each individual series is $1/2$.
If we have two series ($m=2$) to begin with, using $p=1$ component in the mapping will reduce the sum of forecast variance by $0.5$, and using $p=2$ components will reduce the sum of forecast variance by $1$; that is a $50\%$ reduction as the original sum of forecast variances is $2$.

If we keep increasing the number of components, the result in @thm-psdvar still holds, although $\bm{\Phi}$ can no longer contain orthogonal vectors, and the example here becomes intractable. This is an artificial example because the forecast variance $\bm{W}_h$ can hardly be an identity in practice, as the forecasts of a linear combination of two series are likely to be correlated with the forecasts of these series. Nonetheless, we can see how the forecast variance can be reduced as a result of the component forecasts bringing new information about the original series.
:::

The variance reduction becomes larger as we increase the number of components $p$ from $1$ to $2$ in @exm-identityW. This is not a coincidence but a desirable property of forecast projection, as shown in the next section.

## Monotonicity

In the results that follow, we break the base forecast covariance matrix into smaller blocks:
$$
\bm{W}_h =
\begin{bmatrix}
\bm{W}_{z, h} & \bm{W}_{zc, h} \\
\bm{W}_{zc, h}' & \bm{W}_{c, h}
\end{bmatrix},
$$
where $\bm{W}_{z, h}$ is the forecast covariance matrix of $\hat{\bm{z}}_{t+h}$,  $\bm{W}_{c, h}$ is the forecast covariance matrix of $\hat{\bm{c}}_{t+h}$,
and $\bm{W}_{zc, h}$ contains covariances between elements of $\hat{\bm{z}}_{t+h}$ and $\hat{\bm{c}}_{t+h}$.

::: {#thm-monotone}

#### Monotonicity
The sum of forecast variance reductions
$$
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) = \tr(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}')
$$ {#eq-trdvar}
is non-decreasing as $p$ increases.
:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-monotone
Suppose now that we want to include $q$ more components $\bm{c}_{t}^*=\bm{\Phi}^*\bm{z}_t$ in the projection. We define $\bm{y}_{t}^* = \begin{bmatrix}\bm{y}_{t}\\ \bm{c}_{t}^*\end{bmatrix}$, the constraint matrix
$$
\bm{C}^* =
\begin{bmatrix}
\multicolumn{2}{c}{\bm{C}} & \underset{p\times q}{\bm{0}} \\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q}
\end{bmatrix}
=
\begin{bmatrix}
-\underset{p \times m}{\bm{\Phi}}& \bm{I}_{p} &\underset{p\times q}{\bm{0}}\\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q},
\end{bmatrix}
=
\begin{bmatrix}
\overline{\bm{C}} \\ \underline{\bm{C}}
\end{bmatrix}
$$ {#eq-Cstar}
where $\overline{\bm{C}}$ contains the first $p$ rows of $\bm{C}^*$ and $\underline{\bm{C}}$ contains the remaining $q$ rows of $\bm{C}^*$,
the forecast covariance matrix
$$
\Var(\hat{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*) = \bm{W}_h^* = \begin{bmatrix}\bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c,h}^*\end{bmatrix}.
$$
where $\hat{\bm{y}}_{t+h}^*$ is the $h$-step-ahead base forecasts of $\bm{y}_{t}^*$:
$$
\hat{\bm{y}}_{t+h}^* = \begin{bmatrix} \hat{\bm{y}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix},
$$
and the corresponding
$$
\bm{M}^* = \bm{I} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*.
$$

Proving @thm-monotone requires proving the following two items.

1. Including additional components in the mapping without including corresponding component constraints is equivalent to not including these additional components at all.
1. For a fixed set of components to be included in the mapping, adding constraints will reduce forecast variance.

We start by proving the first statement. Consider the case where we include the additional series $\bm{c}_{t}^*$ without using the additional constraint $\bm{\Phi}^*$. Defining $\bm{M}^{+}$ only with $\overline{\bm{C}}$:
$$
\bm{M}^{+} = \bm{I}_{m+p+q} - \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}},
$$ {#eq-Mplus}
we have $\tilde{\bm{y}}_{t+h}^+ = \bm{M}^{+}\hat{\bm{y}}_{t+h}^*$. Further, we obtain
$$
\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c}^*
    \end{bmatrix}
    \begin{bmatrix}
      \bm{C}'\\ \underset{q\times p}{\bm{0}} \\
    \end{bmatrix} \\
 = \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
$$
and
$$
  \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix} \\
 =\bm{C}\bm{W}_h\bm{C}',
$$
which gives
$$
\begin{aligned}
\bm{M}^{+}
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
    (\bm{C}\bm{W}_h\bm{C}')^{-1}
    \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}\\
%& = \bm{I}_{m+p+q} -
%    \begin{bmatrix}
%      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1} \\
%      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}
%    \end{bmatrix}
%    \begin{bmatrix}
%      \bm{C} & \underset{p\times q}{\bm{0}} \\
%    \end{bmatrix} \\
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
    \end{bmatrix},
\end{aligned}
$$
and
$$
\begin{aligned}
\tilde{\bm{y}}_{t+h}^+
& = \bm{M}^{+}\hat{\bm{y}}_{t+h}^* \\
& = \left(
      \bm{I}_{m+p+q} -
      \begin{bmatrix}
        \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
        \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
      \end{bmatrix}
    \right)
    \begin{bmatrix} \hat{\bm{y}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix}\\
%& = \begin{bmatrix}
%      \hat{\bm{y}}_{t+h} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}\\
%      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
%    \end{bmatrix} \\
& = \begin{bmatrix}
      (\bm{I}_{n+p} -  \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\hat{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \bm{M} \hat{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \tilde{\bm{y}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{y}}_{t+h}
    \end{bmatrix}.
\end{aligned}
$$
If we only consider the forecast performance relevant to $\bm{z}_{t+h}$, and define $\bm{J}^* = \bm{J}_{m,p+q}= \begin{bmatrix}\bm{I}_{m} & \bm{0}_{m\times(p+q)} \end{bmatrix}$, we have
$$
\tilde{\bm{z}}_{t+h}^+ = \bm{J}^*\tilde{\bm{y}}_{t+h}^+ = \bm{J}\tilde{\bm{y}}_{t+h} = \tilde{\bm{z}}_{t+h}.
$$
This means adding additional components without imposing the corresponding constraints will yield the same projected forecasts as if these additional components are not added, which implies that the forecast variance stays the same:
$$
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}) = \Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$ {#eq-acnc}
<!-- additional components no constraints -->
This finishes the proof of the first statement. Now we move on to proving the second statement. We have the forecast variance matrices
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) & = \bm{M}^{+}\bm{W}_h^*
    = (\bm{I}_{m+p+q} -\bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
\text{and}\qquad\qquad
\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*) & = \bm{M}^*\bm{W}_h^*
    = (\bm{I}_{m+p+q} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*)\bm{W}_h^*.
\end{aligned}
$$
Taking the difference, we have
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) - \Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)
& = (\bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
& = \bm{W}_h^*(\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*.
\end{aligned}
$$
Using block matrix inversion, we have
$$
\begin{aligned}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    \end{bmatrix}^{-1}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& =   \overline{\bm{C}}'a \overline{\bm{C}}
    + \overline{\bm{C}}'b \underline{\bm{C}}
    + \underline{\bm{C}}'c\overline{\bm{C}}
    + \underline{\bm{C}}'d\underline{\bm{C}},
\end{aligned}
$$
where
$$
\begin{aligned}
a &= (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
     (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
  & \mbox{}\hspace*{1cm}
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' - \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}
    \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \\
  & = (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
b & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' -
     \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
     \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}\\
  & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1},\\
c & = - (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
d & = (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}.
\end{aligned}
$$
Thus,
\begin{align*}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}'\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
&=
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}.
\end{align*}
Therefore,
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*) - \Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)
\hspace*{-4cm} \\
& =
\bm{W}_h^*(
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
-\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*\\
& =
\bm{W}_h^*(
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
)\bm{W}_h^*
\end{aligned}
$$
is positive semi-definite. This concludes the proof of the second statement.
Combining the results above, we have
$$
\begin{aligned}
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) -
\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h})
& =
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}) -
\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}) \\
&=
\bm{J}^*\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}^*)\bm{J}^{*\prime} -
\bm{J}^*\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}^*)\bm{J}^{*\prime} \\
&=
\bm{J}^*
\bm{W}_h^*
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
\bm{W}_h^*
\bm{J}^{*\prime}
\end{aligned}
$$ {#eq-addcom}
being positive semi-definite.
Finally, we have
\begin{multline*}
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}))
-
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) \\
= \tr(
    \Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) -
    \Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h})
)
\end{multline*}
being the trace of a positive semi-definite matrix, which is non-negative. This means using a larger number of components in the mapping achieves a lower sum of forecast variance, giving @thm-monotone.
:::
:::

@thm-monotone is the key result that demonstrates the usefulness of forecast projection. It means that we can keep increasing the number of components to reduce forecast variance, even when the number of components exceeds the number of original series, assuming we know the base forecast covariance of all the series and components. It requires $\bm{C}$ to be $\big[-\bm{\Phi} ~~~ \bm{I}_{p} \big]$ or $\big[-\bm{\Phi} ~~~ \bm{L}\big]$ where $\bm{L}$ is a lower triangular matrix (with upper right corner all $0$s). This implies that the components can also be constructed from existing components, not only from the original series. This has little significance since a linear combination of components of the original series, is just a linear combination of the original series.

Extending the proof of @thm-monotone, we can outline the condition for the reduced variance to be positive. Denote $\bm{\phi}_i$ as the row vector containing the weights associated with the $i$th component, so that with $p$ components, the weights matrix is $\bm{\Phi} = \big[\bm{\phi}_1' ~~~ \bm{\phi}_2' ~~~ \dots ~~~ \bm{\phi}_p'\big]'$. Let $\bm{W}^{(i-1)}_{\tilde{z},h}$ denote the covariance matrix of the projected forecasts of the original series based on the first $i-1$ components, $\bm{w}_{c_{1}z,h}$ denote a vector of covariances of the first component and the base forecasts of the original series, and $\bm{w}_{\tilde{c}_i\tilde{z},h}^{(i-1)}$ denote a vector of covariances of the projected $i$th component, and the projected forecasts of the original series, based on the first $i-1$ components.

::: {#thm-pos-condition}

#### Positive Variance Reduction Condition

For the first component to have a guaranteed reduction of forecast variance (for the reduced variance matrix in @thm-psdvar to have positive trace), the following condition must be satisfied:
$$
\bm{\phi}_1\bm{W}_{z,h}\neq\bm{w}_{c_1z,h}.
$$ {#eq-pcon}
For the $i$th component to have a positive reduction on forecast variance of the original series, the following condition must be satisfied:
$$
\bm{\phi}_i{\bm{W}}^{(i-1)}_{\tilde{z},h}\neq\bm{w}_{\tilde{c}_i\tilde{z},h}^{(i-1)},
$$ {#eq-pconi}

:::
::: {.toappendix}
::: {.proof}
#### Proof of @thm-pos-condition

Denote $\bm{\psi}_i=\begin{bmatrix}-\bm{\phi}_i & \bm{0}_{1\times(i-1)} & 1\end{bmatrix}$ and $\bm{W}_{h}^{(i)}$ to be the base forecast variance of the original series and the first $i$ components. Starting with the first component, @eq-trdvar becomes
$$
\tr(\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1'(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}') =
(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}'\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1',
$$ {#eq-trdvar1}
$$
\begin{aligned}
\text{where}\qquad\qquad
\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' =&
\begin{bmatrix}
-\bm{\phi}_1 & 1
\end{bmatrix}
\begin{bmatrix}
\bm{W}_{z,h} & \bm{w}'_{c_1z,h} \\
\bm{w}_{c_1z,h} & \bm{W}_{c_1,h}
\end{bmatrix}
\begin{bmatrix}
\bm{I}_m\\ 0
\end{bmatrix} \\
=& -\bm{\phi}_1\bm{W}_{z,h} + \bm{w}_{c_1z,h}.
\end{aligned}
$$
@eq-trdvar1 is obviously non-negative. For it to be larger than $0$, we need $\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' \neq 0$, which gives $\bm{\phi}_1\bm{W}_{z,h} \neq \bm{w}_{c_1z,h}$.

When it comes to adding the $i$th component on top of the first $i-1$ components, we define
$$
\overline{\bm{C}}_i =
\begin{bmatrix}
\bm{\psi}_1 & \bm{0}_{1\times i} \\
\bm{\psi}_2 & \bm{0}_{1\times (i-1)} \\
\vdots & \vdots \\
\bm{\psi}_i & 0 \\
\end{bmatrix}
$$
and
$$
\bm{M}^+_i = \bm{I}_{m+i} - \bm{W}_h^{(i)} \overline{\bm{C}}_{i-1}'
(\overline{\bm{C}}_{i-1}\bm{W}_h^{(i)}\overline{\bm{C}}_{i-1}')^{-1}
\overline{\bm{C}}_{i-1}
$$
analogously to @eq-Cstar and @eq-Mplus.
Following @eq-addcom, the additional reduction of forecast variance when adding the $i$th component becomes
$$
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
=
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i
\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'.
$$
Similar to before, we would want $\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}' \neq \bm{0}$. Note that $\bm{\psi}_i$ concerns the first $m$ rows and the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, and $\bm{J}_{m,i}'$ concerns the first $m$ columns. Combined with the implication from @eq-acnc that the $m\times m$ leading principal submatrix in equation $\bm{J}_{m,i}\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}'=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the same, we suppress the straightforward yet tiresome details, and obtain
$$
\bm{\phi}_i\bm{W}_{\tilde{z},h}^{(i-1)} \neq \big[\bm{0}_{1\times m+i-1} ~~~ 1 \big]\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}',
$$
where $\bm{W}_{\tilde{z},h}^{(i-1)}=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the projected forecast variance of the original series using the first $i-1$ components, and the right hand side of the inequality is simply a one-row matrix consisting of the first $m$ elements in the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, which can be denoted as $\bm{w}_{\tilde{c}_i\tilde{z},h}^{(i-1)}$ and interpreted as the covariance between the projected forecast of the original series using the first $i-1$ components, and the projected forecast of the $i$th component using the first $i-1$ components.
:::
:::

We can interpret the condition of @eq-pcon in the following way: for the new component to be beneficial, the information brought by this new component, measured as the covariance, cannot be a linear combination of already existing information.

@thm-pos-condition can potentially provide insights into the selection of component weights and forecast models to satisfy the conditions. We leave this issue to a later article, as practically the conditions in @thm-pos-condition are either almost always satisfied if the weights are simulated randomly on a continuous scale, or the loss associated with the rare occasions where the conditions are not satisfied is neglectable compared to the estimation error imposed by the limited sample size as the number of components increases, as discussed in @sec-simulation and @sec-empirical-applications.


## Alternative Interpretations

@eq-y_tilde can be seen as a solution to the optimisation problem:
$$
\underset{\check{\bm{y}}_{T+h}}{\arg\min}
  (\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})
\qquad \text{s.t. } \bm{C}\check{\bm{y}}_{T+h}=0.
$$
That is, the projection can be interpreted as finding the set of forecasts that are closest (on the transformed space) to the base forecasts, while satisfying the linear constraints imposed by the components.

Moreover, this is equivalent to the optimisation problem:
$$
\underset{\check{\bm{y}}_{T+h}}{\arg\min} (\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{y}}_{T+h}-\check{\bm{y}}_{T+h})
\qquad \text{s.t. } \bm{\Phi}\check{\bm{z}}_{T+h}=\check{\bm{c}}_{T+h},
$$
where $\check{\bm{c}}_{T+h}$ is the vector of the last $p$ elements of $\check{\bm{y}}_{T+h}$, corresponding to the forecast of the components as part of the solution. This equivalence is discussed in @WicEtAl2019, where the authors find the solution by minimising the sum of forecast variance of all series (See @AndNar2022 for a simpler proof). The result is the MinT solution
$$
\tilde{\bm{y}}_{t+h} = \bm{S}\bm{G} \hat{\bm{y}}_{t+h},
$$ {#eq-mint}
where
$\bm{S} = \begin{bmatrix}\bm{I}_m \\\bm{\Phi}\end{bmatrix}$ contains the constraints, so that $\bm{y}_{t} = \bm{S}\bm{z}_{t}$, and\pagebreak[3]
$$
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$ {#eq-G}
In @eq-mint, $\bm{G} \hat{\bm{y}}_{t+h}$ can be viewed as mapping of all the series to a selected few. In the forecast reconciliation context, this is mapping series at all levels to the "bottom level" series. In our multivariate forecasting context, this is mapping all series including the components, to the space of the original series, leading to the solution
$$
\tilde{\bm{z}}_{t+h} = \bm{G}\hat{\bm{y}}_{t+h},
$$ {#eq-stmap}
as equivalent to @eq-lcmap. Recognising that @eq-mint is equivalent to @eq-y_tilde, it is the solution that minimises the sum of forecast variances of the original series and all the components. We go further in @thm-minvar, and show that @eq-mint is also the solution to minimise each individual forecast variance of the original series, and their sum. This can be viewed as a special case of Theorem 3.3 of @PanEtAl2021, or as illustrated by @AndNar2022, but applied in a different context from forecast reconciliation. The earliest work we can find that noted this interpretation in a non-forecasting context is @Lue1969 [p.85]. We establish a few basic results leading to the optimality of this solution first, also to check that @lem-projectionM, @cor-corM and @lem-var
hold under this alternative representation.

::: {#lem-projectionG}
The matrix $\bm{S}\bm{G}$ is a projection onto the space where the constraint $\bm{C}$ is satisfied, provided that $\bm{G}\bm{S}=\bm{I}$.
:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-projectionG
If $\bm{G}\bm{S}=\bm{I}$, $\bm{S}\bm{G}$ is a projection matrix: $\bm{S}\bm{G}\bm{S}\bm{G} = \bm{S}\bm{G}$.

For any $\bm{y}$ such that $\bm{S}\bm{G}\bm{y} = \bm{x}$ for some $\bm{x}$, we have $\bm{C}\bm{x} = \bm{C}\bm{S}\bm{G}\bm{y} = \bm{0}$
because $\bm{C}\bm{S} = \big[-\bm{\Phi} ~~~ \bm{I} \big] \big[\bm{I} ~~~ \bm{\Phi}' \big]' = \bm{0}$. Similarly to $\bm{M}$, $\bm{S}\bm{G}$ projects a vector to the same space where $\bm{C}$ is satisfied.
:::
:::

::: {#cor-corG}
Provided that $\bm{G}\bm{S} = \bm{I}$, the following results hold.

1. The projected forecast in @eq-stmap satisfies the constraint $\bm{C}\tilde{\bm{y}}_{t+h}= \bm{C}\bm{S}\tilde{\bm{z}}_{t+h}= \bm{0}$.
1. For $\bm{y}_{t+h}$ that already satisfies the constraint, the mapping does not change its value: $\bm{G}\bm{y}_{t+h} = \bm{z}_{t+h}$.
1. If the base forecasts are unbiased such that $\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t)$, then the projected forecasts in @eq-stmap are also unbiased: $\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t)$.
:::

::: {.toappendix}
::: {.proof}
#### Proof of @cor-corG

Item 1 is an direct application of @lem-projectionG. From @lem-projectionG and Lemma 2.4 in @Rao1974, we have
$$
\bm{S}\bm{G}\bm{y}_{t+h} = \bm{y}_{t+h} = \bm{S}\bm{z}_{t+h}.
$$
Left multiplying by $\bm{G}$ on both sides, we have $\bm{G}\bm{y}_{t+h} = \bm{z}_{t+h}$ and item 2 is proven.
To prove Item 3, we have
$$
\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) =
\E(\bm{G}\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\hat{\bm{y}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{G}\bm{y}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t).
$$

:::
:::

::: {#lem-st-var}

The covariance matrix of the projected forecasts from @eq-stmap is given by
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) = \bm{G}\bm{W}_h\bm{G}'.
$$

:::

::: {.toappendix}
::: {.proof}
#### Proof of @lem-st-var
Let the base and projected forecast errors be given as
$$
\begin{aligned}
\hat{\bm{e}}_{z,t+h} &= \bm{z}_{t+h} - \hat{\bm{z}}_{t+h},\\
\hat{\bm{e}}_{y,t+h} &= \bm{y}_{t+h} - \hat{\bm{y}}_{t+h},\\
\tilde{\bm{e}}_{z,t+h} &= \bm{z}_{t+h} - \tilde{\bm{z}}_{t+h},\\
\text{and}\qquad
\tilde{\bm{e}}_{y,t+h} &= \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}
= \bm{S}\bm{z}_{t+h} - \bm{S}\tilde{\bm{z}}_{t+h}
= \bm{S} \tilde{\bm{e}}_{z,t+h}. \\
\text{Then we have}\qquad
\tilde{\bm{e}}_{y,t+h}
&= \hat{\bm{e}}_{y,t+h} + \hat{\bm{y}}_{t+h} - \tilde{\bm{y}}_{t+h}\\
&= \hat{\bm{e}}_{y,t+h} + \hat{\bm{y}}_{t+h} - \bm{S}\bm{G}\hat{\bm{y}}_{t+h}\\
&= \hat{\bm{e}}_{y,t+h} + (\bm{I} - \bm{S}\bm{G})(\bm{y}_{t+h} - \hat{\bm{e}}_{y,t+h})\\
\text{and}\qquad
&= \bm{S}\bm{G}\hat{\bm{e}}_{y,t+h} + (\bm{I} - \bm{S}\bm{G})\bm{S}\bm{z}_{t+h}\\
\bm{S} \tilde{\bm{e}}_{z,t+h} &= \bm{S}\bm{G}\hat{\bm{e}}_{y,t+h},
\end{aligned}
$$
where the last line comes from $\bm{G}\bm{S} = \bm{I}$.
Left multiplying by $\bm{G}$ on both sides, we have
$$
\bm{G}\bm{S} \tilde{\bm{e}}_{z,t+h} = \bm{G}\bm{S}\bm{G}\hat{\bm{e}}_{y,t+h}
\qquad\text{and}\qquad
\tilde{\bm{e}}_{z,t+h} = \bm{G}\hat{\bm{e}}_{y,t+h},
$$
and therefore
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h}) =\Var(\tilde{\bm{e}}_{z,t+h})=\Var(\bm{G}\hat{\bm{e}}_{y,t+h})= \bm{G}\Var(\hat{\bm{e}}_{y,t+h})\bm{G}'= \bm{G}\bm{W}_h\bm{G}'.
$$

:::
:::


Putting these together, we have the following theorem.

::: {#thm-minvar}

#### Minimum Variance Unbiased Projected Forecast

The solution to
$$
\underset{\bm{G}}{\arg\min}\ \bm{G}\bm{W}_h\bm{G}'
\qquad \text{s.t. } \bm{G}\bm{S} = \bm{I}
$$ {#eq-obj}
is @eq-G. This problem can be effectively split into independent subproblems such that $\bm{G} = \big[\bm{g}_1 ~~ \bm{g}_2 ~~ \dots ~~ \bm{g}_m\big]'$, where $\bm{g}_i$ is the solution to the subproblem of the $i$th series
$$
\underset{\bm{g}_i}{\arg\min}\ \bm{g}_i'\bm{W}_h\bm{g}_i
\qquad \text{s.t. } \bm{g}_i'\bm{s}_{j} = \delta_{ij}, \quad j= 1, 2, .\ldots, m,
$$ {#eq-subobj}
where $\bm{s}_j$ is the $j$th column of $\bm{S}$, and $\delta_{ij}$ is the Kronecker delta function taking value 1 if $i = j$ and 0 otherwise.

:::

::: {.toappendix}
::: {.proof}
#### Proof of @thm-minvar

This can be proved in a few different ways. We adopt the approach of @AndNar2022 to obtain the solution to @eq-obj, but the procedure from @Lue1969 [p. 85] can also be used, where the problem is divided to @eq-subobj and reconstructed to find the solution to @eq-obj.

There exists a Lagrange multiplier $\bm{\Lambda}$ such that
$$
L(\bm{G}) = \tr(\bm{G}\bm{W}_h\bm{G}') + \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S}))
$$
is stationary at an extremum $\bm{G}$ [@Lue1969, p. 243, Theorem 1]. We set the Gateaux differential [@Lue1969, p. 171] to zero for any matrix $\bm{H}$:
$$
\begin{aligned}
\lim_{\alpha \to 0} \frac{L(\bm{G} + \alpha\bm{H}) - L(\bm{G})}{\alpha} &= 0 \\
\tr(\bm{G}\bm{W}_h\bm{H}') + \tr(\bm{H}\bm{W}_h\bm{G}') - \tr(\bm{\Lambda}'(\bm{H}\bm{S}))
  &=\tr(2\bm{H}\bm{W}_h\bm{G}'-\bm{\Lambda}'\bm{H}\bm{S})\\
  &= \tr(\bm{H}(2\bm{W}_h\bm{G}'-\bm{S}\bm{\Lambda}')) \\
  &=0 \\
2\bm{W}_h\bm{G} &= \bm{S}\bm{\Lambda}' \\
\bm{G}' &= \frac{1}{2}\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'.
\end{aligned}
$$
Multiplying $\bm{S}'$ to the left of both sides. we have
$$
\bm{S}'\bm{G}'=\bm{I} = \frac{1}{2}\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'
\qquad\text{and}\qquad
\bm{\Lambda}' = 2 (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
$$
because $\bm{G}\bm{S} = \bm{I}$. Putting it back in, we have
$$
\bm{G}' = \bm{W}_h^{-1}\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
\qquad\text{and}\qquad
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$
:::
:::
In other words, the forecast projection method gives optimal projected forecast for a given set of components, in the sense that the unbiased forecast of each series has minimum variance.

## Estimation of $\bm{W}_h$

In practice, the base forecast variance $\bm{W}_h$ is unknown and needs to be estimated. Denote $\hat{\bm{e}}_{t,h} = \bm{y}_{t} - \hat{\bm{y}}_{t|t-h}$ as the $h$-step-ahead base forecast residual. The conventional forecast covariance matrix estimator
$$
\widehat{\bm{W}_h}=\frac{1}{T-1}\sum^T_{i=1}\hat{\bm{e}}_{t,h}\hat{\bm{e}}_{t,h}',
$$
albeit unbiased, is not considered a good approximation to the true forecast variance in a finite sample when $(m+p) \approx T$. It is even singular when $(m+p)>T$, which makes the quantities discussed in the previous sections impossible to calculate. For this reason, we adopt the variance shrinkage method of @SchStr2005, which is the "MinT(Shrink)"" method of @WicEtAl2019, and the covariance shrinkage method of @OpgStr2007. Then, the estimated forecast variance matrix is guaranteed to be positive definite with few numerical problems. This estimator is denoted as $\widehat{\bm{W}}_h^{shr} = (\hat{w}_{ij,h}^{shr})_{1\leq i,j\leq m+p}$ with elements
$$
\hat{w}_{ij,h}^{shr} = \hat{r}_{ij,h}^{shr}\sqrt{\hat{v}_{i,h}\hat{v}_{j,h}},
$$
where $\hat{r}_{ij,h}^{shr} = (1-\hat{\lambda}_{cor})\hat{r}_{ij,h}$ and $\hat{v}_{i,h} = \hat{\lambda}_{var}\hat{w}_{h, median} + (1-\hat{\lambda}_{var})\hat{w}_{i, h}$, with $\hat{\lambda}_{cor}$ being the shrinkage intensity parameter for the correlation
$$
\hat{\lambda}_{cor} =
\min\left(1,
\frac
{\sum_{i\neq j}\widehat{\operatorname{var}}(\hat{r}_{ij,h})}
{\sum_{i\neq j}\hat{r}_{ij,h}^2}
\right),
$$
and $\hat{\lambda}_{var}$ being the shrinkage intensity parameter for the variance
$$
\hat{\lambda}_{var} =
\min\left(1,
\frac
{\sum_{i=1}^{m+p}\widehat{\operatorname{var}}(\hat{w}_{i,h})}
{\sum_{i=1}^{m+p}(\hat{w}_{i, h} - \hat{w}_{h, median})^2}
\right),
$$
$\hat{r}_{ij,h}$ the sample correlation of the $h$-step-ahead forecast error between the $i$th and the $j$th series (component) in $\bm{y}_t$, $\hat{w}_{i, h}$ the $h$-step-ahead sample base forecast variance associated with the $i$th series (the $i$th diagonal element of $\widehat{\bm{W}_h}$), and $\hat{w}_{h, median}$ the median of the $h$-step-ahead sample forecast variance of the series and components (the median of the diagonal elements of $\widehat{\bm{W}_h}$). The estimation of $\widehat{\bm{W}}_h^{shr}$ in the following sections are implemented using the package `corpcor` [@corpcor] in R [@R].

Estimating $\widehat{\bm{W}}_h^{shr}$ for each forecast horizon $h$ is desirable but computationally intensive. It involves the calculation of multi-step-ahead in-sample residuals of the forecast models, which is especially challenging for iterative forecasts. Because of this, in practice it is not unreasonable to assume the $h$-step forecast variance is proportional to the $1$-step forecast variance by a constant $\eta_h$, as do @WicEtAl2019:
$$
\widehat{\bm{W}}_h^{shr} = \eta_h\widehat{\bm{W}}_1^{shr}.
$$
Under this assumption, when $\widehat{\bm{W}}_h^{shr}$ is used in @eq-M, the proportionality constant $\eta_h$ cancels out regardless of the value of $h$. We can effectively use only the one-step forecast variance in forecast projection, if we only need point forecasts. We calculate $\widehat{\bm{W}}_h^{shr}$ for each $h$ for the simulation example in @sec-simulation, but assume this proportionality for the application in @sec-empirical-applications.

# Simulation {#sec-simulation}

## Benchmarks

```{r simulation}
```

In this section, we illustrate the performance of the free-lunch forecast projection method in a simulation example. In each sample, we simulate $T=400$ observations from a VAR($3$) process with $m=70$ variables. The coefficients of the VAR model are estimated from the first $70$ series in the Australian tourism data set used in @sec-australian-domestic-tourism. The innovation in the VAR model is simulated from a multivariate normal distribution with an identity covariance matrix. The estimation and simulation are done using package `tsDyn` [@tsDyn]. We simulate $220$ such samples and the forecast is evaluated on each sample.

The first benchmark we use is a univariate ARIMA model. For each series, we fit an ARIMA model using the `auto.arima()` function from the `forecast` package [@forecast]. The function implements an automatic model selection procedure proposed by @HynKha2008. The number of first differences is determined by repeated KPSS tests [@KwiEtAl1992] and the number of seasonal differences is determined by the seasonal strength computed from an STL decomposition [@CleEtAl1990;@fpp3]. The algorithm then chooses different orders of the autoregressive (AR) and moving average (MA) parts by comparing AICc between the corresponding models in a stepwise fashion, up to a maximal order of $5$. Univariate ARIMAs are also used to produce base forecasts of the components used in projection, regardless of the base model.

Another base model is the DFM following @StoWat2002a:
$$
\hat{y}_{T+h} = \hat{\alpha}_h + \sum^n_{j=1}\hat{\bm{\beta}}_{hj}'\hat{\bm{F}}_{T-j+1} + \sum^s_{j=1}\hat{\gamma}_{hj}y_{T-j+1},
$$
where $\hat{\bm{F}}_t$ is the vector of $k$ estimated factors, and $\hat{y}_t$ is the target series to forecast. The factors are estimated using PCA on demeaned and scaled data. The optimal model is selected for each series based on the Bayesian information criterion (BIC) from models fitted using different combinations of meta-parameters in their corresponding range: $1 \leq k \leq 6$, $1 \leq n \leq 3$ and $1 \leq s \leq 3$. Note here DFM produces direct forecasts in the sense that a different model is fitted for each forecast horizon $h$, in contrast to indirect or iterative forecasts.

We use different weighting methods to construct the components. The types of components are listed below. For those randomly simulated, we normalise them into unit vectors to maintain some level of consistency, with $\bm{\phi}_i/\sqrt{\sum_j(\phi_{ij}^2)}$ where $\phi_{ij}$ is the $j$th value in the weight vector of the $i$th component.

PCA+Norm
: The $m$ principal components in PCA are taken first, implemented with the `prcomp` function in package `stats` [@R]. Weights of the additional components are simulated from a standard normal distribution before being normalised to unit vectors.

PCA+Unif
: The $m$ principal components in PCA are taken first, and the weights of the additional components are simulated from a uniform distribution with minimum $-1$ and maximum $1$ before being normalised to unit vectors.

Norm
: The weights of components are simulated from a standard normal distribution before being normalised to unit vectors.

Unif
: The weights of components are simulated from a uniform distribution with minimum $-1$ and maximum $1$ before being normalised to unit vectors.

Ortho+Norm
: A random orthonormal matrix is generated using package `pracma` [@pracma], forming the weights of the first $m$ components. Weights of the additional components are simulated from a standard normal distribution before being normalised to unit vectors.

We employ the Friedman test [@Fri1937; @Fri1939] along with post-hoc Nemenyi tests [@Nem1963;@HolEtAl2013] to compare forecast performance between different methods. The analysis involves the use of Multiple Comparisons with the Best (MCB) plot introduced by @KonEtAl2005 to visualise the comparison. The mean squared error (MSE) of each series over different samples is calculated, and the MSEs of all the series are treated as observations in the Nemenyi test. Our objective is to assess whether there are statistically significant differences between the projected and base forecasts. The average ranks are plotted in @fig-simulation-mcb-series for forecast horizons $1$, $6$ and $12$. The methods using free-lunch forecast projection are named "{Model}-{Comp. Weights}-{No. Comp.}". The maximum number of components is chosen to be $300$. The base models are named "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best-performing model. Methods outside the shaded region are significantly worse than the best model. We also plot the specific MSE values by the number of components $p$ in @fig-simulation-line. Here we include the performance of the true data generating process (DGP) VAR model (VAR -- DGP), the estimated VAR model with the correct specification (VAR -- Est.), and their projections. We do not include those methods involving a uniform distribution and random orthonormal matrices, as they are visually identical to the methods with a normal distribution. The vertical black line indicates the number of series $m=70$.

```{r fig-simulation-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead MSE of different model and component specifications in the simulation. The methods using free-lunch forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
#| fig-height: 8
```

```{r fig-simulation-line}
#| fig-cap: 'MSE of different forecast models and component construction methods by the number of components $p$ used in free-lunch forecast projection in the simulation, for forecast horizons $1$, $6$ and $12$. "VAR -- DGP" indicates the performance of the true data generating VAR model. "VAR -- Est." indicates the performance of the VAR model with the same structure as the true model and estimated parameter values. The vertical black line indicates the location of $p=m$, the number of series.'
```

## Projection over base forecast

The first thing we note is the overall performance difference between the projected forecasts and the base forecasts. In @fig-simulation-mcb-series, the average ranks of the projected forecasts are better than the corresponding base forecast at all forecast horizons, and the differences are all significant except the PCA-related ones with only $m=70$ components for forecast horizon $6$ and $12$. Note here that we need to compare forecasts with the same model: the projected forecast of ARIMA to base ARIMA, and the projected forecast of DFM to base DFM. The number of components seems important: the best-performing models are all with $300$ components. Between the one-step-ahead forecasts, the methods with $300$ components are not significantly different from each other, regardless of the forecast model or how we construct the components.

Indeed, from @fig-simulation-line we can see the MSEs for model ARIMA and DFM keep decreasing from the base forecast, as the number of components increases. This confirms @thm-monotone, that the more components we include, the more variance reduction we can achieve. This is only obvious in this ideal setting where we have $400$ observations in each group while we only use at most $300$ components in the projection. This relatively large number of observations and the simple DGP can ease the challenge of estimation. This continued reduction in variance is not always achievable with real data, as we will see in @sec-empirical-applications, especially with FRED-MD in @sec-fred-md.

## Base forecast model

If we compare ARIMA and DFM, under a VAR DGP, we expect DFM to pick up the correlation between series but not univariate ARIMA, so DFM should have better performance over ARIMA. This is indeed the case. Looking at the base forecasts in @fig-simulation-mcb-series, base DFM is significantly better than base ARIMA, except for $h=1$ where it is close to significant. This is also obvious in @fig-simulation-line. The horizontal line representing the MSE of base DFM is always far below the horizontal line for base ARIMA.

With the help of free-lunch forecast projection, a simple model like ARIMA can achieve comparable performance to more sophisticated models like DFM. In @fig-simulation-mcb-series, all projected ARIMA forecasts except the ones having $m$ PCs are significantly better than base DFM at $h=1$, and all projected ARIMA with $300$ components are significantly better than base DFM at $h=6$. In @fig-simulation-line, the solid and long-dashed lines of projected ARIMA with corresponding component construction go down monotonically as the number of components $p$ increases and reaches the MSE of base DFM at some point: at or below $m$ for $h=1$, at or above $m$ for $h=6$, and around $p=300$ for $h=12$. This is because forecast projection utilises shared information between series by capturing them in the components, making up for the overlooked cross-correlations in univariate ARIMA models.

Interestingly enough, at $h=1$, while the MSE of projected DFM also goes down as $p$ increases, the MSE of the projected ARIMA and the MSE of the projected DFM seems to converge to the same value as $p$ reaches $300$, no matter how the components are constructed. Note here the same forecasts of the components, coming from univariate ARIMA of these components, are used for both the projected ARIMA and the projected DFM. This implies that much information in the series is not captured by ARIMA or DFM, but is captured by the components. As the number of components becomes high enough, the information captured by the components overpowers the information captured by the base models, dominating the performance of the projected forecasts. Once again, this emphasises the importance of the components and forecast projection. In this extreme case, the simple model is as good as the more complicated model after projection, because the forecast model itself is not as valuable as forecast projection.

This observation is not as obvious as the forecast horizon increases. This is because while ARIMA produces forecasts iteratively, DFM is a direct forecast model. With this simple DGP, the performance of DFM can be well maintained with larger $h$ since a different model is fitted for each $h$. This can be seen as the MSE of the base DFM does not change much with different $h$, but the MSE of base ARIMA keeps increasing as $h$ increases.

## Component construction {#sec-component-construction}

The construction of components is obviously important in forecast projection, but might not be as important as expected in this simulation example. In @fig-simulation-mcb-series, the main difference that can be observed is between using a combination of PCA and random weights, and purely using random weights. The distribution that generates the random weights has limited effect: in @fig-simulation-mcb-series, with the same number of components and the same base ARIMA model, the MSEs are not significantly different, regardless of whether the weights are simulated from a normal distribution (Norm) or a uniform distribution (Unif), or a combination of random orthonormal weights and random normal weights (Ortho+Norm). The same conclusion can be found when PCA is used. As long as PCA is used, the performance is not different whether the additional components are simulated from a normal distribution (PCA+Norm) or a uniform distribution (PCA+Unif).

Because the distribution is of limited importance, in @fig-simulation-line, we look only at the inclusion of PCA with distribution set to normal. When $p$ is smaller, the MSE drops faster when PCA is used, but the speed decreases as $p$ increases. The performance of forecasts without PCA reaches and exceeds the performance with PCA before the number of components reaches $m$, and stays in the lead thereafter, although the gap seems to diminish with large $p$. This difference of PCA comes from the variances of principal components being maximised and ranked from largest to smallest, not from the orthogonality of the components, because the performance of using random orthonormal weight matrix is the same as using only random normal weights as discussed before. This might suggest the use of simple random weights if one is going to include a lot of components in the projection and to use PCA only when the number of components is small, but as we will see in @sec-empirical-applications, this is not the case with real data, and PCA is the preferred approach to obtaining components even when the number of components is large.

Different constructions of components remain an important aspect of forecast projection. One important future direction would be to find alternative and optimal components, as we do not limit the structure of the weight matrix in @sec-method. This should be studied together with the selection of the forecast model since both the weight matrix $\bm{\Phi}$ and the base forecast variance $\bm{W}_h$ can affect the projection simultaneously in @eq-M. This is likely to be an extension of the forecast combination literature, focusing on the properties of the base forecasts, and the diversity and robustness of the forecast model and components. Examples of studies on this issue in the forecast combination literature include @BatDua1995, @KanEtAl2022 and @LicWin2020.

## Sources of uncertainty

At the bottom of each panel in @fig-simulation-line, the best forecasts come from the true DGP VAR model (the dashed green line that is partially covered by the solution green line). The forecast projection on the true model does not improve its forecast (the solid green line) as expected, as the uncertainty comes from the intrinsic error in the DGP that cannot be reduced. The second best forecast is from the estimated VAR model, as the uncertainty, apart from the intrinsic error, only comes from estimation error, not model misspecification error like ARIMA and DFM, which are both misspecified in this simulation example. The gap between the estimated VAR and the true VAR becomes bigger for a longer forecast horizon, because VAR produces iterative forecasts, and estimation error accumulates as $h$ becomes larger.

Forecast projection shows little, if any, improvement over the estimated VAR. This means forecast projection cannot reduce estimation error, which is termed as the parameter uncertainty in @PetEtAl2018. On the other hand, it shows significant improvement over misspecified base models. This implies that the uncertainty it can reduce is mainly the model misspecification error, referred to as the model uncertainty in @PetEtAl2018. This is similar to bagging as bagging also reduces variance by controlling model uncertainty. The data uncertainty in @PetEtAl2018 is not examined since the proposed method relies on one given data set, it is not clear how it translates to the forecast projection problem, but it relates to the uncertainty in how the components are constructed, as discussed in @sec-component-construction, and awaits future research.

# Empirical applications {#sec-empirical-applications}

Here we apply the free-lunch forecast projection method to two real data examples and draw most of the same conclusions we did from the simulations, with a few key differences.

## Australian domestic tourism {#sec-australian-domestic-tourism}

```{r visnights}
```

The Australian Tourism Data Set compiled from the National Visitor Survey by Tourism Research Australia contains the total number of nights spent by Australians each month away from home, which we will refer to as visitor nights in what follows. The monthly visitor nights are recorded for each of the $m=77$ regions, covering the period from January 1998 to December 2019. To measure the performance of forecast projection, we conduct time series cross-validation [@fpp3]. The first $T = 84$ observations are kept as the training sample of the first evaluation, and the following $12$ periods are taken as the test set on which the errors are calculated. We repeat the evaluation for the rest of the data, with each training sample including one more observation than the previous one, and each test set shifting one period to the future.

The base forecasts of both the series and the components are produced by univariate ETS models selected and fitted using the `ets()` function in the `forecast` package [@forecast; @HynKha2008]. In an ETS model, the trend term describes the direction of the long-term tendency, the seasonal term describes the periodically recurring pattern with a fixed periodicity, and the error term measures the uncertainty. The trend and seasonal terms are optional. If a trend term exists, we allow it to be additive or additive damped; if a seasonality term exists, it can be addictive or multiplicative; the error can be additive or multiplicative. Excluding models with numerical instabilities, we choose the model with the smallest AICc among the $15$ possible models.

```{r fig-visnights-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the visitor nights data. The methods using forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```

```{r fig-visnights-line}
#| fig-cap: "MSE of different component construction methods by the number of components $p$ used in forecast projection with ETS models on the visitor nights data, for forecast horizons $1$, $6$ and $12$. The vertical black line indicates the location of $p=m$, the number of series."
```

The MCB plot and the MSE plot are shown in @fig-visnights-mcb-series and @fig-visnights-line. Most of the findings are consistent with @sec-simulation. From @fig-visnights-mcb-series, the base forecasts are always ranked last. Even projections with only one component are significantly better than the base forecasts for $h=6$ and $12$.

We highlight two differences. First, from @fig-visnights-line, the MSE of projection does not always go down as the number of components increases --- see the $h=1$ panel with $p>150$. This can also be seen from @fig-visnights-mcb-series, where the two best methods are not significantly different, even though they have very different numbers of components ($p=77$ and $p=200$). Intuitively, choosing the number of components is a tradeoff between the increasing estimation error as the dimension of forecast variance $\bm{W}_h$ increases, and the additional benefit brought by the information embedded in the new components, depending on the complexity of the DGP. For the visitor nights data set, the benefit of components above the estimation error diminishes after the number reaches $p=m$.

Second, unlike @sec-simulation, using PCA as components is significantly better than simply using random normal weights, for the same large enough number of components (@fig-visnights-mcb-series). This is also clear from @fig-visnights-line, where the reduction of variance from using PCA is always in the lead, even after the $m$ PCAs are exhausted and random normal weighted components are added. As discussed in @sec-component-construction, the rationale awaits future research, but we propose two potential explanations for this superior performance, related to the variance maximisation and ranking of PCA.

1. Optimality: By maximizing and ranking the variance of PCs from largest to smallest, we ensure that the projection utilizes components containing significantly more information (as measured by variance) compared to randomly weighted components.
2. Diversity: Actively seeking PCs with the highest variance results in the incorporation of a more diverse set of components into the projection.

## FRED-MD {#sec-fred-md}

```{r fred-md}
```

The FRED-MD [@McCNg2016] data set is a popular monthly data set for macroeconomic variables, and shares similar properties with the @StoWat2002 data. We downloaded and transform the data set using the `fbi` [@fbi] package. The period we use for this exercise is from January 1959 to September 2023, containing $777$ observations. Following @McCNg2016, we replace observations that deviate from the sample median by more than 10 interquartile ranges (which are recognised as outliers), with missing values. We then drop any series with more than 5% observations missing. This left us with $m=122$ series. We fill in the missing values using the expectation-maximization (EM) algorithm described in @StoWat2002a with $8$ factors. The number $8$ is identified by @McCNg2016, albeit with a different time span. As the theory shows a reduction in forecast variance, we want to use MSE as the error measure, instead of other scaled or percentage error measures. To reliably calculate MSE over series with different scales, we demean the series to have mean $0$ and scale the series to have variance $1$. The MSEs are calculated on this standardised scale without back-transformation.

Similar to the visitor nights data, we evaluate the performance of forecasts using time series cross-validation. Starting with $300$ observations in the first training set and the following $12$ observations as the test set, we repeat the evaluation for the rest of the data with the size of the training set increasing by $1$ in each iteration. The base models are the univariate ARIMA model and the DFM model, as described in @sec-simulation. The ranges of the meta-parameters in the DFM models are $1\leq k \leq 8$ (since $8$ factors are identified and used to fill in the missing values), $1 \leq n \leq 3$ and $0 \leq s \leq 6$. The MCB plot and the MSE plot are shown in @fig-fred-md-mcb-series and @fig-fred-md-line.

```{r fig-fred-md-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the FRED-MD data. The methods using forecast projection are named as "{Model}-{Comp. Weights}-{No. Comp.}". The base models are named as "{Model}-Base" and these points are marked with triangles. The shaded region is the interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```
```{r fig-fred-md-line}
#| fig-cap: "MSE of different forecast models and component construction methods by the number of components $p$ used in forecast projection on the FRED-MD data, for forecast horizons $1$, $6$ and $12$. The vertical black line indicates the location of $p=m$ the number of series."
```

The first thing we note is the performance of base ARIMA exceeds that of the base DFM. This difference is significant at $h=6$ (@fig-fred-md-mcb-series). In terms of forecast projection, the best models at all forecast horizons are still forecast projections with PCA, and they are significantly better than the base models at $h=1$ and $6$. The fact that the projection with PCA is better than random weights, which can be seen from both the rankings in @fig-fred-md-mcb-series and the MSE in @fig-fred-md-line, reaffirms our finding about the difference between PCA and random weights in @sec-australian-domestic-tourism.

In @fig-fred-md-line, the forecast projection seems to be worse than the base models at the beginning, when the number of components is small, but improves with larger $p$ and outperforms the base models gradually. The tradeoff between the benefit of new components and the difficulty of estimation of a large dimension is also seen here, but seems to be more extreme: the MSEs start to increase as $p$ increases, once $p$ becomes larger than $m$. The projected forecast even worsens to the same level as the base forecast for ARIMA at $h=6$ and DFM at $h=12$ when $p=300$. The turning point seems to be at $m$, but as we have seen in @sec-simulation and @sec-australian-domestic-tourism, $m$ is not always a clear cut-off point. Where the performance of forecast projection turns should be jointly determined by the number of series $m$, the sample size $T$, the component construction method, and the DGP. In the case of FRED-MD, it signals the importance of PCA, as $m$ is the point that the component changes from PCA to random normal weighted linear combinations, implying PCA can exploit the information in the data while random weights cannot. This is more obvious for $h=1$ and $h=6$, as PCA works when $p<m$, but random normal weights do not seem to work from the beginning.


# Conclusion {#sec-conclusion}

In this article, we have proposed a new approach that we call free-lunch forecast projection which can reduce forecast variance of any multivariate forecasting problem, with any preferred base forecasting model. We simply add on a large set of linear combinations of the series, and forecast them as well. Then, after projecting onto the subspace spanned by the series, the forecasts are guaranteed to be at least as good as the original forecasts, and will often be much better.

We have shown that the forecast projection will reduce forecast variance with the help of the components, and it can be reduced monotonically by including more components, assuming we know the base forecast variance matrix. Furthermore, we show that for a given number of components, within the class of linear projection, the proposed method is the best in the sense that it minimises the forecast variance of the series. To handle the difficulty of estimating the forecast variance matrix, we suggest using a shrinkage estimator, which shrinks variances toward their median and covariances toward zero.

We have illustrated empirically that the proposed free-lunch forecast projection method outperforms base forecasts significantly, and confirm the theoretical results in simulation and two applications on the Australian domestic tourism data set and the FRED-MD data set. We find that using PCA to construct components can achieve satisfactory variance reduction, and leave the issue of finding alternative optimal components to a later article. We recognise, in certain ideal cases, that the use of forecast projection is even more important than the choice of base forecast model, emphasising the relative importance of forecast projection. We observe that the source of the variance reduction is the reduction of model misspecification error, while forecast projection has little impact on estimation error.

As usual, when proposing a new approach to a problem, there are many issues that remain unexplored, which we leave for future research. Some promising issues worthy of future attention include:

* Empirically, we have found that using PCA works better than random weights. We have speculated why this might be the case, but it would be interesting to explore more rigorously.
* There may be other ways to construct components that are better than PCA. For example, @Goerg2013-dg proposed "forecastable components" that are optimal in the sense of minimising the forecast variance of the components, and @Matteson2011-jf proposed "dynamic orthogonal components" that reduce a multivariate time series to a set of uncorrelated univariate time series. It would be interesting to explore whether these components (or other similar suggestions) can be used effectively in forecast projection.
* It may be possible to find optimal components by optimizing @eq-obj over $\bm{\Phi}$ and $\bm{G}$ rather than just $\bm{G}$.
* Our proposal was motivated by the forecast reconcililation literature, and it would be possible to use both forecast reconciliation and forecast projection together. This may be particularly useful when there are relationships between series that are not captured in the known hierarchical structure.

\todo[inline]{Comment on software implementation}

\todo[inline]{Comment on reproducibility and github repo}


# Acknowledgements {.unnumbered}

We thank Daniele Girolimetto for contributing to the initial proof of @thm-monotone in his unpublished work.

\pagebreak
# References {.unnumbered}
