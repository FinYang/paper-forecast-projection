---
title: "Forecast Linear Augmented Projection (FLAP): A free lunch to reduce forecast error variance"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author:
 - familyname: Yang
   othernames: Yangzhuoran Fin
   address: Monash University\newline Melbourne, Australia
   email: Fin.Yang@monash.edu
 - familyname: Athanasopoulos
   othernames: George
   address: Monash University\newline Melbourne, Australia
 - familyname: Hyndman
   othernames: Rob&nbsp;J
   address: Monash University\newline Melbourne, Australia
 - familyname: Panagiotelis
   othernames: Anastasios
   address: University of Sydney\newline Sydney, Australia
pdf-engine: pdflatex
cite-method: biblatex
biblio-style: authoryear-comp
bibliography:
  - references-key.bib
  - references-pkg.bib
blind: false
cover: true
toc: false
keep-tex: true
fig-height: 6
fig-width: 8
number-sections: true
number-depth: 2
execute:
  echo: false
  warning: false
  message: false
  cache: false
editor_options:
  chunk_output_type: console
filters:
  - latex-environment
  - abstract-section
environments: toappendix
header-includes:
  - \usepackage{todonotes}
  - \usepackage[bibliography=common]{apxproof}
  - \def\Var{\operatorname{Var}}
  - \def\E{\operatorname{E}}
  - \def\tr{\operatorname{tr}}
format: wp-pdf
---

# Abstract

A novel forecast linear augmented projection (FLAP) method is introduced, which reduces the forecast error variance of any unbiased multivariate forecast without introducing bias. The method first constructs new component series which are linear combinations of the original series. Forecasts are then generated for both the original and component series. Finally, the full vector of forecasts is projected onto a linear subspace where the constraints implied by the combination weights hold. It is proven that the trace of the forecast error variance is non-increasing with the number of components, and mild conditions are established for which it is strictly decreasing. It is also shown that the proposed method achieves maximum forecast error variance reduction among linear projection methods. The theoretical results are validated through simulations and two empirical applications based on Australian tourism and FRED-MD data. Notably, using FLAP with Principal Component Analysis (PCA) to construct the new series leads to substantial forecast error variance reduction.

```{r setup}
#| cache: false
knitr::read_chunk("free-lunch.R")
```
```{r path}
```

```{r library}
```

# Introduction {#sec-introduction}

We will show that any multivariate forecasting method can be improved through a post-processing framework involving linear combinations of the original series. We call this FLAP: Forecast Linear Augmented Projection. This has immediate implications for multivariate forecasting in all disciplines including macroeconomics [@macroreview] and finance [@financereview].

Our FLAP post-processing framework: (i) augments the data by constructing new series that are linear combinations of the original series; (ii) forecasts both the original and new series; and (iii) recovers a new set of forecasts for the original series via projections. We prove that the method reduces the forecast error variance of the original series in a way that is agnostic both with respect to the weights of the linear combinations used at step (i) and with respect to the model used to generate forecasts at step (ii). The model is inspired by the forecast reconciliation literature [@AthEtAl2023a] whereby forecasts are adjusted to cohere with known linear constraints. In contrast to that literature, the FLAP method focuses on multivariate forecasting where such constraints are not present. Indeed, the method need not only be applied to forecasting problems, but multivariate predictions in general.

It may appear puzzling that forecast accuracy can be improved, not by introducing any new information, but by simply taking linear combinations of existing time series. To give an intuition into how this puzzle can be resolved, we consider a toy example of two series $y_1$ and $y_2$ that are of concern to the forecaster, and two linear combinations or *components* of the series $c_1=0.5z_1+0.5z_2$ and $c_2=0.5z_1-0.5z_2$. Denote by $\hat{y}_1$, $\hat{y}_2$, $\hat{c}_1$, $\hat{c}_2$ any forecasts of these original series and components, which we collectively refer to as *base forecasts*. The base forecasts may be generated by univariate methods, multivariate methods, or even based on expert judgement. When considering $y_1$, there is both a *direct* forecast $\hat{y}_1$ and *indirect* forecast $\hat{c}_1+\hat{c}_2$, similarly for $y_2$ the direct forecast is $\hat{y}_2$ and the indirect forecast $\hat{c}_1-\hat{c}_2$.^[This argument, as well as the terminology direct and indirect forecasts, is inspired by @HolEtAl2021 who discuss this in the forecast reconciliation setting.] With the exception of some pathological cases, the direct and indirect forecasts for the same variable will not, in general, be equal. Therefore forecast accuracy can be improved by combining direct and indirect forecasts, something implicitly achieved by the proposed FLAP method. The puzzle is thus resolved; while no new information is created at the data augmentation step, there is new information embedded into forecasts of the augmented series, which can be leveraged via model combination (see @WanEtAl2023 for a review of forecast combination). Something obscured by the simple toy example is the way our FLAP method differs from the usual forecast combination methods, in particular our combinations are potentially non-convex since they are obtained via projections, in a way that we now elaborate upon.

More formally and more generally, the FLAP method considers a vector of original series $\mathbf{y}\in \mathbb{R}^m$ and a vector of components $\mathbf{c}\in \mathbb{R}^p$. While $\left(\mathbf{y}',\mathbf{c}'\right)'$ is a $p+m$-vector, the construction of components as linear combinations of the original series implies that $\left(\mathbf{y}',\mathbf{c}'\right)'$ lies on a linear subspace of at most dimension $m$. The corresponding vector of forecasts $\left(\hat{\mathbf{y}}',\hat{\mathbf{c}}'\right)'$ will, in general, have support on $\mathbb{R}^{m+p}$. FLAP projects $\left(\hat{\mathbf{y}}',\hat{\mathbf{c}}'\right)'$ onto the $m$-dimensional linear subspace on which $\left(\mathbf{y}',\mathbf{c}'\right)'$ has support. The setup of this problem bears similarities to the well known problem of forecast reconciliation where @PanEtAl2021 provide similar geometric intuition, while @WicEtAl2019, @AthEtAl2017, and @DiGir2023 have all shown that reconciliation can reduce forecast error variance theoretically and empirically. However, we note that these papers establish that reconciliation improves forecast accuracy for the hierarchy *as a whole*. In the general multivariate setting that we consider, this would imply improvements in forecast accuracy for $\mathbf{y}$ and $\mathbf{c}$ taken together. This poses a problem if improvements in forecast accuracy for $\mathbf{c}$ could be offset by a deterioration in forecast accuracy for $\mathbf{y}$, since the former are not of interest in and of themselves. A key insight we make in this paper, is that reductions in forecast error variance accrue even for a subset of variables in the hierarchy. It is this contribution that allows us to propose a method that goes beyond the case where time series adhere to linear constraints, and that instead applies to the more general setting.

While the theoretical results apply for any linear combinations of the original series, in practice we propose to augment the data with principal components (PCs). When doing so, the FLAP method bears a resemblance to Dynamic Factor Models (DFMs), specifically those common in macroeconomic forecasting [@StoWat2002a; @StoWat2002; @StoWat2012], their extensions in the machine learning literature [@DeEtAl2019], as well as the factor augmented VAR [@BerEtAl2005]. The factor models assume that the multivariate time series possesses common components and the dynamics of the observed series are governed by the dynamics of these unobserved factors, often assumed to follow some parametric model. In contrast, FLAP is a post-forecasting step; indeed, forecasts can even be made using a DFM and then further improved by implementing the FLAP method, something we demonstrate in @sec-simulation and @sec-empirical-applications.

<!-- , where we also show how the performance of projected forecasts from univariate ARIMA models is comparable to base DFM forecasts, with the help of components. -->

<!-- We introduce a new forecast linear augmented projection (FLAP) method for improving the accuracy of any multivariate time series method, often substantially. This is done by leveraging direct and indirect forecast of the series without introducing any new data, or any new information. Thus, we can call it a "free lunch" method [@NFLT]: it is a simple addition to any existing multivariate forecasting method that can improve its accuracy. -->

<!-- The method is based on the idea that the forecasts of linear combinations of the series should be consistent with the forecasts of the series themselves. For example, suppose we have two observed series $z_{t,1}$ and $z_{t,2}$, and we also construct the combination $c_t = 2 z_{t,1} - z_{t,1}$, then the forecasts of $c_t$ should satisfy the same linear constraint: $\hat{c}_t = 2 \hat{z}_{t,1} - \hat{z}_{t,2}$. If they do not, then we can improve the forecasts of all three series by adjusting them to be consistent. While the forecasts of $c_t$ may be of no interest in themselves, they can be used to improve the forecasts of $z_{t,1}$ and $z_{t,2}$. -->

<!-- The idea can be extended to any number of linear combinations, and can be applied to any number of multivariate series. It does not depend on the forecasting method being used, and works well even if all series are forecast using univariate models. In fact, when used with univariate models, this allows cross-correlations between the series to be implicitly captured in the forecasts. -->

<!-- We call these linear combinations of the observed time series "components", and we call the original forecasts of the observed series "base forecasts". Our FLAP method is to adjust the base forecasts to be consistent with the forecasts of the components by projecting all forecasts onto the space where the linear constraints are satisfied. We show (theoretically and empirically) that this method leads to significant reduction in the forecast error variance, without introducing any bias. -->

<!-- Our FLAP method has close connections to forecast reconciliation in the hierarchical forecasting literature. See @AthEtAl2023a for a recent review of the area. In particular, the projection formulation is inspired by the minimum trace [MinT, @WicEtAl2019] solution of the forecast reconciliation problem. Forecast reconciliation is a method to modify forecasts using projection so that they conform to a specific hierarchical, grouped or temporal structure. Notably, @WicEtAl2019, @AthEtAl2017, and @DiGir2023 have shown that forecast reconciliation can reduce forecast error variance theoretically and empirically, in cross-sectional settings, temporal settings, and in cross-temporal settings. @PanEtAl2021 have provided insight into the geometric interpretation of the projection used in forecast reconciliation. However, forecast reconciliation cannot be directly applied in a general multivariate time series unless the series satisfy some linear constraints such as a hierarchical structure. In contrast, our method can be applied to any multivariate time series. It can also be used in conjunction with forecast reconciliation to further reduce forecast error variance. -->

In the sense that forecast accuracy can be improved without any new information, FLAP has parallels with bootstrap aggregation or "bagging" [@Bre1996;@BerEtAl2016]. Bagging can reduce prediction variance without increasing bias [@HasEtAl2003], by mitigating model uncertainty [@PetEtAl2018], and does so without introducing any new data, but rather resampled versions of the existing data. Our FLAP method also reduces forecast error variance without introducing new data, but using linear combinations of the existing data, rather than bootstrapping. The FLAP method improves forecast accuracy through forecast combination and data augmentation. Other methods that also do this include the theta method [@AssNik2000], temporal aggregation [@KouEtAl2014; @AthEtAl2017], forecasting with sub-seasonal series [FOSS, @LiEtAl2022c] and forecast combination with multiple starting points [@DisPet2015]; a review of all these methods can be found in @PetSpi2021 who refer to them as using "the wisdom of data". Our FLAP method is distinct in that it aims to exploit information in the data with a focus on linear combinations of multivariate series.

<!-- A second difference is that bagging is model dependent: it is a procedure applied to enhance the models that produce the forecasts, where the same models are fitted repeatedly. Our method is model independent: it linearly transforms a set of forecasts, regardless of which models they come from. As a result, the two methods can be used in conjunction: the projections can be applied to forecasts produced by a bagged predictor. -->

<!-- Another approach to improve forecast accuracy is forecast combination. Point forecast combinations usually involve combining multiple forecasts of the same series from different models. See @WanEtAl2023 for a recent comprehensive review. Our FLAP method differs from forecast combination in (a) the forecasts we combine, and (b) in how we combine them. First, rather than combine multiple forecasts of a single series, we combine single forecasts of many different linear combinations of all observed series. Second, our combinations are obtained via projections, and so the final forecasts of a particular series are linear combinations of all series in the collection, including the observed series and all constructed components. Our approach can be used in conjunction with standard forecast combination, as the base forecasts can be obtained from any combination of forecasts. -->

The remainder of the paper is structured as follows. In @sec-method, we propose the FLAP method, and highlight its theoretical properties and associated estimation methods. In @sec-simulation, we present a simulation example demonstrating its performance and discuss the implications for sources of uncertainty. @sec-empirical-applications examines the performance of FLAP in two empirical applications: forecasting Australian domestic tourism and forecasting macroeconomic variables in the FRED-MD data set. @sec-conclusion concludes with some thoughts on future research directions. The methods introduced in this paper are implemented in the `flap` package for R [@flap]. This paper is fully reproducible with code and documentation provided at [https://github.com/FinYang/paper-forecast-projection](https://github.com/FinYang/paper-forecast-projection).

# Forecast Linear Augmented Projection (FLAP) {#sec-method}

## Method and theoretical properties

In the following, all vectors and matrices are denoted in bold font. We use $\bm{I}_n$ to denote the $n\times n$ identity matrix, and $\bm{O}_{n\times k}$ to denote the $n\times k$ zero matrix.

Let $\bm{y}_t\in\mathbf{R}^m$ be a vector of $m$ observed time series we are interested in forecasting. The FLAP method involves three steps:

  1. *Form components*. Form $\bm{c}_t = \bm{\Phi}\bm{y}_t\in\mathbf{R}^p$, a vector of $p$ linear combinations of $\bm{y}_t$ at time $t$, where $\bm{\Phi}\in\mathbf{R}^{p\times m}$. We call $\bm{c}_t$ the components of $\bm{y}_t$ and the component weights $\bm{\Phi}$ are known in the sense that they are chosen by the user of FLAP. Let $\bm{z}_{t} = \big[\bm{y}_t', \bm{c}'_{t}\big]'$ be the concatenation of series $\bm{y}_t$ and components $\bm{c}_{t}$. We note that $\bm{z}_{t}$ will be constrained in the sense that $\bm{C}\bm{z}_t= \bm{c}_{t} - \bm{\Phi}\bm{y}_t = \bm{0}$ for any $t$ where $\bm{C} = \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]$ is referred to as the constraint matrix.
  2. *Generate forecasts*. Denote as $\hat{\bm{z}}_{t+h}$ the $h$-step-ahead base forecast of $\bm{z}_{t}$. The method used to generate forecasts is again selected by the user. This can be univariate or multivariate. In the setting where $\bm{z}_t$ are not time series but cross sectional data, any prediction method can be used. In general, the constraints that hold for $\bm{z}_t$ will not hold for $\hat{\bm{z}}_{t+h}$, i.e $\bm{C}\hat{\bm{z}}_{t+h}\neq \bm{0}$
  3. *Project the base forecasts*. Let $\tilde{\bm{z}}_{t+h}$ be a set of projected forecasts such that,
$$
\tilde{\bm{z}}_{t+h} = \bm{M} \hat{\bm{z}}_{t+h}
$$ {#eq-z_tilde}
with projection matrix
$$
\bm{M} = \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C},
$$ {#eq-M}
where $\Var(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) = \bm{W}_h$ is the forecast error covariance matrix. For the proofs of this section, we will assume that $\bm{W}_h$ is known, in practice a plug-in estimate can be used that will be discussed in @sec-estW.

In practice we are interested in forecasts of $\bm{y}_t$ and not the full vector $\bm{z}_t$. We now introduce some notation to handle this issue. Define the selection matrix $\bm{J}_{n,k} = \big[\bm{I}_n ~~~ \bm{O}_{n\times k}\big]$, so that $\bm{J}_{n,k}\bm{A}$ selects the first $n$ rows of a matrix $\bm{A}$. Let $\hat{\bm{y}}_{t+h}$ and $\tilde{\bm{y}}_{t+h}$ denote the first $m$ elements of $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$, comprising the base and projected forecasts of $\bm{y}_t$ respectively. Similarly, let $\hat{\bm{c}}_{t+h}$ and $\tilde{\bm{c}}_{t+h}$ denote the last $p$ elements of $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$, comprising the base and projected forecasts of $\bm{c}_t$ respectively. Then the projected forecast of $\bm{y}_t$ can be found by
$$
\tilde{\bm{y}}_{t+h} = \bm{J}\tilde{\bm{z}}_{t+h} = \bm{J}\bm{M} \hat{\bm{z}}_{t+h},
$$ {#eq-lcmap}
where $\bm{J} = \bm{J}_{m,p}$.

We now present some theoretical results regarding the FLAP method, with proofs provided in the Appendix. @thm-psdvar establishes that the forecasts produced by the FLAP method, $\tilde{\bm{y}}_{t+h}$, dominate the base forecasts, $\hat{\bm{y}}_{t+h}$, in the sense that the difference between their forecast error variances is always positive semi-definite. @thm-monotone establishes that the trace of the covariance of the forecast errors of $\tilde{\bm{y}}_{t+h}$ is non-increasing with the number of components $p$. The conditions needed to make the trace strictly decreasing are discussed in @thm-pos-condition. In @thm-minvar we prove that the projection in @eq-M achieves the minimum forecast error variance amongst the class of all projections. Finally, while the theoretical results imply that components could in principle continue to be added to improve forecasts, in practice, larger values of $p$ will make estimation of the plug-in covariance matrix $\bm{W}_h$ unreliable. We explore this issue in a simulation setting and empirically in @sec-simulation and @sec-empirical-applications, respectively.

## Positive Semi-Definiteness of Error Variance Reduction

We first provide some intermediate results.

::: {#lem-projectionM}
Matrix $\bm{M}$ is a projection onto the space where the constraint $\bm{C}\bm{z}_t=\bm{0}$ is satisfied.
:::

Proof in Appendix, page \pageref{proof1}.

::: {.toappendix}
### Proof of @lem-projectionM
\label{proof1}
We have
$$
\begin{aligned}
\bm{M}\bm{M}
& = \bm{I}_{m+p} - 2 \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
& \mbox{}\hspace*{1cm} + \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\\
& =\bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} \\
& = \bm{M},
\end{aligned}
$$
so $\bm{M}$ is a projection matrix.
For any $\bm{z}$ such that $\bm{M}\bm{z} = \bm{y}$ for some $\bm{y}$, we have
$$
\bm{C}\bm{y} = \bm{C}\bm{M}\bm{z} = \bm{C}\bm{z} - \bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{z} = \bm{0}.
$$
Thus, $\bm{M}$ projects any vector onto the space where the constraint $\bm{C}\bm{y}=0$ is satisfied.
:::

Based on the properties of projections, we have the following corollaries.

::: {#cor-corM}
1. The projected forecast $\tilde{\bm{z}}_{t+h}$ satisfies the constraint $\bm{C}\tilde{\bm{z}}_{t+h}=\bm{0}$.
1. For $\bm{z}_{t+h}$ that already satisfies the constraint, the projection does not change its value, i.e., $\bm{M}\bm{z}_{t+h} = \bm{z}_{t+h}$ [@Rao1974-JRSSSBSM, Lemma 2.4].
1. If the base forecasts are unbiased such that $\E(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t)$, then the projected forecasts are also unbiased, i.e., $\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t)$.
:::

Proof in Appendix, page \pageref{proof2}.

::: {.toappendix}
### Proof of @cor-corM
\label{proof2}
Items 1 and 2 are trivial application of @lem-projectionM. To prove 3, we have
$$
\E(\tilde{\bm{z}}_{t+h}|\mathcal{I}_t) =
\E(\bm{M}\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{M}\E(\bm{z}_{t+h}|\mathcal{I}_t) = \E(\bm{M}\bm{z}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t).
$$
:::

The assumption of unbiasedness of the base forecasts is not unreasonable in practice, and where it does not hold, a bias correction method can be applied. Note this is not a requirement on model specification. We do not assume the model producing the base forecast is correctly specified like in the DFM literature [e.g., @StoWat2002]. In fact, the power of FLAP manifests when the models are misspecified, as discussed in @sec-simulation.

::: {#lem-var}
The forecast error covariance matrix of the component-augmented projected $h$-step-ahead forecasts $\tilde{\bm{z}}_{t+h}$ is
$$
\Var(\bm{z}_{t+h} - \tilde{\bm{z}}_{t+h}) = \bm{M}\bm{W}_h\bm{M}' = \bm{M}\bm{W}_h,
$$
and the forecast error covariance matrix of the projected $h$-step-ahead forecasts $\tilde{\bm{y}}_{t+h}$ is
$$
\Var(\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$
:::

Proof in Appendix, page \pageref{proof3}.

::: {.toappendix}
### Proof of @lem-var
\label{proof3}
$$
\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})
= \Var(\bm{M}\hat{\bm{z}}_{t+h} - \bm{M}\bm{z}_{t+h}) \\
= \bm{M}\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h})\bm{M}' \\
= \bm{M}\bm{W}_h\bm{M}'.
$$
If we simplify it further, we have
$$
\begin{aligned}
\bm{M}\bm{W}_h\bm{M}'
&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h(\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})'\\
%&= (\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h)(\bm{I} - \bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h) \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&\mbox{}\hspace*{1cm} + \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
&= \bm{W}_h - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h \\
%&= (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h \\
&= \bm{M}\bm{W}_h.
\end{aligned}
$$
To get $\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})$, we just need to recognise that it is the first $m\times m$ leading principal submatrix of $\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})$.
:::

@lem-var is a well-known result in the forecast reconciliation literature [e.g., @DiGir2023].

::: {#thm-psdvar}
#### Positive Semi-Definiteness of Error Variance Reduction

The difference between the forecast error variances of the base and projected component-augmented forecasts,
$$
\begin{aligned}
\Var(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\Var(\bm{z}_{t+h}-\tilde{\bm{z}}_{t+h} ) &=\bm{W}_h-\bm{M}\bm{W}_h\\
&= \bm{W}_h - (\bm{I} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\bm{W}_h\\
&=\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h,
\end{aligned}
$$
is positive semi-definite.
The difference between the forecast error variances of the base and projected forecasts of the original series,
$$
\Var(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\Var(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}',
$$
is therefore also positive semi-definite.
:::

Proof in Appendix, page \pageref{proof4}.

::: {.toappendix}
### Proof of @thm-psdvar
\label{proof4}
Trivially,
$\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$ and $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ are positive semi-definite. Note that $\Var(\hat{\bm{y}}_{t+h} - \bm{z}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{z}_{t+h})$ is the leading principal submatrix of $\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h$, and the leading principal submatrix of a positive semi-definite matrix is positive semi-definite.
:::

@thm-psdvar is why FLAP works. The trace of $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'$ is the sum of the reduction in forecast error variances, and is non-negative because the matrix is positive semi-definite. It implies that the forecast error variance can be reduced by simply forecasting the components (the artificially constructed linear combinations of the original data), and mapping the forecasts using matrix $\bm{M}$. For the improvement to be zero, the trace must be zero. This implies that $\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}' = \bm{O}_{m\times m}$, as this is a positive semi-definite matrix, something rarely observed in practice. See @thm-pos-condition for more discussion.

The following example illustrates the mechanism of the reduction in the forecast error variance.

::: {#exm-identityW}

Suppose $\bm{z}_t$ comprises $m$ original series $\bm{y}_t$ and $p$ components $\bm{c}_t$. Let $\hat{\bm{z}}_{t+h}$ and $\tilde{\bm{z}}_{t+h}$ be $h$-step-ahead base and projected forecasts of $\bm{z}_t$. Assume that their corresponding forecast errors are uncorrelated with unit variance such that $\bm{W}_h = \bm{I}_{m+p}$. Then
$$
\begin{aligned}
\Var(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})
&=
\bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C}\\
& =
\begin{bmatrix} -\bm{\Phi}'\\ \bm{I}_{p} \end{bmatrix}
(\bm{\Phi}\bm{\Phi}' + \bm{I}_p)^{-1}
\begin{bmatrix} -\bm{\Phi}& \bm{I}_{p} \end{bmatrix} ,
\end{aligned}
$$
where
$$
\bm{C} = \begin{bmatrix} - \bm{\Phi}& \bm{I}_{p} \end{bmatrix}.
$$
Let $\bm{\Phi}$ consist of $p\le m$ orthogonal unit vectors, for example, those obtained from Principal Component Analysis [PCA, @Jol2002]. In this case $\bm{\Phi}\bm{\Phi}' = \bm{I}_p$ and
$$
\Var(\bm{z}_{t+h}-\hat{\bm{z}}_{t+h}) -\Var(\bm{z}_{t+h}-\tilde{\bm{z}}_{t+h})
  =  \frac{1}{2}
      \begin{bmatrix}
        \bm{\Phi}'\bm{\Phi} & -\bm{\Phi}'\\
        -\bm{\Phi} & \bm{I}_p
       \end{bmatrix}.
$$
Focusing on the forecast error variance reduction of the forecasts of the original series $\bm{y}_t$, i.e., $\tr(\Var(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\Var(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h})) = \frac{1}{2}\tr(\bm{\Phi}'\bm{\Phi})$.

- When $p<m$, since $\bm{\Phi}'\bm{\Phi}$ is idempotent, $\tr(\bm{\Phi}'\bm{\Phi}) = \operatorname{rank}(\bm{\Phi}'\bm{\Phi}) = p$. Hence, focusing on the original series the reduction in the total forecast error variance of the FLAP forecasts relative to the base forecasts, is $p/2$.
- When $p=m$, where all principal components are used, $\bm{\Phi}'\bm{\Phi} = \bm{I}_m$. This implies a total reduction in the error variance of $m/2$, and that for each of the $m$ individual series the error variance is halved.

<!-- NOT SURE THIS HELPS -- In simplest of cases where we have two original series ($m=2$), using $p=1$ component will reduce the total forecast error variance by $0.5$, while using both components ($p=2$) will reduce the total forecast error variance by $1$; that is a $50\%$ reduction as the original sum of forecast error variances is $2$. -->

If we keep increasing the number of components beyond $m$, the result in @thm-psdvar still holds, although $\bm{\Phi}$ can no longer contain orthogonal vectors, and the example here becomes intractable. This is an artificial example as the forecast error variance $\bm{W}_h$ can hardly be an identity matrix in practice. It is likely that the forecast error of a linear combination of series will be correlated to the forecast error of forecasting these series directly. Nonetheless, the aim of the example is to demonstrate how the forecast error variance can be reduced as a result of the component forecasts bringing new information about the original series. The forecast error variance reduction becomes larger as we increase the number of components $p$. This is not a coincidence but a desirable property of FLAP, as shown in the next section.

:::

## Monotonicity

In the results that follow, we break the base forecast error covariance matrix into smaller blocks.
$$
\bm{W}_h =
\begin{bmatrix}
\bm{W}_{y, h} & \bm{W}_{yc, h} \\
\bm{W}_{yc, h}' & \bm{W}_{c, h}
\end{bmatrix},
$$
where $\bm{W}_{y, h}$ is the forecast error covariance matrix of $\hat{\bm{y}}_{t+h}$, $\bm{W}_{c, h}$ is the forecast error covariance matrix of $\hat{\bm{c}}_{t+h}$,
and $\bm{W}_{yc, h}$ contains error covariances between elements of $\hat{\bm{y}}_{t+h}$ and $\hat{\bm{c}}_{t+h}$.

::: {#thm-monotone}
#### Monotonicity
The forecast error variance reductions for each series, i.e., the diagonal elements in the matrix
$$
\Var(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\Var(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h}) = \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'
$$
is non-decreasing as $p$ increases.
In particular, the sum of forecast error variance reductions
$$
\tr(\Var(\bm{y}_{t+h}-\hat{\bm{y}}_{t+h}) -\Var(\bm{y}_{t+h}-\tilde{\bm{y}}_{t+h})) = \tr(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}')
$$ {#eq-trdvar}
is non-decreasing as $p$ increases.
:::

Proof in Appendix, page \pageref{proof5}.

::: {.toappendix}
### Proof of @thm-monotone
\label{proof5}
Suppose now that we want to include $q$ more components $\bm{c}_{t}^*=\bm{\Phi}^*\bm{y}_t$ in the projection. We define $\bm{z}_{t}^* = \begin{bmatrix}\bm{z}_{t}\\ \bm{c}_{t}^*\end{bmatrix}$, the constraint matrix
$$
\bm{C}^* =
\begin{bmatrix}
\multicolumn{2}{c}{\bm{C}} & \underset{p\times q}{\bm{0}} \\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q}
\end{bmatrix}
=
\begin{bmatrix}
-\underset{p \times m}{\bm{\Phi}}& \bm{I}_{p} &\underset{p\times q}{\bm{0}}\\
-\underset{q\times m}{\bm{\Phi}^*} & \underset{q\times p}{\bm{0}} & \bm{I}_{q},
\end{bmatrix}
=
\begin{bmatrix}
\overline{\bm{C}} \\ \underline{\bm{C}}
\end{bmatrix}
$$ {#eq-Cstar}
where $\overline{\bm{C}}$ contains the first $p$ rows of $\bm{C}^*$ and $\underline{\bm{C}}$ contains the remaining $q$ rows of $\bm{C}^*$,
the forecast error variance matrix
$$
\Var(\hat{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*) = \bm{W}_h^* = \begin{bmatrix}\bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c,h}^*\end{bmatrix}.
$$
where $\hat{\bm{z}}_{t+h}^*$ is the $h$-step-ahead base forecasts of $\bm{z}_{t}^*$:
$$
\hat{\bm{z}}_{t+h}^* = \begin{bmatrix} \hat{\bm{z}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix},
$$
and the corresponding
$$
\bm{M}^* = \bm{I} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*.
$$

Proving @thm-monotone requires proving the following two items.

1. Including additional components in the mapping without including corresponding component constraints is equivalent to not including these additional components at all.
1. For a fixed set of components to be included in the mapping, adding constraints will reduce forecast error variance.

We start by proving the first statement. Consider the case where we include the additional series $\bm{c}_{t}^*$ without using the additional constraint $\bm{\Phi}^*$. Defining $\bm{M}^{+}$ only with $\overline{\bm{C}}$:
$$
\bm{M}^{+} = \bm{I}_{m+p+q} - \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}},
$$ {#eq-Mplus}
we have $\tilde{\bm{z}}_{t+h}^+ = \bm{M}^{+}\hat{\bm{z}}_{t+h}^*$. Further, we obtain
$$
\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{W}_h & \bm{W}_{yc, h}^*\\ \bm{W}_{cy, h}^* & \bm{W}_{c, h}^*
    \end{bmatrix}
    \begin{bmatrix}
      \bm{C}'\\ \underset{q\times p}{\bm{0}} \\
    \end{bmatrix} \\
 = \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
$$
and
$$
  \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
 = \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix} \\
 =\bm{C}\bm{W}_h\bm{C}',
$$
which gives
$$
\begin{aligned}
\bm{M}^{+}
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}' \\
      \bm{W}_{cy, h}^*\bm{C}'
    \end{bmatrix}
    (\bm{C}\bm{W}_h\bm{C}')^{-1}
    \begin{bmatrix}
      \bm{C} & \underset{p\times q}{\bm{0}} \\
    \end{bmatrix}\\
%& = \bm{I}_{m+p+q} -
%    \begin{bmatrix}
%      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1} \\
%      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}
%    \end{bmatrix}
%    \begin{bmatrix}
%      \bm{C} & \underset{p\times q}{\bm{0}} \\
%    \end{bmatrix} \\
& = \bm{I}_{m+p+q} -
    \begin{bmatrix}
      \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
      \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
    \end{bmatrix},
\end{aligned}
$$
and
$$
\begin{aligned}
\tilde{\bm{z}}_{t+h}^+
& = \bm{M}^{+}\hat{\bm{z}}_{t+h}^* \\
& = \left(
      \bm{I}_{m+p+q} -
      \begin{bmatrix}
        \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0} \\
        \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C} & \bm{0}
      \end{bmatrix}
    \right)
    \begin{bmatrix} \hat{\bm{z}}_{t+h} \\ \hat{\bm{c}}_{t+h}^* \end{bmatrix}\\
%& = \begin{bmatrix}
%      \hat{\bm{z}}_{t+h} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}\\
%      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
%    \end{bmatrix} \\
& = \begin{bmatrix}
      (\bm{I}_{n+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C})\hat{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \bm{M} \hat{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \tilde{\bm{z}}_{t+h}\\
      \hat{\bm{c}}_{t+h}^* - \bm{W}_{cy, h}^*\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\hat{\bm{z}}_{t+h}
    \end{bmatrix}.
\end{aligned}
$$
If we only consider the forecast performance relevant to $\bm{y}_{t+h}$, and define $\bm{J}^* = \bm{J}_{m,p+q}= \begin{bmatrix}\bm{I}_{m} & \bm{0}_{m\times(p+q)} \end{bmatrix}$, we have
$$
\tilde{\bm{y}}_{t+h}^+ = \bm{J}^*\tilde{\bm{z}}_{t+h}^+ = \bm{J}\tilde{\bm{z}}_{t+h} = \tilde{\bm{y}}_{t+h}.
$$
This means adding additional components without imposing the corresponding constraints will yield the same projected forecasts as if these additional components are not added, which implies that the forecast error variance stays the same:
$$
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}) = \Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) = \bm{J}\bm{M}\bm{W}_h\bm{J}'.
$$ {#eq-acnc}
<!-- additional components no constraints -->
This finishes the proof of the first statement. Now we move on to proving the second statement. We have the forecast error variance matrices
$$
\begin{aligned}
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) & = \bm{M}^{+}\bm{W}_h^*
    = (\bm{I}_{m+p+q} -\bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
\text{and}\qquad\qquad
\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*) & = \bm{M}^*\bm{W}_h^*
    = (\bm{I}_{m+p+q} - \bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*)\bm{W}_h^*.
\end{aligned}
$$
Taking the difference, we have
$$
\begin{aligned}
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) - \Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)
& = (\bm{W}_h^*\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \bm{W}_h^*\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^* \\
& = \bm{W}_h^*(\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^* -
      \overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*.
\end{aligned}
$$
Using block matrix inversion, we have
$$
\begin{aligned}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' & \underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    \end{bmatrix}^{-1}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& = \begin{bmatrix}
      \overline{\bm{C}}' & \underline{\bm{C}}'
    \end{bmatrix}
    \begin{bmatrix}
      a & b \\
      c & d
    \end{bmatrix}
    \begin{bmatrix}
      \overline{\bm{C}}\\  \underline{\bm{C}}
    \end{bmatrix} \\
& =   \overline{\bm{C}}'a \overline{\bm{C}}
    + \overline{\bm{C}}'b \underline{\bm{C}}
    + \underline{\bm{C}}'c\overline{\bm{C}}
    + \underline{\bm{C}}'d\underline{\bm{C}},
\end{aligned}
$$
where
$$
\begin{aligned}
a &= (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
     (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' \\
  & \mbox{}\hspace*{1cm}
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' - \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}
    \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
    (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} \\
  & = (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1} +
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
b & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}' -
     \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}' (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
     \overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}')^{-1}\\
  & = - (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
    (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1},\\
c & = - (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
      \underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
      (\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1},\\
d & = (\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}.
\end{aligned}
$$
Thus,
\begin{align*}
\bm{C}^{*\prime}(\bm{C}^*\bm{W}_h^*\bm{C}^{*\prime})^{-1}\bm{C}^*
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}'\bm{W}_h^*\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\\
&=
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\\
& \mbox{}\hspace*{1cm} -
\overline{\bm{C}}
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}\bm{W}_h^*\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& \mbox{}\hspace*{1cm} +
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}\\
& =
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}.
\end{align*}
Therefore,
$$
\begin{aligned}
\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*) - \Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)
\hspace*{-4cm} \\
& =
\bm{W}_h^*(
\overline{\bm{C}}'
(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}
\overline{\bm{C}}
+
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
-\overline{\bm{C}}'(\overline{\bm{C}}\bm{W}_h^*\overline{\bm{C}}')^{-1}\overline{\bm{C}})\bm{W}_h^*\\
& =
\bm{W}_h^*(
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
)\bm{W}_h^*
\end{aligned}
$$
is positive semi-definite. This concludes the proof of the second statement.
Combining the results above, we have
$$
\begin{aligned}
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) -
\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h})
& =
\Var(\tilde{\bm{y}}_{t+h}^+ - \bm{y}_{t+h}) -
\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}) \\
&=
\bm{J}^*\Var(\tilde{\bm{z}}_{t+h}^+ - \bm{z}_{t+h}^*)\bm{J}^{*\prime} -
\bm{J}^*\Var(\tilde{\bm{z}}_{t+h}^* - \bm{z}_{t+h}^*)\bm{J}^{*\prime} \\
&=
\bm{J}^*
\bm{W}_h^*
\bm{M}^{+\prime}
\underline{\bm{C}}'
(\underline{\bm{C}}\bm{M}^{+}\bm{W}_h^*\underline{\bm{C}}')^{-1}
\underline{\bm{C}}\bm{M}^{+}
\bm{W}_h^*
\bm{J}^{*\prime}
\end{aligned}
$$ {#eq-addcom}
being positive semi-definite.
Finally, we have
\begin{multline*}
(\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h}))
-
(\Var(\hat{\bm{y}}_{t+h} - \bm{y}_{t+h}) -\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h})) \\
=
    \Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) -
    \Var(\tilde{\bm{y}}_{t+h}^* - \bm{y}_{t+h})
\end{multline*}
being a positive semi-definite matrix where the diagonal terms are non-negative, whose trace, therefore, is non-negative. This means using a larger number of components in the mapping achieves lower forecast error variances, giving @thm-monotone.
:::

@thm-monotone is the key result that demonstrates the usefulness of FLAP. It means that we can keep increasing the number of components to reduce forecast error variance, even when the number of components exceeds the number of original series. It requires $\bm{C}$ to be $\big[-\bm{\Phi} ~~~ \bm{I}_{p} \big]$ or $\big[-\bm{\Phi} ~~~ \bm{L}\big]$ where $\bm{L}$ is a lower triangular matrix. This implies that the components can also be constructed from existing components, not only from the original series. This has little significance since a linear combination of components of the original series, is just a linear combination of the original series. This of course assumes that the forecast error covariances are known, something we explore in @sec-estW and @sec-simulation.

Extending the proof of @thm-monotone, we can state the condition required for the reduction sum of forecast error variances to be strictly positive. Denote $\bm{\phi}_i$ as the row vector containing the weights associated with the $i$th component, so that with $p$ components, the weights matrix is $\bm{\Phi} = \big[\bm{\phi}_1' ~~~ \bm{\phi}_2' ~~~ \dots ~~~ \bm{\phi}_p'\big]'$. Let $\bm{W}^{(i-1)}_{\tilde{\bm{y}},h}$ denote the error covariance matrix of the projected forecasts of the original series based on the first $i-1$ components, $\bm{w}_{c_{1}\hat{\bm{y}},h}$ denote a vector of covariances of the first component and the base forecasts of the original series, and $\bm{w}_{c_i\tilde{y},h}^{(i-1)}$ denote a vector of covariances of the projected $i$th component and the projected forecasts of the original series, based on the first $i-1$ components.

::: {#thm-pos-condition}

#### Positive Forecast Error Variance Reduction Condition

For the first component to achieve a guaranteed reduction of forecast error variance (for the matrix in @thm-psdvar to have strictly positive trace),
$$
\bm{\phi}_1\bm{W}_{y,h}\neq\bm{w}_{c_1y,h}.
$$ {#eq-pcon}
For the $i$th component to have a positive reduction on forecast error variance of the original series,
$$
\bm{\phi}_i{\bm{W}}^{(i-1)}_{\tilde{y},h}\neq\bm{w}_{c_i\tilde{y},h}^{(i-1)}.
$$ {#eq-pconi}
:::

Proof in Appendix, page \pageref{proof6}.

::: {.toappendix}
### Proof of @thm-pos-condition
\label{proof6}
Denote $\bm{\psi}_i=\begin{bmatrix}-\bm{\phi}_i & \bm{0}_{1\times(i-1)} & 1\end{bmatrix}$ and $\bm{W}_{h}^{(i)}$ to be the base forecast error variance of the original series and the first $i$ components. Starting with the first component, @eq-trdvar becomes
$$
\tr(\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1'(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}') =
(\bm{\psi}_1\bm{W}_h^{(1)}\bm{\psi}_1')^{-1}\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}'\bm{J}_{m,1}\bm{W}_h^{(1)}\bm{\psi}_1',
$$ {#eq-trdvar1}
$$
\begin{aligned}
\text{where}\qquad\qquad
\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' =&
\begin{bmatrix}
-\bm{\phi}_1 & 1
\end{bmatrix}
\begin{bmatrix}
\bm{W}_{z,h} & \bm{w}'_{c_1z,h} \\
\bm{w}_{c_1z,h} & \bm{W}_{c_1,h}
\end{bmatrix}
\begin{bmatrix}
\bm{I}_m\\ 0
\end{bmatrix} \\
=& -\bm{\phi}_1\bm{W}_{z,h} + \bm{w}_{c_1z,h}.
\end{aligned}
$$
@eq-trdvar1 is obviously non-negative. For it to be larger than $0$, we need $\bm{\psi}_1\bm{W}_h^{(1)}\bm{J}_{m,1}' \neq 0$, which gives $\bm{\phi}_1\bm{W}_{z,h} \neq \bm{w}_{c_1z,h}$.

When it comes to adding the $i$th component on top of the first $i-1$ components, we define
$$
\overline{\bm{C}}_i =
\begin{bmatrix}
\bm{\psi}_1 & \bm{0}_{1\times i} \\
\bm{\psi}_2 & \bm{0}_{1\times (i-1)} \\
\vdots & \vdots \\
\bm{\psi}_i & 0 \\
\end{bmatrix}
$$
and
$$
\bm{M}^+_i = \bm{I}_{m+i} - \bm{W}_h^{(i)} \overline{\bm{C}}_{i-1}'
(\overline{\bm{C}}_{i-1}\bm{W}_h^{(i)}\overline{\bm{C}}_{i-1}')^{-1}
\overline{\bm{C}}_{i-1}
$$
analogously to @eq-Cstar and @eq-Mplus.
Following @eq-addcom, the additional reduction of forecast error variance when adding the $i$th component becomes
$$
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
=
(\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{\psi}_i')^{-1}
\bm{\psi}_i
\bm{M}^{+}_i
\bm{W}_h^{(i)}
\bm{J}_{m,i}'
\bm{J}_{m,i}
\bm{W}_h^{(i)}
\bm{M}^{+\prime}_i
\bm{\psi}_i'.
$$
Similar to before, we would want $\bm{\psi}_i\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}' \neq \bm{0}$. Note that $\bm{\psi}_i$ concerns the first $m$ rows and the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, and $\bm{J}_{m,i}'$ concerns the first $m$ columns. Combined with the implication from @eq-acnc that the $m\times m$ leading principal submatrix in equation $\bm{J}_{m,i}\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}'=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the same, we suppress the straightforward yet tiresome details, and obtain
$$
\bm{\phi}_i\bm{W}_{\tilde{z},h}^{(i-1)} \neq \big[\bm{0}_{1\times m+i-1} ~~~ 1 \big]\bm{M}^{+}_i\bm{W}_h^{(i)}\bm{J}_{m,i}',
$$
where $\bm{W}_{\tilde{z},h}^{(i-1)}=\bm{J}_{m,i-1}\bm{M}_{i-1}\bm{W}_h^{(i-1)}\bm{J}_{m,i-1}'$ is the projected forecast error variance of the original series using the first $i-1$ components, and the right hand side of the inequality is simply a one-row matrix consisting of the first $m$ elements in the last row of $\bm{M}^{+}_i\bm{W}_h^{(i)}$, which can be denoted as $\bm{w}_{\tilde{c}_i\tilde{z},h}^{(i-1)}$ and interpreted as the covariance between the projected forecast of the original series using the first $i-1$ components, and the projected forecast of the $i$th component using the first $i-1$ components.
:::

The condition of @eq-pcon demonstrates that for a new component to be beneficial, the information introduced by this new component, reflected in the error covariance, cannot be a linear combination of already existing information.

@thm-pos-condition can potentially provide insights into the selection of component weights and forecast models to satisfy the conditions. Heuristically, @thm-pos-condition our choice of principal components as weight vectors since they are orthogonal by constructions. Optimal choices of weights we leave to future research, but note that practically speaking, the conditions in @thm-pos-condition are either almost always satisfied if the weights are simulated randomly on a continuous scale, or the loss associated with the rare occasions where the conditions are not satisfied is negligible compared to the estimation error imposed by the limited sample size as the number of components increases, as discussed in @sec-simulation and @sec-empirical-applications.

## Optimality of the projection

@eq-z_tilde can be seen as a solution to the optimisation problem,
$$
\underset{\check{\bm{z}}_{T+h}}{\arg\min}
  (\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})'\bm{W}_h^{-1}(\hat{\bm{z}}_{T+h}-\check{\bm{z}}_{T+h})
\qquad \text{s.t. } \bm{C}\check{\bm{z}}_{T+h}=0.
$$
If we consider the transformed space where all the vectors are first transformed via pre-multiplying by $\bm{W}_h^{-1/2}$, where $\bm{W}_h^{-1} = (\bm{W}_h^{-1/2})'\bm{W}_h^{-1/2}$, then this optimisation problem can be interpreted as finding the set of forecasts that are closest to the base forecasts on the transformed space, while satisfying the linear constraints imposed by the components.

An alternative way of characterising the same constraints is 
$$ 
\bm{\Phi}\check{\bm{y}}_{T+h}=\check{\bm{c}}_{T+h},
$$
where $\check{\bm{c}}_{T+h}$ is the vector of the last $p$ elements of $\check{\bm{z}}_{T+h}$, corresponding to the forecast of the components as part of the solution. This equivalence is discussed in @WicEtAl2019, where the authors find the solution by minimising the sum of forecast error variance of all series (See @AndNar2024-F for an alternative proof). The result is
$$
\tilde{\bm{z}}_{t+h} = \bm{S}\bm{G} \hat{\bm{z}}_{t+h},
$$ {#eq-mint}
where
$\bm{S} = \begin{bmatrix}\bm{I}_m \\\bm{\Phi}\end{bmatrix}$ contains the constraints, so that $\bm{z}_{t} = \bm{S}\bm{y}_{t}$, and\pagebreak[3]
$$
\bm{G} = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$ {#eq-G}
In @eq-mint, $\bm{G} \hat{\bm{z}}_{t+h}$ can be viewed as mapping all of the series to a selected few. In the forecast reconciliation context, this is a mapping of all series to the "bottom level". In our multivariate forecasting context, this is a mapping of all series including the components, to the space of the original series. This leads to the solution
$$
\tilde{\bm{y}}_{t+h} = \bm{G}\hat{\bm{z}}_{t+h},
$$ {#eq-stmap}
as equivalent to @eq-lcmap. Recognising that @eq-mint is equivalent to @eq-z_tilde, it is the solution that minimises the sum of forecast error variances of the original series and all the components. We go further in @thm-minvar, and show that @eq-mint is also the solution to minimise each individual forecast error variance of the original series, and their sum. This can be viewed as a special case of Theorem 3.3 in @PanEtAl2021 applied in a forecast projection context, by taking $\bm{W}=\bm{S}(\bm{S}'\bm{S})^{-1}(\bm{S}'\bm{S})^{-1}\bm{S}'$ in @PanEtAl2021 [Thm. 3.3], as illustrated by @AndNar2024-F. The earliest work we can find that noted this interpretation in a non-forecasting context is @Lue1969 [p. 85]. We establish a few basic results leading to the optimality of this solution first, also to check that @lem-projectionM, @cor-corM and @lem-var hold under this alternative representation.

::: {#lem-projectionG}
The matrix $\bm{S}\bm{G}$ is a projection onto the space where the constraint $\bm{C}\bm{z}_t=\bm{0}$ is satisfied, provided that $\bm{G}\bm{S}=\bm{I}$.
:::

Proof in Appendix, page \pageref{proof7}.

::: {.toappendix}
### Proof of @lem-projectionG
\label{proof7}
If $\bm{G}\bm{S}=\bm{I}$, $\bm{S}\bm{G}$ is a projection matrix: $\bm{S}\bm{G}\bm{S}\bm{G} = \bm{S}\bm{G}$.

For any $\bm{z}$ such that $\bm{S}\bm{G}\bm{z} = \bm{y}$ for some $\bm{y}$, we have $\bm{C}\bm{y} = \bm{C}\bm{S}\bm{G}\bm{z} = \bm{0}$
because $\bm{C}\bm{S} = \big[-\bm{\Phi} ~~~ \bm{I} \big] \big[\bm{I} ~~~ \bm{\Phi}' \big]' = \bm{0}$. Similarly to $\bm{M}$, $\bm{S}\bm{G}$ projects a vector to the same space where $\bm{C}$ is satisfied.
:::

::: {#cor-corG}
Provided that $\bm{G}\bm{S} = \bm{I}$, the following results hold.

1. The projected forecast in @eq-stmap satisfies the constraint $\bm{C}\tilde{\bm{z}}_{t+h}= \bm{C}\bm{S}\tilde{\bm{y}}_{t+h}= \bm{0}$.
1. For $\bm{z}_{t+h}$ that already satisfies the constraint, the mapping does not change its value, i.e., $\bm{G}\bm{z}_{t+h} = \bm{y}_{t+h}$.
1. If the base forecasts are unbiased such that $\E(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \E(\bm{z}_{t+h}|\mathcal{I}_t)$, then the projected forecasts in @eq-stmap are also unbiased: $\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t)$.
:::

Proof in Appendix, page \pageref{proof8}.

::: {.toappendix}
### Proof of @cor-corG
\label{proof8}
Item 1 is an direct application of @lem-projectionG. From @lem-projectionG and Lemma 2.4 in @Rao1974-JRSSSBSM, we have
$$
\bm{S}\bm{G}\bm{z}_{t+h} = \bm{z}_{t+h} = \bm{S}\bm{y}_{t+h}.
$$
Left multiplying by $\bm{G}$ on both sides, we have $\bm{G}\bm{z}_{t+h} = \bm{y}_{t+h}$ and item 2 is proven.
To prove Item 3, we have
$$
\E(\tilde{\bm{y}}_{t+h}|\mathcal{I}_t) =
\E(\bm{G}\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\hat{\bm{z}}_{t+h}|\mathcal{I}_t) = \bm{G}\E(\bm{z}_{t+h}|\mathcal{I}_t) = \E(\bm{G}\bm{z}_{t+h}|\mathcal{I}_t) = \E(\bm{y}_{t+h}|\mathcal{I}_t).
$$
:::

::: {#lem-st-var}
The covariance matrix of the projected forecasts from @eq-stmap is given by
$$
\Var(\bm{y}_{t+h} - \tilde{\bm{y}}_{t+h}) = \bm{G}\bm{W}_h\bm{G}'.
$$
:::

Proof in Appendix, page \pageref{proof9}.

::: {.toappendix}
### Proof of @lem-st-var
\label{proof9}
Let the base and projected forecast errors be given as
$$
\begin{aligned}
\hat{\bm{e}}_{y, t+h} &= \bm{y}_{t+h} - \hat{\bm{y}}_{t+h},\\
\hat{\bm{e}}_{z, t+h} &= \bm{z}_{t+h} - \hat{\bm{z}}_{t+h},\\
\tilde{\bm{e}}_{y, t+h} &= \bm{y}_{t+h} - \tilde{\bm{y}}_{t+h},\\
\text{and}\qquad
\tilde{\bm{e}}_{z, t+h} &= \bm{z}_{t+h} - \tilde{\bm{z}}_{t+h}
= \bm{S}\bm{y}_{t+h} - \bm{S}\tilde{\bm{y}}_{t+h}
= \bm{S} \tilde{\bm{e}}_{y, t+h}. \\
\text{Then we have}\qquad
\tilde{\bm{e}}_{z, t+h}
&= \hat{\bm{e}}_{z, t+h} + \hat{\bm{z}}_{t+h} - \tilde{\bm{z}}_{t+h}\\
&= \hat{\bm{e}}_{z, t+h} + \hat{\bm{z}}_{t+h} - \bm{S}\bm{G}\hat{\bm{z}}_{t+h}\\
&= \hat{\bm{e}}_{z, t+h} + (\bm{I} - \bm{S}\bm{G})(\bm{z}_{t+h} - \hat{\bm{e}}_{z, t+h})\\
\text{and}\qquad
&= \bm{S}\bm{G}\hat{\bm{e}}_{z, t+h} + (\bm{I} - \bm{S}\bm{G})\bm{S}\bm{y}_{t+h}\\
\bm{S} \tilde{\bm{e}}_{y, t+h} &= \bm{S}\bm{G}\hat{\bm{e}}_{z, t+h},
\end{aligned}
$$
where the last line comes from $\bm{G}\bm{S} = \bm{I}$.
Left multiplying by $\bm{G}$ on both sides, we have
$$
\bm{G}\bm{S} \tilde{\bm{e}}_{y, t+h} = \bm{G}\bm{S}\bm{G}\hat{\bm{e}}_{z, t+h}
\qquad\text{and}\qquad
\tilde{\bm{e}}_{y, t+h} = \bm{G}\hat{\bm{e}}_{z, t+h},
$$
and therefore
$$
\Var(\tilde{\bm{y}}_{t+h} - \bm{y}_{t+h}) =\Var(\tilde{\bm{e}}_{y, t+h})=\Var(\bm{G}\hat{\bm{e}}_{z, t+h})= \bm{G}\Var(\hat{\bm{e}}_{z, t+h})\bm{G}'= \bm{G}\bm{W}_h\bm{G}'.
$$
:::

We are now ready to present the following theorem.

::: {#thm-minvar}

#### Minimum Variance Unbiased Projected Forecast

The solution to
$$
\underset{\bm{G}}{\arg\min}\ \tr(\bm{G}\bm{W}_h\bm{G}')
\qquad \text{s.t. } \bm{G}\bm{S} = \bm{I}
$$ {#eq-obj}
is @eq-G. This problem can be effectively split into independent subproblems such that $\bm{G} = \big[\bm{g}_1 ~~ \bm{g}_2 ~~ \dots ~~ \bm{g}_m\big]'$, where $\bm{g}_i$ is the solution to the subproblem of the $i$th series
$$
\underset{\bm{g}_i}{\arg\min}\ \bm{g}_i'\bm{W}_h\bm{g}_i
\qquad \text{s.t. } \bm{g}_i'\bm{s}_{j} = \delta_{ij}, \quad j= 1, 2, .\ldots, m,
$$ {#eq-subobj}
where $\bm{s}_j$ is the $j$th column of $\bm{S}$, and $\delta_{ij}$ is the Kronecker delta function taking value 1 if $i = j$ and 0 otherwise.
:::

Proof in Appendix, page \pageref{proof10}.

::: {.toappendix}
### Proof of @thm-minvar
\label{proof10}
This can be proved in a few different ways. We adopt the approach of @AndNar2024-F to obtain the solution to @eq-obj, but the procedure from @Lue1969 [p. 85] can also be used, where the problem is divided to @eq-subobj and reconstructed to find the solution to @eq-obj.

There exists a Lagrange multiplier $\bm{\Lambda}$ such that
$$
L(\bm{G}) = \tr(\bm{G}\bm{W}_h\bm{G}') + \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S}))
$$
is stationary at an extremum $\bm{G}$ [@Lue1969, p. 243, Theorem 1]. We find the numerator of the Gateaux differential [@Lue1969, p. 171] 
$$
\lim_{\alpha \to 0} \frac{L(\bm{G} + \alpha\bm{H}) - L(\bm{G})}{\alpha}
$$
to be
$$
\begin{aligned}
L(\bm{G} + \alpha\bm{H}) - L(\bm{G}) =&
\tr((\bm{G} + \alpha\bm{H})\bm{W}_h(\bm{G} + \alpha\bm{H})') + \tr(\bm{\Lambda}'(\bm{I} - (\bm{G} + \alpha\bm{H})\bm{S})) - \tr(\bm{G}\bm{W}_h\bm{G}') - \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S})) \\
=&
\tr(\bm{G}\bm{W}_h\bm{G}') + \tr(\alpha^2\bm{H}\bm{W}_h\bm{H}') + \tr(\alpha\bm{G}\bm{W}_h\bm{H}') + \tr(\alpha\bm{H}\bm{W}_h\bm{G}') + \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S})) - \tr(\alpha\bm{\Lambda}'\bm{H}\bm{S}) \\
&- \tr(\bm{G}\bm{W}_h\bm{G}') - \tr(\bm{\Lambda}'(\bm{I} - \bm{G}\bm{S}))\\
=& \alpha^2\tr(\bm{H}\bm{W}_h\bm{H}') + \alpha\tr(\bm{G}\bm{W}_h\bm{H}') + \alpha\tr(\bm{H}\bm{W}_h\bm{G}')  -\alpha\tr(\bm{\Lambda}'\bm{H}\bm{S}).
\end{aligned}
$$
Thus, the Gateaux differential becomes
$$
\begin{aligned}
\lim_{\alpha \to 0} \frac{L(\bm{G} + \alpha\bm{H}) - L(\bm{G})}{\alpha} =&
\lim_{\alpha \to 0} \alpha\tr(\bm{H}\bm{W}_h\bm{H}') + \tr(\bm{G}\bm{W}_h\bm{H}') + \tr(\bm{H}\bm{W}_h\bm{G}')  -\tr(\bm{\Lambda}'\bm{H}\bm{S})\\
=& 
\tr(\bm{G}\bm{W}_h\bm{H}') + \tr(\bm{H}\bm{W}_h\bm{G}') - \tr(\bm{\Lambda}'(\bm{H}\bm{S}))\\
=&\tr(2\bm{H}\bm{W}_h\bm{G}'-\bm{\Lambda}'\bm{H}\bm{S})\\
=& \tr(\bm{H}(2\bm{W}_h\bm{G}'-\bm{S}\bm{\Lambda}')),
\end{aligned}
$$
which we set to zero with a value of $\bm{G}^*$
$$
\begin{aligned}
\tr(\bm{H}(2\bm{W}_h\bm{G}^{*\prime}-\bm{S}\bm{\Lambda}'))&=0 \\
2\bm{W}_h\bm{G}^* &= \bm{S}\bm{\Lambda}' \\
\bm{G}^{*\prime} &= \frac{1}{2}\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'.
\end{aligned}
$$
Multiplying $\bm{S}'$ to the left of both sides. we have
$$
\bm{S}'\bm{G}^{*\prime}=\bm{I} = \frac{1}{2}\bm{S}'\bm{W}_h^{-1}\bm{S}\bm{\Lambda}'
\qquad\text{and}\qquad
\bm{\Lambda}' = 2 (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
$$
because $\bm{G}^*\bm{S} = \bm{I}$. Putting it back in, we have
$$
\bm{G}^{*\prime} = \bm{W}_h^{-1}\bm{S}(\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}
\qquad\text{and}\qquad
\bm{G}^* = (\bm{S}'\bm{W}_h^{-1}\bm{S})^{-1}\bm{S}'\bm{W}_h^{-1}.
$$

To see how @eq-obj can be split into separate problems, recognise
$$
\tr(\bm{G}\bm{W}_h\bm{G}') = \sum^m_{i=1} \bm{g}_i'\bm{W}_h\bm{g}_i,
$$
where $\bm{W}_h$ is a positive definite variance covariance matrix, which makes each $\bm{g}_i'\bm{W}_h\bm{g}_i$ positive. Additionally, the element in the $i$th row and $j$th column of $\bm{G}\bm{S}$ is $\bm{g}_i'\bm{s}_{j}$, and the element in the $i$th row and $j$th column of an identity matrix is $\delta_{ij}$. Therefore, the problem in @eq-obj is $m$ separate problems.

:::

In other words, the forecast projection method gives optimal projected forecast for a given set of components, in the sense that the unbiased forecast of each series has minimum variance.

## Estimation of $\bm{W}_h$ {#sec-estW}

In practice, the base forecast error variance $\bm{W}_h$ is unknown and needs to be estimated. Denote $\hat{\bm{e}}_{t,h} = \bm{z}_{t} - \hat{\bm{z}}_{t|t-h}$ as the $h$-step-ahead base forecast in-sample residual. The conventional forecast error variance matrix estimator
$$
\widehat{\bm{W}}_h=\frac{1}{T-h-1}\sum^T_{t=h+1}\hat{\bm{e}}_{t,h}\hat{\bm{e}}_{t,h}',
$$
albeit unbiased, is not considered a good approximation to the true forecast error variance in a finite sample when $(m+p) \approx T-h$. It is even singular when $(m+p)>T-h$, which makes the quantities discussed in the previous sections impossible to calculate. For this reason, while other shrinkage estimators can be considered, we adopt the covariance shrinkage method of @SchStr2005 and the variance shrinkage method of @OpgStr2007. Then, the estimated forecast error variance matrix is guaranteed to be positive definite with no numerical problems when computing their inverse. This estimator is denoted as $\widehat{\bm{W}}_h^{shr} = (\hat{w}_{ij,h}^{shr})_{1\leq i,j\leq m+p}$ with the element in row $i$ and column $j$
$$
\hat{w}_{ij,h}^{shr} = \hat{r}_{ij,h}^{shr}\sqrt{\hat{v}_{i,h}\hat{v}_{j,h}},
$$
where $\hat{r}_{ij,h}^{shr} = (1-\hat{\lambda}_{cor})\hat{r}_{ij,h}$ and $\hat{v}_{i,h} = \hat{\lambda}_{var}\hat{w}_{h, median} + (1-\hat{\lambda}_{var})\hat{w}_{i, h}$, with $\hat{\lambda}_{cor}$ being the shrinkage intensity parameter for the correlation
$$
\hat{\lambda}_{cor} =
\min\left(1,
\frac
{\sum_{i\neq j}\widehat{\operatorname{var}}(\hat{r}_{ij,h})}
{\sum_{i\neq j}\hat{r}_{ij,h}^2}
\right),
$$
and $\hat{\lambda}_{var}$ being the shrinkage intensity parameter for the variance
$$
\hat{\lambda}_{var} =
\min\left(1,
\frac
{\sum_{i=1}^{m+p}\widehat{\operatorname{var}}(\hat{w}_{i,h})}
{\sum_{i=1}^{m+p}(\hat{w}_{i, h} - \hat{w}_{h, median})^2}
\right),
$$
$\hat{r}_{ij,h}$ the sample correlation of the $h$-step-ahead forecast error between the $i$th and the $j$th series (component) in $\bm{z}_t$, $\hat{w}_{i, h}$ the $h$-step-ahead sample base forecast error variance associated with the $i$th series (the $i$th diagonal element of $\widehat{\bm{W}}_h$), and $\hat{w}_{h, median}$ the median of the $h$-step-ahead sample forecast error variance of the series and components (the median of the diagonal elements of $\widehat{\bm{W}}_h$). The estimation of $\widehat{\bm{W}}_h^{shr}$ in the following sections are implemented using the package `corpcor` [@corpcor] in R [@R].

Estimating $\widehat{\bm{W}}_h^{shr}$ for each forecast horizon $h$ is desirable but computationally intensive. It involves the calculation of multi-step-ahead in-sample residuals of the forecast models, which is especially challenging for iterative forecasts. Because of this, in practice it is not unreasonable to assume the $h$-step forecast error variance is proportional to the $1$-step forecast error variance by a constant $\eta_h$, as do @WicEtAl2019:
$$
\widehat{\bm{W}}_h^{shr} = \eta_h\widehat{\bm{W}}_1^{shr}.
$$
Under this assumption, when $\widehat{\bm{W}}_h^{shr}$ is used in @eq-M, the proportionality constant $\eta_h$ cancels out regardless of the value of $h$. We can effectively use only the one-step forecast error variance in forecast projection, if we only need point forecasts. We calculate $\widehat{\bm{W}}_h^{shr}$ for each $h$ for the simulation example in @sec-simulation, but assume this proportionality for the application in @sec-empirical-applications.

# Simulation {#sec-simulation}

In this section, we illustrate the performance of FLAP in a simulation setting. We generate time series of length $T=400$ from a $m=70$ variable VAR($3$) data generating process (DGP). The coefficients for the VAR DGP are estimated from the first $70$ series in the Australian tourism data set used in @sec-australian-domestic-tourism. The innovations are simulated from a multivariate normal distribution with an identity covariance matrix. The estimation and simulation is performed using the `tsDyn` package [@tsDyn].

For each simulated sample we generate $h=1$ to $12$-step-ahead base forecasts from benchmark models. We implement FLAP with a varying number of components and component construction methods. This process is repeated $220$ times. The improvements of FLAP forecasts over base forecasts is assessed, and the statistical significance of these improvements is evaluated.

## Generating base and FLAP forecasts {#sec-benchmarks}

```{r simulation}
```

We generate base forecasts from two benchmarks. The first benchmark is the univariate ARIMA model. For each series, we generate base forecasts from an ARIMA model using the `auto.arima()` function from the `forecast` package [@forecast] with the default settings.

<!-- The function implements an automatic model selection procedure proposed by @HynKha2008. The number of first differences is determined by repeated KPSS tests [@KwiEtAl1992] and the number of seasonal differences is determined by the seasonal strength computed from an STL decomposition [@CleEtAl1990;@fpp3]. The algorithm then chooses different orders of the autoregressive (AR) and moving average (MA) parts by comparing AICc between the corresponding models in a stepwise fashion, up to a maximal order of $5$. Univariate ARIMAs are also used to produce base forecasts of the components used in projection, including the cases where the base forecasts of the series are produced by a different model. -->

The second benchmark is the dynamic factor model (DFM). Following @StoWat2002a, base forecasts
$$
\hat{y}_{T+h} = \hat{\alpha}_h + \sum^n_{j=1}\hat{\bm{\beta}}_{hj}'\hat{\bm{F}}_{T-j+1} + \sum^s_{j=1}\hat{\gamma}_{hj}y_{T-j+1},
$$
where $\hat{\bm{F}}_t$ is the vector of $k$ estimated factors, and $\hat{y}_t$ is the target series to forecast. The factors are estimated using PCA on demeaned and scaled data. The optimal model is selected for each series based on the Bayesian information criterion (BIC) from models fitted using different combinations of meta-parameters in their corresponding range: $1 \leq k \leq 6$, $1 \leq n \leq 3$ and $1 \leq s \leq 3$. We note that the DFM produces direct forecasts in the sense that a different model is fitted for each forecast horizon $h$, in contrast to indirect or iterative forecasts generated by the ARIMA models.

<!-- ## Generating FLAP forecasts {#sec-flap} -->

We use several sets of weights to construct linear components for the FLAP method. The types of components are listed in @tbl-comp. Principal components from PCA are established using the `prcomp()` function in the `stats` package [@R]. We generate random components using weights simulated from a standard normal (Norm) distribution and a uniform (Unif) distribution with range $(-1, 1)$. We normalise the weights of all randomly generated components into unit vectors to ensure numerically stable computation, with $\bm{\phi}_i/\sqrt{\sum_j(\phi_{ij}^2)}$ where $\phi_{ij}$ is the $j$th value in the weight vector of the $i$th component. The total number of components $p$ is selected to be either $m=70$ or $300$.

| Component | Description |
| - | ----- |
| PCA | Principal components from PCA. |
| Norm | The weights of all components are simulated from a standard normal distribution. |
| Unif | The weights of all components are simulated from a uniform distribution |
| PCA+Norm | $m$ principal components from PCA are complemented with random components whose weights are simulated from a standard normal distribution. |
| PCA+Unif | $m$ principal components from PCA are complemented with random components whose weights are simulated from a uniform distribution. |
| Ortho | A random orthonormal matrix is generated using package `pracma` [@pracma] as the weight matrix. |
| Ortho+Norm | A random orthonormal matrix is generated using package `pracma`, forming the weights of the first $m$ components. Weights of the additional components are simulated from a standard normal distribution. |

: Component construction methods for FLAP {#tbl-comp}

## Forecast evaluation {#sec-evaluation}

We employ the Friedman test [@Fri1937; @Fri1939-JASA] along with post-hoc Nemenyi tests [@Nem1963;@HolEtAl2013] using the `tsutils` package [@tsutils] to compare forecast performance between different methods. The analysis involves the use of Multiple Comparisons with the Best (MCB) plot introduced by @KonEtAl2005 to visualise the comparison. The MSE of each series over different samples is calculated, and the MSEs of all the series are treated as observations in the Nemenyi test. The average ranks are plotted in @fig-simulation-mcb-series for forecast horizons $1$, $6$ and $12$. The methods using FLAP are labelled "Model -- Component Weights -- Number of Components". The benchmarks are labelled "Model -- Benchmark". These points are marked with triangles. The shaded region is the confidence interval of the best-performing method. Methods outside the shaded region are significantly worse than the best model.

In @fig-simulation-line, we plot the out-of-sample MSE values as a function of the number of components $p$ included in the FLAP method. Here we also include the performance of the DGP VAR model (VAR -- DGP), and an estimated VAR model with the correct specification (VAR -- Est.), as well as their corresponding FLAP generated forecasts. We have not presented the FLAP forecasts using components generated from a uniform distribution or random orthonormal matrices, as these are visually identical to the methods with random weights generated from a standard normal distribution. The vertical black line indicates when $p=m=70$, the number of original series.

```{r fig-simulation-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead MSE of different model and component specifications in the simulation. The methods using FLAP are labelled as "Model -- Component Weights -- Number of Components". The two benchmark models generating base forecasts are labelled "Model -- Benchmark". Their MSEs are marked with triangles. The shaded region is the confidence interval of the best performing method. Methods outside the shaded region are significantly worse than the best method.'
#| fig-height: 8
```

```{r fig-simulation-line}
#| fig-cap: 'Out-of-sample MSE for base and FLAP forecasts as the number of components $p$ increases, for forecast horizons $1$, $6$ and $12$. "VAR -- DGP" indicates the performance of the true data generating VAR model. "VAR -- Est." indicates the performance of the VAR model with the same structure as the true model but with estimated parameters. The solid horizontal lines show the MSE for the base forecasts (with no projection) while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of $p=m=70$, the number of series.'
#| fig-height: 7
```

### Evaluating base forecasts

Having simulated data from a VAR DGP, we expect the DFM to capture the correlations between series, something not possible with univariate ARIMA models. Hence, we expect the DFM benchmark to perform better than the ARIMA benchmark. This is indeed the case. @fig-simulation-mcb-series shows that the base DFM forecasts are significantly better than base ARIMA forecasts, with the exception for $h=1$, where the difference in rank is statistically insignificant but only marginally.

This is also observed in @fig-simulation-line. The solid horizontal line representing the MSE of the base DFM forecasts is always much lower than the horizontal solid line of the base ARIMA forecasts.

### Evaluating FLAP forecasts

The most important observation from @fig-simulation-mcb-series are the statistically significant improvements of the FLAP forecasts over their corresponding benchmark base forecasts. Note that we are referring here to forecasts corresponding to the same model, i.e., FLAP ARIMA forecasts, compared to base ARIMA forecasts and FLAP DFM forecasts, compared to base DFM forecasts. The average ranks of the FLAP forecasts are better than the corresponding base forecasts for all forecast horizons, and the differences are all statistically significant. The only exception is for ARIMA base forecasts with FLAP using only the $p=m=70$ principal components, where the difference in rank is marginally statistically insignificant.

Hence, the number of components seems to be important. The best-performing models all comprise the maximum $p=300$ components. For $h=1$-step-ahead forecasts, the differences between the methods with $p=300$ components are not significantly significant, regardless of the model that generated the base forecasts or the method used to construct the components.

Indeed, @fig-simulation-line shows that the MSEs for the FLAP ARIMA and DFM forecasts, represented by the dashed lines, decrease monotonically relative to the base forecasts, as the number of components increases. This supports @thm-monotone, demonstrating that increasing the number of components in FLAP reduces the forecast error variance. Of course this is only feasible in this ideal setting where each time series has $400$ observations and we use no more than $300$ components in FLAP. The relatively large number of observations coupled with the relatively simple VAR DGP, has potentially eased the challenge of estimation. This continuous reduction in forecast error variance is not always achievable with real data, as demonstrated in the empirical applications in @sec-empirical-applications, especially with FRED-MD dataset in @sec-fred-md.

Worth noting from @fig-simulation-line is the performance of the FLAP ARIMA forecasts relative to DFM forecasts as the number of components $p$ increases. The MSE of the FLAP ARIMA forecasts becomes lower than the DFM base forecasts for $p<70$ for $h=1$, for $70<p<100$ for $h=6$ and for a larger $p$ for $h=12$. Hence, FLAP is able to enhance univariate ARIMA forecasts with shared information between series by capturing and projecting these cross-correlations with the components.

Interestingly, for $h=1$, the MSEs of FLAP ARIMA and FLAP DFM forecasts converge to the same value as $p$ reaches $300$, no matter how the components are constructed. For the longer forecast horizons, the MSEs again converge; they do not reach the same value for $p=300$, although they are closer for $h=6$ than $h=12$. The reason for this is not the contribution of FLAP, but the starting point of the two sets of base forecasts. As the forecast horizon increases from $h=1$ to $6$ and $12$ the ARIMA base forecasts MSE increases. In contrast the MSEs for DFM base forecasts remains at a relatively constant level. Thus, it seems that the direct forecasts from the DFM model, particularly fitting a different model for each forecast horizon $h$, is advantageous for the longer forecast horizons, in this setting of forecasting data generated from a simple VAR DGP. This is in contrast to the iterative nature of ARIMA generated forecasts.

Note that the same forecasts of the components, generated from univariate ARIMA models, are used for both the FLAP ARIMA and the FLAP DFM methods. It is the increase in the number of the components, that leads to the dominant performance of the FLAP forecasts.
Hence, there is valuable information in the series that is not captured by either the ARIMA model or the DFM, but is captured by the components. Once again, this emphasises the importance of the components and FLAP. In this extreme case, a simple model after FLAP, performs as well as a more complicated model, while the forecast model itself is not as important as FLAP.

## Forming components for FLAP {#sec-component-construction}

The construction of components is obviously a primary element in the proposed FLAP method. However, the simulation results show that the method of generating the components may not be as important as one may have expected.

The most important feature in @fig-simulation-mcb-series, as discussed in the previous section, is the number of components used in this simulation setting. The results clearly show that the FLAP forecasts using the maximum considered $p=300$ components outrank the FLAP forecasts using $p=m=70$ components, no matter how the components are generated.
The only exception seems to be for $h=12$ and for the DFM base forecasts when using all $p=m=70$ PCA components or complementing these with randomly generated components from normal weights. Our conjecture, as also stated above, is that the key difference here lies in the quality of the direct DFM base forecasts for the longer forecast horizons, rather than in the component generating mechanism itself.
Furthermore, between the methods using the maximum considered $p=300$, there seems to be no evidence that the component generating mechanism makes any difference to the ranks, and especially there seems to be no difference between the distribution used to generate the random components.

Hence, in @fig-simulation-line, we present only the results for principal components and randomly generated components from a standard normal distribution. The results from using components generated from a uniform distribution or random orthonormal matrices are omitted, as these are visually identical to the latter.

Focusing on the ARIMA base forecasts it seems that for a relatively small $p$, the MSE decreases at a faster rate when principal components are used compared to random components. However, FLAP with random components seems to achieve a lower MSE than PCA as $p$ increases. The gap between the two diminishes for large $p$. We conjecture that the initial favourable performance of PCA stems from the construction of the orthogonal components aimed at capturing the maximum variance in the data. These components are then ranked from largest to smallest variance, which likely contributes to the method's effectiveness for smaller $p$ compared to random components. We comment more on PCA when analysing the results in @sec-empirical-applications.

The results show that the performance of FLAP using orthonormal weights is almost identical to FLAP using weights simulated from a normal distribution. This leads us to infer that it is not the orthogonality of the principal components that leads to the strong performance of FLAP using PCA. It may also suggest using simple random weights if one is prepared to include a relatively large number of components in FLAP; and using PCA when the intention is to use a small number of components. However, as we will see in @sec-empirical-applications, this is not the case with real data. In this case, PCA seems to be the preferred approach even when the number of components is large.

## Sources of uncertainty

The MSEs of the true VAR data generating process, VAR-DGP, and the correctly specified but estimated VAR, VAR-Est, are plotted at the bottom of each panel in @fig-simulation-line. As expected, this confirms that they are the best performing models. As the forecast horizon increases so does the MSE difference between these, with estimation error for the VAR-Est accumulating as forecasts are iteratively generated for the longer forecast horizons.

For both these models, FLAP cannot improve on the base forecasts. More importantly, in the case of VAR-Est, FLAP does not seem to be able to reduce the estimation error or the parameter uncertainty. On the other hand, FLAP shows significant improvements over base forecasts generated from misspecified models, such as the two benchmarks. This implies that the uncertainty which FLAP can account for and reduce, is mainly due to model misspecification. It seems that FLAP operates similarly to bagging, as bagging also reduces variance by controlling model uncertainty [@PetEtAl2018]. The uncertainty in how the components are constructed, unique in FLAP problems, is discussed in @sec-component-construction and awaits future research.

# Empirical applications {#sec-empirical-applications}

In this section, we apply the FLAP method in an empirical setting using two diverse datasets. The analysis confirms many of the results and conclusions drawn from the simulation setting, but it also identifies a few key differences.

## Australian domestic tourism {#sec-australian-domestic-tourism}

```{r visnights}
```

The Australian Tourism Data Set compiled from the National Visitor Survey by Tourism Research Australia contains the total number of nights spent by Australians away from home. We refer to these as visitor nights. The monthly visitor nights are recorded for $m=77$ regions around Australia, covering the period January 1998 to December 2019. To evaluate the forecast performance, we conduct am expanding window time series cross-validation [@fpp3]. More specifically, the first $T = 84$ observations are used as the first training set and the following $12$ months are used as the test set for evaluation. We repeat the evaluation for the rest of the data, by expanding each training sample by one observation at a time. This generates $169$ forecasts for evaluation, for each of the forecast horizons, $h=1$ to $12$.
Base forecasts for each series and for the components are generated using univariate ETS models selected and fitted using the `ets()` function in the `forecast` package [@forecast; @HynKha2008]. See @fpp2 [Chapter 7] for more details.
<!-- In an ETS model, the trend term describes the direction of the long-term tendency, the seasonal term describes the periodically recurring pattern with a fixed periodicity, and the error term measures the uncertainty. The trend and seasonal terms are optional. If a trend term exists, we allow it to be additive or additive damped; if a seasonality term exists, it can be addictive or multiplicative; the error can be additive or multiplicative. Excluding models with numerical instabilities, we choose the model with the smallest AICc among the $15$ possible models. -->

```{r fig-visnights-mcb-series}
#| fig-pos: "!b"
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the visitor nights data. The methods using forecast projection are named as "Model -- Component Weights -- Number of Components". The base models are named as "Model -- Benchmark" and these points are marked with triangles. The shaded region is the confidence interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```

```{r fig-visnights-line}
#| fig-cap: "Out-of-sample MSE of base and FLAP forecasts as the number of components $p$ increases, for forecast horizons $1$, $6$ and $12$, using the visitor nights data. The solid horizontal lines show the MSE for the base forecasts while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of $p=m$, the number of series."
```

MCB and MSE plots for $h=1$, $6$ and $12$ (as described in @sec-evaluation) are presented in @fig-visnights-mcb-series and @fig-visnights-line, respectively.
The FLAP forecasts in general outperform the base forecasts, which is consistent with the findings in @sec-simulation. In @fig-visnights-mcb-series, the base forecasts are always ranked last. Interestingly, FLAP forecasts improve over base forecasts, even when FLAP is using either one or at most two principal components. These improvements are statistically significant across all forecast horizons.

We highlight two findings that diverge from those observed in @sec-simulation.

First, in contrast to the results from @sec-simulation, the method implemented to construct components matters. Using principal components in FLAP improves base forecasts more compared to using random components with normal weights. @fig-visnights-mcb-series shows that the best ranked forecasts are the FLAP forecasts using either the $p=m=77$ principal components, or complementing these with random components so that $p=200$. These improve on FLAP forecasts using only $p=200$ random components and the improvements are statistically significant.

This is also clearly reflected in @fig-visnights-line, where the reduction in terms of MSE from using PCA is always lower, even after the $p=m$ principal components are exhausted and random components with weights generated from a normal distribution are added.

PCA aims to find components along the direction where the data varies the most. The first principal components accounts for the maximum variance in the data. Subsequent principal components are orthogonal to the previous ones and capture the maximum remaining variance. Based on this variance maximisation and ranking of PCA, we propose two potential explanations for its superior performance.

1. Optimality: By maximising and ranking the variance of PCs from largest to smallest, we ensure that the projection utilises components containing significantly more information (as measured by variance) compared to randomly weighted components.
2. Diversity: Actively seeking principal components with the highest variance, results in the incorporation of a more diverse set of components into the projection.

Second, consistent with the results in @sec-simulation, the MSEs of FLAP forecasts shown in @fig-visnights-line decrease as the number of components increases. However, this decreases ceases, especially for $h=1$, approximately for $p>150$. This is also reflected in the results presented in @fig-visnights-mcb-series, where the difference in ranking of the two best methods is not statistically significant, even though they use very different numbers of components ($p=m=77$ and $p=200$).

Choosing the number of components is a trade-off between the increasing estimation error as the dimension of forecast error variance $\bm{W}_h$ increases, and the additional benefit brought by the information embedded in the new components. Of course this all depends on the complexity of the DGP. For the visitor nights data, the benefit of components over and above the estimation error, diminishes after the number of components reaches $p=m=77$.

## FRED-MD {#sec-fred-md}

```{r fred-md}
```

The FRED-MD [@McCNg2016] is a popular monthly data set of macroeconomic variables, and shares similar properties with the @StoWat2002 data. We downloaded and transformed the data using the `fbi` package [@fbi]. The period we use for this exercise is from January 1959 to September 2023, containing $777$ observations. Following @McCNg2016, we replace observations that deviate from the sample median by more than 10 interquartile ranges (which are recognised as outliers), with missing values. We then drop any series with more than $5$% observations missing. This results in $m=122$ series. We fill in the missing values using the expectation-maximization (EM) algorithm described in @StoWat2002a with $8$ factors. The number $8$ is identified by @McCNg2016, albeit with a different time span.

In order to relate to the theoretical forecast error variance reduction, we use MSE as the error measure, instead of other scaled or percentage error measures. To reliably calculate MSE over series with different scales, we demean the series and scale them to have variance $1$. The MSEs are calculated on this standardised scale without back-transformation.

We evaluate the performance of forecasts using time series cross-validation. We start with $300$ observations in the first training set and the following $12$ observations as the test set. We repeat the exercise by expanding the training set increasing by $1$ observation at a time. This provides us with $466$ forecasts for each of the forecast horizons from $h=1$ to $12$ for evaluation.

We generate base forecasts from ARIMA models using `auto.arima()` function from the `forecast` package [@forecast] with the default settings, and DFMs. The ranges of the meta-parameters in the DFMs are $1\leq k \leq 8$ (since $8$ factors are identified and used to fill in the missing values), $1 \leq n \leq 3$ and $0 \leq s \leq 6$. For more details see @sec-benchmarks. The MCB and MSE plots for $h=1$, $6$ and $12$ are presented in @fig-fred-md-mcb-series and @fig-fred-md-line, respectively.

```{r fig-fred-md-mcb-series}
#| fig-cap: 'Average ranks of $1$-, $6$- and $12$-step-ahead cross-validation MSE of different model and component specifications on the FRED-MD data. The methods using forecast projection are named as "Model -- Component Weights -- Number of Components". The base models are named as "Model -- Benchmark" and these points are marked with triangles. The shaded region is the confidence interval of the best performing model. Methods outside the shaded region are significantly worse than the best model.'
```
```{r fig-fred-md-line}
#| fig-cap: "Out-of-sample MSE for base and FLAP forecasts as the number of components $p$ increases, for forecast horizons $1$, $6$ and $12$, using the FRED-MD data. The solid horizontal lines show the MSE for the base forecasts while the dashed lines show the MSEs of the FLAP forecasts. The vertical black line indicates the location of $p=m$ the number of series."
```

The results presented in @fig-fred-md-mcb-series show that the ARIMA base forecasts rank better that the DFM base forecasts.
This difference is statistically significant for $h=6$. Likewise, ARIMA base forecasts return a lower MSE compared to DFM as shown in @fig-fred-md-line across all forecast horizons.

The best ranked forecasts across all forecast horizons are once again FLAP forecasts generated using principal components. The improvements over the ARIMA base forecasts are significantly significant for $h=1$ and $6$. Furthermore, FLAP forecasts using PCA rank better than FLAP forecasts using components with random weights, and these differences are statistically significant. This is also observed in terms of MSE in @fig-fred-md-line, reaffirming our findings based on the tourism data set discussed in @sec-australian-domestic-tourism.
@fig-fred-md-line shows that both FLAP ARIMA and FLAP DFM forecasts, seem to be worse than the base forecasts when using a small number of components $p$ for $h=6$ and $12$. However, this improves as $p$ increases and FLAP forecasts outperform the base forecasts gradually. The trade-off between the benefit of including more components and the loss in degrees of freedom due to estimation in a large dimension is once again observed. However in this case it seems to be more extreme, compared to the tourism example, with MSEs starting to increase for $p>m$.
<!-- The projected forecast even worsens to the same level as the base forecast for ARIMA at $h=6$ and DFM at $h=12$ when $p=300$. -->

As we have seen in @sec-simulation and @sec-australian-domestic-tourism, $m$ is not always a clear cut-off point in determining the number of components to use. The performance of FLAP is jointly determined by the number of series $m$, the sample size $T$, the component construction method, and the DGP. The example of FRED-MD shows the importance of PCA, as $m$ is the point that the component changes from PCA to random normal weighted linear combinations, implying that PCA can exploit the information in the data while random weights cannot. This is more obvious for $h=1$ and $h=6$, as PCA works when $p<m$, but random normal weights do not seem to work from the beginning.

# Conclusion {#sec-conclusion}

The proposed forecast linear augmented projection (FLAP) method has been shown to be a simple but effective way to reduce forecast error variance of any multivariate forecasting problem. It simply involves augmenting the data with linear combinations, forecasting these and then projecting the augmented vector of forecasts. We have shown theoretically that FLAP will continue to reduce forecast error covariance as more components are added, assuming that the forecast error covariance matrix is known. In practice, a plug-in estimate of this covariance matrix can be used, and in both simulated and empirical data we demonstrate that a simple shrinkage estimator does indeed lead to improvements in forecast accuracy. Regarding the construction of components, we find that PCA in practice achieves significant improvements in forecast accuracy. Another appealing property of FLAP is that the projection step can even compensate for a poor choice of base forecasting model. This is particularly attractive since it makes FLAP robust against model misspecification in the base forecasting step.

One outstanding issue is to find alternatives to PCA to select component weights. For example, @Goerg2013-dg proposed "forecastable components" that are optimal in the sense of minimising the forecast error variance of the components, while @Matteson2011-jf proposed "dynamic orthogonal components" that reduce a multivariate time series to a set of uncorrelated univariate time series. It would be interesting to explore whether these components (or other similar suggestions) can be used effectively in FLAP. Another route to improving FLAP may be found by optimizing @eq-obj over $\bm{\Phi}$ and $\bm{G}$ rather than just $\bm{G}$.

Component construction should be studied together with the selection of the forecast model since both the weight matrix $\bm{\Phi}$ and the base forecast error variance $\bm{W}_h$ can affect the projection simultaneously in @eq-M. This is likely to be related to forecast combination methods, focusing on the properties of the base forecasts, and the diversity and robustness of the forecast model and components. Examples of studies on this issue in the forecast combination literature include @BatDua1995, @KanEtAl2022 and @LicWin2020.

Finally, while FLAP is motivated by the forecast reconciliation literature, our focus here is very much on multivariate time series with no constraints. However, it would be possible to use both forecast reconciliation and forecast projection together. This may be particularly useful when there are relationships between series that are not captured in the known hierarchical structure.

# Acknowledgements {.unnumbered}

We thank Daniele Girolimetto for contributing to the initial proof of @thm-monotone in his unpublished work.

# References {.unnumbered}
