---
title: "Free Lunch Multivariate Forecast Projection: reducing forecast variance using linear combinations"
author: [
  "Yangzhuoran Fin Yang",
  "Rob J Hyndman",
  "George Athanasopoulos",
  "Anastasios Panagiotelis"
  ]
template: template.tex
keep-tex: true
bibliography:
  - ../references-key.bib
  - ../references-pkg.bib
format: 
  beamer: 
    pdf-engine: pdflatex
    biblio-title: "References"
    theme: monash
    fonttheme: monash
    colortheme: monashwhite
header-includes:
  - \usepackage{bm}
  - \usepackage[makeroom]{cancel}
  - \widowpenalties 1 150
  - \def\Var{\operatorname{Var}}
  - \def\E{\operatorname{E}}
  - \def\tr{\operatorname{tr}}
titlefontsize: 18
knitr:
   opts_chunk: 
     out-width: 100%
     out-height: 100%
execute: 
  eval: true
  cache: true
editor_options: 
  chunk_output_type: console
---

```{r setup}
#| cache: false
knitr::read_chunk("free-lunch-slides.R")
```
```{r library}
#| cache: false
```
```{r data}
```


## Free lunch forecast projection

A model-independent post-forecast adjustment method that can reduce forecast variance using linear combinations (components) without introducing new data or information.

- Averaging indirect forecasts 
- Projecting forecasts of augmented series


## Australian tourism data

* The data include tourism information on seven states and territories which can be divided into 77 regions
  - For example, Melbourne, Sydney, East Coast

### Visitor nights
The total number of nights spent by Australians away from home recorded monthly

## Melbourne and Sydney

```{r p_syd_mel}
```

## Total and Region

```{r p_aus_mel}
```


## Intuition
\begin{block}{Observation}

1. Similar patterns are shared by different series.
2. Better signal-noise ratio in the linear combination.

\end{block}

\pause

\begin{alertblock}{One step further}
Finding components that 


1. are easy to forecast;

2. can capture the common signals;

3. can improve forecast of original series.
\end{alertblock}

## Literature

### Forecast reconciliation

* @WicEtAl2019: Projecting forecasts to be consistent with the hierarchical structure

### Forecast combination

* Combining forecasts of the target series
* @HolEtAl2021: Combining direct and indirect forecasts
* @PetSpi2021: Combining selection and transformation of the target series ("wisdom of data")

## Literature

### Bagging

* @BerEtAl2016: Bagging ETS models to forecast
* @PetEtAl2018: The benefits of bagging originate from the model uncertainty

### Dynamic factor model (DFM)

* @StoWat2002a, @StoWat2002

* @DeEtAl2019: Machine learning extension


## Series $\bm{z}_t\in \mathbb{R}^m$

```{r series}
```

## Components $\bm{c}_t = \bm{\Phi}\bm{z}_t \in \mathbb{R}^p$

```{r components}
```

## Free lunch forecast projection

$$
\bm{y}_t = \begin{bmatrix} \bm{z}_t\\ \bm{c}_t \end{bmatrix}
\qquad \tilde{\bm{y}}_{t+h} = \bm{M} \hat{\bm{y}}_{t+h}
$$

\begin{block}{}

$$
\tilde{\bm{z}}_{t+h} = \bm{J}\tilde{\bm{y}}_{t+h} = \bm{J}\bm{M}\hat{\bm{y}}_{t+h}
$$
\end{block}

$$
\begin{aligned}
\bm{M} &= \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\\
\bm{J} &= \bm{J}_{m,p} = \big[\bm{I}_m ~~~ \bm{O}_{m\times p}\big]\\
\bm{C} &= \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]\\
\bm{W}_h &= \Var(\hat{\bm{y}}_{t+h})
\end{aligned}
$$


## Forecasts and projection of series 

```{r series-fc}
```


## Unbiasedness

\begin{alertblock}{Unbiasedness}
If the base forecasts are unbiased, then the projected forecasts are also unbiased.
\end{alertblock}

### Projection matrix

1. The matrix $\bm{M}$ is a projection onto the space where the constraint $\bm{C}$ is satisfied.
1. The projected forecast $\tilde{\bm{y}}_{t+h}$ satisfies the constraint $\bm{C}$.
1. For $\bm{y}_{t+h}$ that already satisfies the constraint, the projection does not change its value: $\bm{M}\bm{y}_{t+h} = \bm{y}_{t+h}$ 


## Nonnegative Variance reduction

Under unbiasedness, the variance reduction is __positive semi-definite__:
$$
\begin{aligned}
\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) &-\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})\\ 
&= \bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}'
\end{aligned}
$$

## Example $\bm{W}_h = \bm{I}_{m+p}$

$$
\begin{aligned}
\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) &-\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})\\ 
&= \bm{J}\bm{C}'(\bm{C}\bm{C}')^{-1}\bm{C}\bm{J}'\\
&= \bm{\Phi}'
(\bm{\Phi}\bm{\Phi}' + \bm{I})^{-1}
\bm{\Phi}
\end{aligned}
$$

Let $\bm{\Phi}$ consist of orthogonal unit vectors:
$$
\begin{aligned}
\bm{\Phi}\bm{\Phi}' &= \bm{I}_p\text{ when } p\le m\\
\bm{\Phi}'\bm{\Phi} &= \bm{I}_m\text{ when } p= m.
\end{aligned}
$$

$$
\begin{aligned}
\tr(\Var(\hat{\bm{z}}_{t+h} - \bm{z}_{t+h}) &-\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) \\
&=\frac{1}{2}tr(\bm{\Phi}'\bm{\Phi})=\frac{1}{2}p
\end{aligned}
$$

## Positive condition

For the first component to have a guaranteed reduction of forecast variance, the following condition must be satisfied:

$$
\bm{\phi}_1\bm{W}_{z,h}\neq\bm{w}_{c_1z,h},
$$
<!-- where $\bm{\phi}_1$ is the weight vector of the first component, $\bm{W}_{z,h} = \Var(\hat{\bm{z}_{t+h}})$, and $\bm{w}_{c_1z,h}$ is the forecast covariance between the first component and the original series. -->

* $\bm{\phi}_1$ is the weight vector of the first component
* $\bm{W}_{z,h} = \Var(\hat{\bm{z}}_{t+h})$ 
* $\bm{w}_{c_1z,h}$ is the forecast covariance between the first component and the original series.


## Monotonicity

The sum of forecast variance reductions
$$
\begin{aligned}
\tr(\Var(\hat{\bm{z}}_{t+h} &- \bm{z}_{t+h}) -\Var(\tilde{\bm{z}}_{t+h} - \bm{z}_{t+h})) \\&= \tr(\bm{J}\bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}\bm{W}_h\bm{J}')
\end{aligned}
$$
is non-decreasing as $p$ increases.

## Minimum variance of individual series

The projection is equivalent to the mapping
$$
\tilde{\bm{z}}_{t+h} = \bm{G}\hat{\bm{y}}_{t+h},
$$
where $\bm{G} = \big[\bm{g}_1 ~~ \bm{g}_2 ~~ \dots ~~ \bm{g}_m\big]' \in \mathbb{R}^{m\times (m+p)}$ is the solution to
$$
\underset{\bm{G}}{\arg\min}\ \bm{G}\bm{W}_h\bm{G}'
\qquad \text{s.t. } \bm{G}\bm{S} = \bm{I}
$$ 
or
$$
\underset{\bm{g}_i}{\arg\min}\ \bm{g}_i'\bm{W}_h\bm{g}_i
\qquad \text{s.t. } \bm{g}_i'\bm{s}_{j} = \bm{1}(i=j),
$$
where $\bm{S} = \begin{bmatrix}\bm{I}_m \\\bm{\Phi}\end{bmatrix} = \big[\bm{s}_1\cdots \bm{s}_m\big]$.

## Key results

1. The forecast variance is __reduced__ with forecast projection
1. The forecast variance __monotonically__ decreases with increasing number of components
1. The forecast projection is __optimal__ to achieve minimum forecast variance of each series

## Estimation

$$
\tilde{\bm{z}}_{t+h} = \bm{J}\tilde{\bm{y}}_{t+h} = \bm{J}\bm{M}\hat{\bm{y}}_{t+h}
$$

\begin{block}{}

$$
\bm{M} = \bm{I}_{m+p} - \bm{W}_h\bm{C}'(\bm{C}\bm{W}_h\bm{C}')^{-1}\bm{C}
$$
\end{block}

$$
\begin{aligned}
\bm{W}_h &= \Var(\hat{\bm{y}}_{t+h})\\
\bm{C} &= \big[- \bm{\Phi} ~~~ \bm{I}_{p}\big]\\
\end{aligned}
$$

* Estimation of $\bm{W}_h$
* Construction of $\bm{\Phi}$


## Estimation of $\bm{W}_h$

__Shrinking variance__ towards their median [@OpgStr2007] and __shrinking covariance__ towards zero [@SchStr2005].

The shrinkage estimator is

* Positive definite, and 
* Numerically stable.

In empirical applications, we assume
$$
\widehat{\bm{W}}_h^{shr} = \eta_h\widehat{\bm{W}}_1^{shr}.
$$

## Construction of $\bm{\Phi}$

### Principal component analysis (PCA)

Finding the weights matrix so that the resulting components \alert{\textbf{maximise variance}}

### Simulation

Generating values from a random distribution and normalising them to unit vectors

* Normal distribution
* Uniform distribution
* Orthonormal matrix [@pracma]


## Tourism (ETS)

```{r visnights}
```


## FRED-MD (DFM)

```{r fred-md}
```

## Simulation

* Data generating process (DGP): VAR($3$) with $m=70$ variables
* Sample size: $T=400$
* Number of repeated samples: $220$
* Base model: ARIMA and DFM

## Simulation

```{r simulation}
```

## Future research directions

* Investigate why PCA performs better than random weights
* Find other components that are better than PCA
* Find optimal components by minimising forecast variance with respect to $\bm{\Phi}$
* Use forecast projection and forecast reconciliation together

## References {.allowframebreaks}

::: {#refs}
:::
